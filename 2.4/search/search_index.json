{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Percona XtraBackup - Documentation \u00b6 Percona XtraBackup is an open-source hot backup utility for MySQL - based servers that does not lock your database during the backup. It can back up data from InnoDB , XtraDB, and MyISAM tables on MySQL 5.1 , 5.5, 5.6 and 5.7 servers, as well as Percona Server with XtraDB. Note Support for InnoDB 5.1 builtin has been removed in Percona XtraBackup 2.1 For a high-level overview of many of its advanced features, including a feature comparison, please see About Percona XtraBackup . Whether it is a 24x7 highly loaded server or a low-transaction-volume environment, Percona XtraBackup is designed to make backups a seamless procedure without disrupting the performance of the server in a production environment. Commercial support contracts are available . Important Percona XtraBackup 2.4 does not support making backups of databases created in MySQL 8.0 , Percona Server for MySQL 8.0 , or Percona XtraDB Cluster 8.0 . Use Percona XtraBackup 8.0 for the version 8.0 databases.","title":"Percona XtraBackup - Documentation"},{"location":"index.html#percona-xtrabackup-documentation","text":"Percona XtraBackup is an open-source hot backup utility for MySQL - based servers that does not lock your database during the backup. It can back up data from InnoDB , XtraDB, and MyISAM tables on MySQL 5.1 , 5.5, 5.6 and 5.7 servers, as well as Percona Server with XtraDB. Note Support for InnoDB 5.1 builtin has been removed in Percona XtraBackup 2.1 For a high-level overview of many of its advanced features, including a feature comparison, please see About Percona XtraBackup . Whether it is a 24x7 highly loaded server or a low-transaction-volume environment, Percona XtraBackup is designed to make backups a seamless procedure without disrupting the performance of the server in a production environment. Commercial support contracts are available . Important Percona XtraBackup 2.4 does not support making backups of databases created in MySQL 8.0 , Percona Server for MySQL 8.0 , or Percona XtraDB Cluster 8.0 . Use Percona XtraBackup 8.0 for the version 8.0 databases.","title":"Percona XtraBackup - Documentation"},{"location":"404.html","text":"404 - Not Found \u00b6 We can\u2019t find the page you are looking for. Try using the Search or return to the homepage .","title":"404 - Not Found"},{"location":"404.html#404-not-found","text":"We can\u2019t find the page you are looking for. Try using the Search or return to the homepage .","title":"404 - Not Found"},{"location":"copyright-and-licensing-information.html","text":"Copyright and licensing information \u00b6 Documentation licensing \u00b6 This software documentation is (C)2009-2022 Percona LLC and/or its affiliates and is distributed under the Creative Commons Attribution-ShareAlike 2.0 Generic license.","title":"Copyright and licensing information"},{"location":"copyright-and-licensing-information.html#copyright-and-licensing-information","text":"","title":"Copyright and licensing information"},{"location":"copyright-and-licensing-information.html#documentation-licensing","text":"This software documentation is (C)2009-2022 Percona LLC and/or its affiliates and is distributed under the Creative Commons Attribution-ShareAlike 2.0 Generic license.","title":"Documentation licensing"},{"location":"faq.html","text":"Frequently Asked Questions \u00b6 Do I need an InnoDB Hot Backup license to use Percona XtraBackup? \u00b6 No. Although innobackupex is derived from the same GPL and open-source wrapper script that InnoDB Hot Backup uses, it does not execute ibbackup , and the xtrabackup binary does not execute or link to ibbackup . You can use Percona XtraBackup without any license; it is completely separate from InnoDB Hot Backup. What\u2019s the difference between innobackupex and innobackup? \u00b6 The innobackupex binary is a patched version of the Oracle innobackup script (renamed mysqlbackup). They are similar, and familiarity with innobackup might be helpful. Besides the available options for specific features of innobackupex , the main differences are: Prints to STDERR instead of STDOUT which enables the innobackupex --stream option Detects the configuration file - my.cnf - is automatically (or set with innobackupex --defaults-file ) instead of requiring the configuration file as the the first argument Defaults to xtrabackup as binary to use in the innobackupex --ibbackup See The innobackupex Option Reference for more details. Which Web-based backup tools are based on Percona XtraBackup? \u00b6 Zmanda Recovery Manager is a commercial tool that uses Percona XtraBackup for Non-Blocking Backups: \u201cZRM provides support for non-blocking backups of MySQL using Percona XtraBackup . ZRM with Percona XtraBackup provides resource utilization management by providing throttling based on the number of IO operations per second. Percona XtraBackup based backups also allow for table-level recovery even though the backup was done at the database level. This operation requires the recovery database server to be Percona Server for MySQL with XtraDB.\u201d xtrabackup binary fails with a floating point exception \u00b6 In most of the cases this is due to not having installed the required libraries (and version) by xtrabackup . Installing the GCC suite with the supporting libraries and recompiling xtrabackup will solve the issue. See Compiling and Installing from Source Code for instructions on the procedure. How does xtrabackup handle the ibdata/ib_log files on restore if they are not in the MySQL datadir? \u00b6 If the ibdata and ib_log files are located in different directories outside of the datadir, you move them to their proper place after the logs have been applied. Backup fails with Error 24: \u2018Too many open files\u2019 \u00b6 This error usually occurs when the database being backed up contains large amount of files and Percona XtraBackup can\u2019t open all of them to create a successful backup. In order to avoid this error the operating system should be configured appropriately so that Percona XtraBackup can open all its files. On Linux, this can be done with the ulimit command for specific backup session or by editing the /etc/security/limits.conf to change it globally Note The maximum possible value that can be set up is 1048576 which is a hard-coded constant in the Linux kernel. How to deal with skipping of redo logs for DDL operations? \u00b6 To prevent creating corrupted backups when running DDL operations, Percona XtraBackup aborts if it detects that redo logging is disabled. In this case, the following error is printed: [FATAL] InnoDB: An optimized (without redo logging) DDL operation has been performed. All modified pages may not have been flushed to the disk yet. Percona XtraBackup will not be able to take a consistent backup. Retry the backup operation. Note Redo logging is disabled during a sorted index build To avoid this error, Percona XtraBackup can use metadata locks on tables while they are copied: To block all DDL operations, use the xtrabackup --lock-ddl option that issues LOCK TABLES FOR BACKUP . If LOCK TABLES FOR BACKUP is not supported, you can block DDL for each table before XtraBackup starts to copy it and until the backup is completed using the xtrabackup --lock-ddl-per-table option.","title":"Frequently Asked Questions"},{"location":"faq.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq.html#do-i-need-an-innodb-hot-backup-license-to-use-percona-xtrabackup","text":"No. Although innobackupex is derived from the same GPL and open-source wrapper script that InnoDB Hot Backup uses, it does not execute ibbackup , and the xtrabackup binary does not execute or link to ibbackup . You can use Percona XtraBackup without any license; it is completely separate from InnoDB Hot Backup.","title":"Do I need an InnoDB Hot Backup license to use Percona XtraBackup?"},{"location":"faq.html#whats-the-difference-between-innobackupex-and-innobackup","text":"The innobackupex binary is a patched version of the Oracle innobackup script (renamed mysqlbackup). They are similar, and familiarity with innobackup might be helpful. Besides the available options for specific features of innobackupex , the main differences are: Prints to STDERR instead of STDOUT which enables the innobackupex --stream option Detects the configuration file - my.cnf - is automatically (or set with innobackupex --defaults-file ) instead of requiring the configuration file as the the first argument Defaults to xtrabackup as binary to use in the innobackupex --ibbackup See The innobackupex Option Reference for more details.","title":"What\u2019s the difference between innobackupex and innobackup?"},{"location":"faq.html#which-web-based-backup-tools-are-based-on-percona-xtrabackup","text":"Zmanda Recovery Manager is a commercial tool that uses Percona XtraBackup for Non-Blocking Backups: \u201cZRM provides support for non-blocking backups of MySQL using Percona XtraBackup . ZRM with Percona XtraBackup provides resource utilization management by providing throttling based on the number of IO operations per second. Percona XtraBackup based backups also allow for table-level recovery even though the backup was done at the database level. This operation requires the recovery database server to be Percona Server for MySQL with XtraDB.\u201d","title":"Which Web-based backup tools are based on Percona XtraBackup?"},{"location":"faq.html#xtrabackup-binary-fails-with-a-floating-point-exception","text":"In most of the cases this is due to not having installed the required libraries (and version) by xtrabackup . Installing the GCC suite with the supporting libraries and recompiling xtrabackup will solve the issue. See Compiling and Installing from Source Code for instructions on the procedure.","title":"xtrabackup binary fails with a floating point exception"},{"location":"faq.html#how-does-xtrabackup-handle-the-ibdataib_log-files-on-restore-if-they-are-not-in-the-mysql-datadir","text":"If the ibdata and ib_log files are located in different directories outside of the datadir, you move them to their proper place after the logs have been applied.","title":"How does xtrabackup handle the ibdata/ib_log files on restore if they are not in the MySQL datadir?"},{"location":"faq.html#backup-fails-with-error-24-too-many-open-files","text":"This error usually occurs when the database being backed up contains large amount of files and Percona XtraBackup can\u2019t open all of them to create a successful backup. In order to avoid this error the operating system should be configured appropriately so that Percona XtraBackup can open all its files. On Linux, this can be done with the ulimit command for specific backup session or by editing the /etc/security/limits.conf to change it globally Note The maximum possible value that can be set up is 1048576 which is a hard-coded constant in the Linux kernel.","title":"Backup fails with Error 24: \u2018Too many open files\u2019"},{"location":"faq.html#how-to-deal-with-skipping-of-redo-logs-for-ddl-operations","text":"To prevent creating corrupted backups when running DDL operations, Percona XtraBackup aborts if it detects that redo logging is disabled. In this case, the following error is printed: [FATAL] InnoDB: An optimized (without redo logging) DDL operation has been performed. All modified pages may not have been flushed to the disk yet. Percona XtraBackup will not be able to take a consistent backup. Retry the backup operation. Note Redo logging is disabled during a sorted index build To avoid this error, Percona XtraBackup can use metadata locks on tables while they are copied: To block all DDL operations, use the xtrabackup --lock-ddl option that issues LOCK TABLES FOR BACKUP . If LOCK TABLES FOR BACKUP is not supported, you can block DDL for each table before XtraBackup starts to copy it and until the backup is completed using the xtrabackup --lock-ddl-per-table option.","title":"How to deal with skipping of redo logs for DDL operations?"},{"location":"glossary.html","text":"Glossary \u00b6 .ARM file \u00b6 Contains the metadata for each Archive Storage Engine table. .ARZ file \u00b6 Contains the data for each Archive Storage Engine table. .CSM \u00b6 Each table with the CSV Storage Engine has .CSM file which contains the metadata of it. .CSV \u00b6 Each table with the CSV Storage engine has .CSV file which contains the data of it (which is a standard Comma Separated Value file). .exp \u00b6 Files with the .exp extension are created by Percona XtraBackup per each InnoDB tablespace when the --export option is used in the prepare phase. See restoring individual tables . .frm \u00b6 For each table, the server will create a file with the .frm extension containing the table definition (for all storage engines). .ibd \u00b6 On a multiple tablespace setup (innodb_file_per_table enabled), MySQL will store each newly created table in a file with a .ibd extension. .MRG \u00b6 Each table using the MERGE storage engine, besides of a .frm file, will have .MRG file containing the names of the MyISAM tables associated with it. .MYD \u00b6 Each MyISAM table has .MYD (MYData) file which contains the data on it. .MYI \u00b6 Each MyISAM table has .MYI (MYIndex) file which contains the table\u2019s indexes. .opt \u00b6 MySQL stores options of a database (like charset) in a file with a .opt extension in the database directory. .par \u00b6 Each partitioned table has .par file which contains metadata about the partitions. .TRG \u00b6 File containing the triggers associated with a table, for example :file:mytable.TRG With the .TRN file, they represent all the Trigger definitions. .TRN \u00b6 File containing the names of the triggers that are associated with a table, for example :file:mytable.TRN . With the .TRG file, they represent all the trigger definitions. backup \u00b6 The process of copying data or tables to be stored in a different location. compression \u00b6 The method that produces backups in a reduced size. configuration file \u00b6 The file that contains the server startup options. crash \u00b6 An unexpected shutdown that does not allow the normal server shutdown cleanup activities. crash recovery \u00b6 The actions that occur when MySQL is restarted after a crash. data dictionary \u00b6 The metadata for the tables, indexes, and table columns stored in the InnoDB system tablespace. datadir \u00b6 The directory in which the database server stores its data files. Most Linux distribution use /var/lib/mysql by default. full backup \u00b6 A backup that contains the complete source data from an instance. ibdata \u00b6 Default prefix for tablespace files, for example, ibdata1 is a 10MB auto-extensible file that MySQL creates for the shared tablespace by default. incremental backup \u00b6 A backup stores data from a specific point in time. InnoDB \u00b6 Storage engine which provides ACID-compliant transactions and foreign key support, among others improvements over MyISAM. It is the default engine for MySQL as of the 8.0 series. innodb_buffer_pool_size \u00b6 The size in bytes of the memory buffer to cache data and indexes of InnoDB \u2019s tables. This aims to reduce disk access to provide better performance. By default: [mysqld] innodb_buffer_pool_size=8MB innodb_data_home_dir \u00b6 The directory (relative to datadir) where the database server stores the files in a shared tablespace setup. This option does not affect the location of innodb_file_per_table. For example: [mysqld] innodb_data_home_dir = ./ innodb_data_file_path \u00b6 Specifies the names, sizes and location of shared tablespace files: [mysqld] innodb_data_file_path=ibdata1:50M;ibdata2:50M:autoextend innodb_file_per_table \u00b6 By default, InnoDB creates tables and indexes in a file-per-tablespace . If the innodb_file_per_table variable is disabled, you can enable the variable in your configuration file: [mysqld] innodb_file_per_table or start the server with --innodb_file_per_table . innodb_log_group_home_dir \u00b6 Specifies the location of the InnoDB log files: [mysqld] innodb_log_group_home=/var/lib/mysql logical backup \u00b6 A backup which contains a set of SQL statements. The statements can be used to recreate the databases. LSN \u00b6 Each InnoDB page (usually 16kb in size) contains a log sequence number, or LSN. The LSN is the system version number for the entire database. Each page\u2019s LSN shows how recently it was changed. my.cnf \u00b6 The database server\u2019s main configuration file. Most Linux distributions place it as /etc/mysql/my.cnf or /etc/my.cnf , but the location and name depends on the particular installation. Note that this is not the only way of configuring the server, some systems does not have one and rely on the command options to start the server and its default values. MyISAM \u00b6 Previous default storage engine for MySQL for versions prior to 5.5. It doesn\u2019t fully support transactions but in some scenarios may be faster than InnoDB. Each table is stored on disk in 3 files: .frm, .MYD, .MYI. physical backup \u00b6 A backup that copies the data files. point in time recovery \u00b6 This method allows data to be restored to the state it was in any selected point of time. prepared backup \u00b6 A consistent set of backup data that is ready to be restored. restore \u00b6 Copies the database backups taken using the backup command to the original location or a different location. A restore returns data that has been either lost, corrupted, or stolen to the original condition at a specific point in time. xbcrypt \u00b6 To support encryption and decryption of the backups, a new tool xbcrypt was introduced to Percona XtraBackup . This utility has been modeled after the xbstream binary to perform encryption and decryption outside of Percona XtraBackup . xbstream \u00b6 To support simultaneous compression and streaming, Percona XtraBackup uses the xbstream format. For more information, see --stream XtraDB \u00b6 Percona XtraDB is an enhanced version of the InnoDB storage engine, designed to better scale on modern hardware. Percona XtraDB includes a variety of other features useful in high performance environments. It is fully backwards compatible, and so can be used as a drop-in replacement for standard InnoDB. More information here .","title":"Glossary"},{"location":"glossary.html#glossary","text":"","title":"Glossary"},{"location":"glossary.html#arm-file","text":"Contains the metadata for each Archive Storage Engine table.","title":".ARM file"},{"location":"glossary.html#arz-file","text":"Contains the data for each Archive Storage Engine table.","title":".ARZ file"},{"location":"glossary.html#csm","text":"Each table with the CSV Storage Engine has .CSM file which contains the metadata of it.","title":".CSM"},{"location":"glossary.html#csv","text":"Each table with the CSV Storage engine has .CSV file which contains the data of it (which is a standard Comma Separated Value file).","title":".CSV"},{"location":"glossary.html#exp","text":"Files with the .exp extension are created by Percona XtraBackup per each InnoDB tablespace when the --export option is used in the prepare phase. See restoring individual tables .","title":".exp"},{"location":"glossary.html#frm","text":"For each table, the server will create a file with the .frm extension containing the table definition (for all storage engines).","title":".frm"},{"location":"glossary.html#ibd","text":"On a multiple tablespace setup (innodb_file_per_table enabled), MySQL will store each newly created table in a file with a .ibd extension.","title":".ibd"},{"location":"glossary.html#mrg","text":"Each table using the MERGE storage engine, besides of a .frm file, will have .MRG file containing the names of the MyISAM tables associated with it.","title":".MRG"},{"location":"glossary.html#myd","text":"Each MyISAM table has .MYD (MYData) file which contains the data on it.","title":".MYD"},{"location":"glossary.html#myi","text":"Each MyISAM table has .MYI (MYIndex) file which contains the table\u2019s indexes.","title":".MYI"},{"location":"glossary.html#opt","text":"MySQL stores options of a database (like charset) in a file with a .opt extension in the database directory.","title":".opt"},{"location":"glossary.html#par","text":"Each partitioned table has .par file which contains metadata about the partitions.","title":".par"},{"location":"glossary.html#trg","text":"File containing the triggers associated with a table, for example :file:mytable.TRG With the .TRN file, they represent all the Trigger definitions.","title":".TRG"},{"location":"glossary.html#trn","text":"File containing the names of the triggers that are associated with a table, for example :file:mytable.TRN . With the .TRG file, they represent all the trigger definitions.","title":".TRN"},{"location":"glossary.html#backup","text":"The process of copying data or tables to be stored in a different location.","title":"backup"},{"location":"glossary.html#compression","text":"The method that produces backups in a reduced size.","title":"compression"},{"location":"glossary.html#configuration-file","text":"The file that contains the server startup options.","title":"configuration file"},{"location":"glossary.html#crash","text":"An unexpected shutdown that does not allow the normal server shutdown cleanup activities.","title":"crash"},{"location":"glossary.html#crash-recovery","text":"The actions that occur when MySQL is restarted after a crash.","title":"crash recovery"},{"location":"glossary.html#data-dictionary","text":"The metadata for the tables, indexes, and table columns stored in the InnoDB system tablespace.","title":"data dictionary"},{"location":"glossary.html#datadir","text":"The directory in which the database server stores its data files. Most Linux distribution use /var/lib/mysql by default.","title":"datadir"},{"location":"glossary.html#full-backup","text":"A backup that contains the complete source data from an instance.","title":"full backup"},{"location":"glossary.html#ibdata","text":"Default prefix for tablespace files, for example, ibdata1 is a 10MB auto-extensible file that MySQL creates for the shared tablespace by default.","title":"ibdata"},{"location":"glossary.html#incremental-backup","text":"A backup stores data from a specific point in time.","title":"incremental backup"},{"location":"glossary.html#innodb","text":"Storage engine which provides ACID-compliant transactions and foreign key support, among others improvements over MyISAM. It is the default engine for MySQL as of the 8.0 series.","title":"InnoDB"},{"location":"glossary.html#innodb_buffer_pool_size","text":"The size in bytes of the memory buffer to cache data and indexes of InnoDB \u2019s tables. This aims to reduce disk access to provide better performance. By default: [mysqld] innodb_buffer_pool_size=8MB","title":"innodb_buffer_pool_size"},{"location":"glossary.html#innodb_data_home_dir","text":"The directory (relative to datadir) where the database server stores the files in a shared tablespace setup. This option does not affect the location of innodb_file_per_table. For example: [mysqld] innodb_data_home_dir = ./","title":"innodb_data_home_dir"},{"location":"glossary.html#innodb_data_file_path","text":"Specifies the names, sizes and location of shared tablespace files: [mysqld] innodb_data_file_path=ibdata1:50M;ibdata2:50M:autoextend","title":"innodb_data_file_path"},{"location":"glossary.html#innodb_file_per_table","text":"By default, InnoDB creates tables and indexes in a file-per-tablespace . If the innodb_file_per_table variable is disabled, you can enable the variable in your configuration file: [mysqld] innodb_file_per_table or start the server with --innodb_file_per_table .","title":"innodb_file_per_table"},{"location":"glossary.html#innodb_log_group_home_dir","text":"Specifies the location of the InnoDB log files: [mysqld] innodb_log_group_home=/var/lib/mysql","title":"innodb_log_group_home_dir"},{"location":"glossary.html#logical-backup","text":"A backup which contains a set of SQL statements. The statements can be used to recreate the databases.","title":"logical backup"},{"location":"glossary.html#lsn","text":"Each InnoDB page (usually 16kb in size) contains a log sequence number, or LSN. The LSN is the system version number for the entire database. Each page\u2019s LSN shows how recently it was changed.","title":"LSN"},{"location":"glossary.html#mycnf","text":"The database server\u2019s main configuration file. Most Linux distributions place it as /etc/mysql/my.cnf or /etc/my.cnf , but the location and name depends on the particular installation. Note that this is not the only way of configuring the server, some systems does not have one and rely on the command options to start the server and its default values.","title":"my.cnf"},{"location":"glossary.html#myisam","text":"Previous default storage engine for MySQL for versions prior to 5.5. It doesn\u2019t fully support transactions but in some scenarios may be faster than InnoDB. Each table is stored on disk in 3 files: .frm, .MYD, .MYI.","title":"MyISAM"},{"location":"glossary.html#physical-backup","text":"A backup that copies the data files.","title":"physical backup"},{"location":"glossary.html#point-in-time-recovery","text":"This method allows data to be restored to the state it was in any selected point of time.","title":"point in time recovery"},{"location":"glossary.html#prepared-backup","text":"A consistent set of backup data that is ready to be restored.","title":"prepared backup"},{"location":"glossary.html#restore","text":"Copies the database backups taken using the backup command to the original location or a different location. A restore returns data that has been either lost, corrupted, or stolen to the original condition at a specific point in time.","title":"restore"},{"location":"glossary.html#xbcrypt","text":"To support encryption and decryption of the backups, a new tool xbcrypt was introduced to Percona XtraBackup . This utility has been modeled after the xbstream binary to perform encryption and decryption outside of Percona XtraBackup .","title":"xbcrypt"},{"location":"glossary.html#xbstream","text":"To support simultaneous compression and streaming, Percona XtraBackup uses the xbstream format. For more information, see --stream","title":"xbstream"},{"location":"glossary.html#xtradb","text":"Percona XtraDB is an enhanced version of the InnoDB storage engine, designed to better scale on modern hardware. Percona XtraDB includes a variety of other features useful in high performance environments. It is fully backwards compatible, and so can be used as a drop-in replacement for standard InnoDB. More information here .","title":"XtraDB"},{"location":"how-tos.html","text":"How-tos and Recipes \u00b6 Recipes for innobackupex \u00b6 Make a Local Full Backup (Create, Prepare and Restore) Make a Streaming Backup Making an Incremental Backup Making a Compressed Backup Backing Up and Restoring Individual Partitions Recipes for xtrabackup \u00b6 Making a Full Backup Making an Incremental Backup Restoring the Backup How-Tos \u00b6 How to setup a replica for replication in 6 simple steps with Percona XtraBackup Verifying Backups with replication and pt-checksum How to create a new (or repair a broken) GTID based slave Auxiliary Guides \u00b6 Enabling the server to communicate via TCP/IP Privileges and Permissions for Users Installing and configuring a SSH server Assumptions in this section \u00b6 The context should make the recipe or tutorial understandable. To assure that this is true, a list of the assumptions, names and other objects that appears in this section. This items are specified at the beginning of each recipe or tutorial. HOST A system with a MySQL -based server installed, configured and running. We assume the following about this system: The MySQL server is able to communicate with others by the standard TCP/IP port ; An SSH server is installed and configured - see here if it is not; You have an user account in the system with the appropriate permissions You have a MySQL\u2019s user account with appropriate Connection and Privileges Needed . USER This is a user account with shell access and the appropriate permissions for the task. A guide for checking them is here . DB-USER This is a user account in the database server with the appropriate privileges for the task. A guide for checking them is here .","title":"How-tos and Recipes"},{"location":"how-tos.html#how-tos-and-recipes","text":"","title":"How-tos and Recipes"},{"location":"how-tos.html#recipes-for-innobackupex","text":"Make a Local Full Backup (Create, Prepare and Restore) Make a Streaming Backup Making an Incremental Backup Making a Compressed Backup Backing Up and Restoring Individual Partitions","title":"Recipes for innobackupex"},{"location":"how-tos.html#recipes-for-xtrabackup","text":"Making a Full Backup Making an Incremental Backup Restoring the Backup","title":"Recipes for xtrabackup"},{"location":"how-tos.html#how-tos","text":"How to setup a replica for replication in 6 simple steps with Percona XtraBackup Verifying Backups with replication and pt-checksum How to create a new (or repair a broken) GTID based slave","title":"How-Tos"},{"location":"how-tos.html#auxiliary-guides","text":"Enabling the server to communicate via TCP/IP Privileges and Permissions for Users Installing and configuring a SSH server","title":"Auxiliary Guides"},{"location":"how-tos.html#assumptions-in-this-section","text":"The context should make the recipe or tutorial understandable. To assure that this is true, a list of the assumptions, names and other objects that appears in this section. This items are specified at the beginning of each recipe or tutorial. HOST A system with a MySQL -based server installed, configured and running. We assume the following about this system: The MySQL server is able to communicate with others by the standard TCP/IP port ; An SSH server is installed and configured - see here if it is not; You have an user account in the system with the appropriate permissions You have a MySQL\u2019s user account with appropriate Connection and Privileges Needed . USER This is a user account with shell access and the appropriate permissions for the task. A guide for checking them is here . DB-USER This is a user account in the database server with the appropriate privileges for the task. A guide for checking them is here .","title":"Assumptions in this section"},{"location":"how_xtrabackup_works.html","text":"How Percona XtraBackup Works \u00b6 Percona XtraBackup is based on InnoDB\u2019s crash-recovery functionality. It copies your InnoDB data files, which results in data that is internally inconsistent; but then it performs crash recovery on the files to make them a consistent, usable database again. This works because InnoDB maintains a redo log, also called the transaction log. This contains a record of every change to InnoDB data. When InnoDB starts, it inspects the data files and the transaction log, and performs two steps. It applies committed transaction log entries to the data files, and it performs an undo operation on any transactions that modified data but did not commit. Percona XtraBackup works by remembering the log sequence number (LSN) when it starts, and then copying away the data files. It takes some time to do this, so if the files are changing, then they reflect the state of the database at different points in time. At the same time, Percona XtraBackup runs a background process that watches the transaction log files, and copies changes from it. Percona XtraBackup needs to do this continually because the transaction logs are written in a round-robin fashion, and can be reused. Percona XtraBackup needs the transaction log records for every change to the data files since it began execution. Percona XtraBackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. When backup locks are supported by the server, xtrabackup will first copy InnoDB data, run the LOCK TABLES FOR BACKUP and copy the MyISAM tables and .frm files. Once this is done, the backup of the files will begin. It will backup .frm , .MRG , .MYD , .MYI , .TRG , .TRN , .ARM , .ARZ , .CSM , .CSV , .par , and .opt files. Note Locking is done only for MyISAM and other non-InnoDB tables, and only after Percona XtraBackup is finished backing up all InnoDB/XtraDB data and logs. Percona XtraBackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. After that, xtrabackup will use LOCK BINLOG FOR BACKUP to block all operations that might change either the binary log position or Exec_Master_Log_Pos or Exec_Gtid_Set (i.e. source binary log coordinates corresponding to the current SQL thread state on a replication replica) as reported by SHOW MASTER/SLAVE STATUS . xtrabackup will then finish copying the REDO log files and fetch the binary log coordinates. After this is completed xtrabackup will unlock the binary log and tables. Finally, the binary log position will be printed to STDERR and xtrabackup exits, returning 0 if all went OK. Note that the STDERR of xtrabackup is not written in any file. You will have to redirect it to a file, e.g., xtrabackup OPTIONS 2> backupout.log . It will also create the following files in the directory of the backup. During the prepare phase, Percona XtraBackup performs crash recovery against the copied data files, using the copied transaction log file. After this is done, the database is ready to restore and use. The backed-up MyISAM and InnoDB tables will be eventually consistent with each other, because after the prepare (recovery) process, InnoDB \u2019s data is rolled forward to the point at which the backup completed, not rolled back to the point at which it started. This point in time matches where the FLUSH TABLES WITH READ LOCK was taken, so the MyISAM data and the prepared InnoDB data are in sync. The xtrabackup and innobackupex tools both offer many features not mentioned in the preceding explanation. Each tool\u2019s functionality is explained in more detail further in the manual. In brief, though, the tools permit you to do operations such as streaming and incremental backups with various combinations of copying the data files, copying the log files, and applying the logs to the data. Restoring a backup \u00b6 To restore a backup with xtrabackup you can use the xtrabackup --copy-back or xtrabackup --move-back options. xtrabackup will read from the my.cnf the variables datadir , innodb_data_home_dir , innodb_data_file_path , innodb_log_group_home_dir and check that the directories exist. It will copy the MyISAM tables, indexes, etc. ( .frm , .MRG , .MYD , .MYI , .TRG , .TRN , .ARM , .ARZ , .CSM , .CSV , par and .opt files) first, InnoDB tables and indexes next and the log files at last. It will preserve file\u2019s attributes when copying them, you may have to change the files\u2019 ownership to mysql before starting the database server, as they will be owned by the user who created the backup. Alternatively, the xtrabackup --move-back option may be used to restore a backup. This option is similar to xtrabackup --copy-back with the only difference that instead of copying files it moves them to their target locations. As this option removes backup files, use it with caution. It is useful when there is not enough free disk space to hold both the data files and their backup copies.","title":"How Percona XtraBackup Works"},{"location":"how_xtrabackup_works.html#how-percona-xtrabackup-works","text":"Percona XtraBackup is based on InnoDB\u2019s crash-recovery functionality. It copies your InnoDB data files, which results in data that is internally inconsistent; but then it performs crash recovery on the files to make them a consistent, usable database again. This works because InnoDB maintains a redo log, also called the transaction log. This contains a record of every change to InnoDB data. When InnoDB starts, it inspects the data files and the transaction log, and performs two steps. It applies committed transaction log entries to the data files, and it performs an undo operation on any transactions that modified data but did not commit. Percona XtraBackup works by remembering the log sequence number (LSN) when it starts, and then copying away the data files. It takes some time to do this, so if the files are changing, then they reflect the state of the database at different points in time. At the same time, Percona XtraBackup runs a background process that watches the transaction log files, and copies changes from it. Percona XtraBackup needs to do this continually because the transaction logs are written in a round-robin fashion, and can be reused. Percona XtraBackup needs the transaction log records for every change to the data files since it began execution. Percona XtraBackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. When backup locks are supported by the server, xtrabackup will first copy InnoDB data, run the LOCK TABLES FOR BACKUP and copy the MyISAM tables and .frm files. Once this is done, the backup of the files will begin. It will backup .frm , .MRG , .MYD , .MYI , .TRG , .TRN , .ARM , .ARZ , .CSM , .CSV , .par , and .opt files. Note Locking is done only for MyISAM and other non-InnoDB tables, and only after Percona XtraBackup is finished backing up all InnoDB/XtraDB data and logs. Percona XtraBackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. After that, xtrabackup will use LOCK BINLOG FOR BACKUP to block all operations that might change either the binary log position or Exec_Master_Log_Pos or Exec_Gtid_Set (i.e. source binary log coordinates corresponding to the current SQL thread state on a replication replica) as reported by SHOW MASTER/SLAVE STATUS . xtrabackup will then finish copying the REDO log files and fetch the binary log coordinates. After this is completed xtrabackup will unlock the binary log and tables. Finally, the binary log position will be printed to STDERR and xtrabackup exits, returning 0 if all went OK. Note that the STDERR of xtrabackup is not written in any file. You will have to redirect it to a file, e.g., xtrabackup OPTIONS 2> backupout.log . It will also create the following files in the directory of the backup. During the prepare phase, Percona XtraBackup performs crash recovery against the copied data files, using the copied transaction log file. After this is done, the database is ready to restore and use. The backed-up MyISAM and InnoDB tables will be eventually consistent with each other, because after the prepare (recovery) process, InnoDB \u2019s data is rolled forward to the point at which the backup completed, not rolled back to the point at which it started. This point in time matches where the FLUSH TABLES WITH READ LOCK was taken, so the MyISAM data and the prepared InnoDB data are in sync. The xtrabackup and innobackupex tools both offer many features not mentioned in the preceding explanation. Each tool\u2019s functionality is explained in more detail further in the manual. In brief, though, the tools permit you to do operations such as streaming and incremental backups with various combinations of copying the data files, copying the log files, and applying the logs to the data.","title":"How Percona XtraBackup Works"},{"location":"how_xtrabackup_works.html#restoring-a-backup","text":"To restore a backup with xtrabackup you can use the xtrabackup --copy-back or xtrabackup --move-back options. xtrabackup will read from the my.cnf the variables datadir , innodb_data_home_dir , innodb_data_file_path , innodb_log_group_home_dir and check that the directories exist. It will copy the MyISAM tables, indexes, etc. ( .frm , .MRG , .MYD , .MYI , .TRG , .TRN , .ARM , .ARZ , .CSM , .CSV , par and .opt files) first, InnoDB tables and indexes next and the log files at last. It will preserve file\u2019s attributes when copying them, you may have to change the files\u2019 ownership to mysql before starting the database server, as they will be owned by the user who created the backup. Alternatively, the xtrabackup --move-back option may be used to restore a backup. This option is similar to xtrabackup --copy-back with the only difference that instead of copying files it moves them to their target locations. As this option removes backup files, use it with caution. It is useful when there is not enough free disk space to hold both the data files and their backup copies.","title":"Restoring a backup"},{"location":"intro.html","text":"About Percona XtraBackup \u00b6 Percona XtraBackup is the world\u2019s only open-source, free MySQL hot backup software that performs non-blocking backups for InnoDB and XtraDB databases. With Percona XtraBackup , you can achieve the following benefits: Backups that complete quickly and reliably Uninterrupted transaction processing during backups Savings on disk space and network bandwidth Automatic backup verification Higher uptime due to faster restore time See the compatibility matrix in Percona Software and Platform Lifecycle to find out which versions of MySQL, MariaDB, and Percona Server for MySQL are supported by Percona XtraBackup and supports encryption with any kind of backups. Non-blocking backups of InnoDB, Percona XtraDB Cluster, and HailDB storage engines are supported. In addition, Percona XtraBackup can back up the following storage engines by briefly pausing writes at the end of the backup: MyISAM, Merge <.MRG> , and Archive <.ARM> , including partitioned tables, triggers, and database options. InnoDB tables are still locked while copying non-InnoDB data. Fast incremental backups are supported for Percona Server with Percona XtraDB Cluster changed page tracking enabled. Important Percona XtraBackup 2.4 only supports Percona XtraDB Cluster 5.7. Percona XtraBackup 2.4 does not support the MyRocks storage engine or TokuDB storage engine. Percona XtraBackup is not compatible with MariaDB 10.3 and later. Percona\u2019s enterprise-grade commercial MySQL Support contracts include support for Percona XtraBackup . We recommend support for critical production deployments. What are the features of Percona XtraBackup? \u00b6 Here is a short list of Percona XtraBackup features. See the documentation for more. Create hot InnoDB backups without pausing your database Make incremental backups of MySQL Stream compressed MySQL backups to another server Move tables between MySQL servers on-line Create new MySQL replication replicas easily Backup MySQL without adding load to the server Backup locks are a lightweight alternative to FLUSH TABLES WITH READ LOCK available in Percona Server 5.6+. Percona XtraBackup uses them automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Percona XtraBackup performs throttling based on the number of IO operations per second. Percona XtraBackup skips secondary index pages and recreates them when a compact backup is prepared. Percona XtraBackup can export individual tables even from a full backup, regardless of the InnoDB version. Tables exported with Percona XtraBackup can be imported into Percona Server 5.1, 5.5 or 5.6+, or MySQL 5.6+.","title":"About Percona XtraBackup"},{"location":"intro.html#about-percona-xtrabackup","text":"Percona XtraBackup is the world\u2019s only open-source, free MySQL hot backup software that performs non-blocking backups for InnoDB and XtraDB databases. With Percona XtraBackup , you can achieve the following benefits: Backups that complete quickly and reliably Uninterrupted transaction processing during backups Savings on disk space and network bandwidth Automatic backup verification Higher uptime due to faster restore time See the compatibility matrix in Percona Software and Platform Lifecycle to find out which versions of MySQL, MariaDB, and Percona Server for MySQL are supported by Percona XtraBackup and supports encryption with any kind of backups. Non-blocking backups of InnoDB, Percona XtraDB Cluster, and HailDB storage engines are supported. In addition, Percona XtraBackup can back up the following storage engines by briefly pausing writes at the end of the backup: MyISAM, Merge <.MRG> , and Archive <.ARM> , including partitioned tables, triggers, and database options. InnoDB tables are still locked while copying non-InnoDB data. Fast incremental backups are supported for Percona Server with Percona XtraDB Cluster changed page tracking enabled. Important Percona XtraBackup 2.4 only supports Percona XtraDB Cluster 5.7. Percona XtraBackup 2.4 does not support the MyRocks storage engine or TokuDB storage engine. Percona XtraBackup is not compatible with MariaDB 10.3 and later. Percona\u2019s enterprise-grade commercial MySQL Support contracts include support for Percona XtraBackup . We recommend support for critical production deployments.","title":"About Percona XtraBackup"},{"location":"intro.html#what-are-the-features-of-percona-xtrabackup","text":"Here is a short list of Percona XtraBackup features. See the documentation for more. Create hot InnoDB backups without pausing your database Make incremental backups of MySQL Stream compressed MySQL backups to another server Move tables between MySQL servers on-line Create new MySQL replication replicas easily Backup MySQL without adding load to the server Backup locks are a lightweight alternative to FLUSH TABLES WITH READ LOCK available in Percona Server 5.6+. Percona XtraBackup uses them automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Percona XtraBackup performs throttling based on the number of IO operations per second. Percona XtraBackup skips secondary index pages and recreates them when a compact backup is prepared. Percona XtraBackup can export individual tables even from a full backup, regardless of the InnoDB version. Tables exported with Percona XtraBackup can be imported into Percona Server 5.1, 5.5 or 5.6+, or MySQL 5.6+.","title":"What are the features of Percona XtraBackup?"},{"location":"known_issues.html","text":"Known issues and limitations \u00b6 There is a number of Percona XtraBackup related issues with compressed InnoDB tables. These issues result from either server-side bugs, or OS configuration and thus, cannot be fixed on the Percona XtraBackup side. Known issues: For MySQL or Percona Server for MySQL versions 5.1 and 5.5 there are known and unfixed bugs with redo-logging of updates to compressed InnoDB tables. For example, internal Oracle bug #16267120 has been fixed only in MySQL 5.6.12, but not in 5.1 or 5.5. The bug is about compressed page images not being logged on page reorganization and thus, creating a possibility for recovery process to fail in case a different zlib version is being used when replaying a MLOG_ZIP_PAGE_REORGANIZE redo log record. For MySQL or Percona Server for MySQL version 5.6 it is NOT recommended to set innodb_log_compressed_pages=OFF for servers that use compressed InnoDB tables which are backed up with Percona XtraBackup . This option makes InnoDB recovery (and thus, backup prepare) sensible to zlib versions. In case the host where a backup prepare is performed uses a different zlib version than the one that was used by the server during runtime, backup prepare may fail due to differences in compression algorithms. Backed-up table data could not be recovered if backup was taken while running OPTIMIZE TABLE (bug https://bugs.launchpad.net/percona-xtrabackup/+bug/1541763 ) or ALTER TABLE ... TABLESPACE (bug PXB-1360 ) on that table. Compact Backups currently don\u2019t work due to bug PXB-372 . Backup fails with Error 24: 'Too many open files' . This usually happens when database being backed up contains large amount of files and Percona XtraBackup can\u2019t open all of them to create a successful backup. In order to avoid this error the operating system should be configured appropriately so that Percona XtraBackup can open all its files. On Linux, this can be done with the ulimit command for specific backup session or by editing the /etc/security/limits.conf to change it globally ( NOTE : the maximum possible value that can be set up is 1048576 which is a hard-coded constant in the Linux kernel). The xtrabackup binary has some limitations you should be aware of to ensure that your backups go smoothly and are recoverable. Limitations: The Aria storage engine is part of MariaDB and has been integrated in it for many years and Aria table files backup support has been added to innobackupex in 2011. The issue is that the engine uses recovery log files and an aria_log_control file that are not backed up by xtrabackup . As stated in the documentation , starting MariaDB without the maria_log_control file will mark all the Aria tables as corrupted with this error when doing a CHECK on the table: Table is from another system and must be zerofilled or repaired to be usable on this system . This means that the Aria tables from an xtrabackup backup must be repaired before being usable (this could ben quite long depending on the size of the table). Another option is aria_chk --zerofil table on all Aria tables present on the backup after the prepare phase. If the xtrabackup_logfile is larger than 4GB, the --prepare step will fail on 32-bit versions of xtrabackup . xtrabackup doesn\u2019t understand the very old --set-variable my.cnf syntax that MySQL uses. See Configuring xtrabackup .","title":"Known issues and limitations"},{"location":"known_issues.html#known-issues-and-limitations","text":"There is a number of Percona XtraBackup related issues with compressed InnoDB tables. These issues result from either server-side bugs, or OS configuration and thus, cannot be fixed on the Percona XtraBackup side. Known issues: For MySQL or Percona Server for MySQL versions 5.1 and 5.5 there are known and unfixed bugs with redo-logging of updates to compressed InnoDB tables. For example, internal Oracle bug #16267120 has been fixed only in MySQL 5.6.12, but not in 5.1 or 5.5. The bug is about compressed page images not being logged on page reorganization and thus, creating a possibility for recovery process to fail in case a different zlib version is being used when replaying a MLOG_ZIP_PAGE_REORGANIZE redo log record. For MySQL or Percona Server for MySQL version 5.6 it is NOT recommended to set innodb_log_compressed_pages=OFF for servers that use compressed InnoDB tables which are backed up with Percona XtraBackup . This option makes InnoDB recovery (and thus, backup prepare) sensible to zlib versions. In case the host where a backup prepare is performed uses a different zlib version than the one that was used by the server during runtime, backup prepare may fail due to differences in compression algorithms. Backed-up table data could not be recovered if backup was taken while running OPTIMIZE TABLE (bug https://bugs.launchpad.net/percona-xtrabackup/+bug/1541763 ) or ALTER TABLE ... TABLESPACE (bug PXB-1360 ) on that table. Compact Backups currently don\u2019t work due to bug PXB-372 . Backup fails with Error 24: 'Too many open files' . This usually happens when database being backed up contains large amount of files and Percona XtraBackup can\u2019t open all of them to create a successful backup. In order to avoid this error the operating system should be configured appropriately so that Percona XtraBackup can open all its files. On Linux, this can be done with the ulimit command for specific backup session or by editing the /etc/security/limits.conf to change it globally ( NOTE : the maximum possible value that can be set up is 1048576 which is a hard-coded constant in the Linux kernel). The xtrabackup binary has some limitations you should be aware of to ensure that your backups go smoothly and are recoverable. Limitations: The Aria storage engine is part of MariaDB and has been integrated in it for many years and Aria table files backup support has been added to innobackupex in 2011. The issue is that the engine uses recovery log files and an aria_log_control file that are not backed up by xtrabackup . As stated in the documentation , starting MariaDB without the maria_log_control file will mark all the Aria tables as corrupted with this error when doing a CHECK on the table: Table is from another system and must be zerofilled or repaired to be usable on this system . This means that the Aria tables from an xtrabackup backup must be repaired before being usable (this could ben quite long depending on the size of the table). Another option is aria_chk --zerofil table on all Aria tables present on the backup after the prepare phase. If the xtrabackup_logfile is larger than 4GB, the --prepare step will fail on 32-bit versions of xtrabackup . xtrabackup doesn\u2019t understand the very old --set-variable my.cnf syntax that MySQL uses. See Configuring xtrabackup .","title":"Known issues and limitations"},{"location":"manual.html","text":"Percona XtraBackup User Manual \u00b6 Percona XtraBackup is a set of following tools: innobackupex innobackupex is the symlink for *xtrabackup*. innobackupex still supports all features and syntax as 2.2 version did, but is now deprecated and will be removed in next major release. xtrabackup a compiled *C* binary that provides functionality to backup a whole *MySQL* database instance with *MyISAM*, *InnoDB*, and XtraDB tables. xbcrypt utility used for encrypting and decrypting backup files. xbstream utility that allows streaming and extracting files to/from the xbstream format. xbcloud utility used for downloading and uploading full or part of xbstream archive from/to cloud. After Percona XtraBackup 2.3 release the recommend way to take the backup is using the xtrabackup script. More information on script options can be found in how to use xtrabackup .","title":"Percona XtraBackup User Manual"},{"location":"manual.html#percona-xtrabackup-user-manual","text":"Percona XtraBackup is a set of following tools: innobackupex innobackupex is the symlink for *xtrabackup*. innobackupex still supports all features and syntax as 2.2 version did, but is now deprecated and will be removed in next major release. xtrabackup a compiled *C* binary that provides functionality to backup a whole *MySQL* database instance with *MyISAM*, *InnoDB*, and XtraDB tables. xbcrypt utility used for encrypting and decrypting backup files. xbstream utility that allows streaming and extracting files to/from the xbstream format. xbcloud utility used for downloading and uploading full or part of xbstream archive from/to cloud. After Percona XtraBackup 2.3 release the recommend way to take the backup is using the xtrabackup script. More information on script options can be found in how to use xtrabackup .","title":"Percona XtraBackup User Manual"},{"location":"release-notes.html","text":"Percona XtraBackup 2.4 Release Notes \u00b6 Percona XtraBackup 2.4.26 (2022-05-09) Percona XtraBackup 2.4.25 (2022-04-16) Percona XtraBackup 2.4.24 (2021-09-14) Percona XtraBackup 2.4.23 (2021-06-22) Percona XtraBackup 2.4.22 (2021-03-22) Percona XtraBackup 2.4.21 (2020-11-12) Percona XtraBackup 2.4.20 (2020-04-14) Percona XtraBackup 2.4.19 (2020-03-25) Percona XtraBackup 2.4.18 (2019-12-16) Percona XtraBackup 2.4.17 (2019-12-09) Percona XtraBackup 2.4.16 (2019-11-04) Percona XtraBackup 2.4.15 (2019-07-10) Percona XtraBackup 2.4.14 (2019-05-01) Percona XtraBackup 2.4.13 (2019-01-18) Percona XtraBackup 2.4.12 (2018-06-22) Percona XtraBackup 2.4.11 (2018-04-23) Percona XtraBackup 2.4.10 (2018-03-30) Percona XtraBackup 2.4.9 (2017-11-29) Percona XtraBackup 2.4.8 (2017-07-24) Percona XtraBackup 2.4.7-2 (2017-05-29) Percona XtraBackup 2.4.7 (2017-04-17) Percona XtraBackup 2.4.6 (2017-02-22) Percona XtraBackup 2.4.5 (2016-11-29) Percona XtraBackup 2.4.4 (2016-07-25) Percona XtraBackup 2.4.3 (2016-05-23) Percona XtraBackup 2.4.2 (2016-04-01) Percona XtraBackup 2.4.1 (2016-02-16)","title":"Percona XtraBackup 2.4 Release Notes"},{"location":"release-notes.html#percona-xtrabackup-24-release-notes","text":"Percona XtraBackup 2.4.26 (2022-05-09) Percona XtraBackup 2.4.25 (2022-04-16) Percona XtraBackup 2.4.24 (2021-09-14) Percona XtraBackup 2.4.23 (2021-06-22) Percona XtraBackup 2.4.22 (2021-03-22) Percona XtraBackup 2.4.21 (2020-11-12) Percona XtraBackup 2.4.20 (2020-04-14) Percona XtraBackup 2.4.19 (2020-03-25) Percona XtraBackup 2.4.18 (2019-12-16) Percona XtraBackup 2.4.17 (2019-12-09) Percona XtraBackup 2.4.16 (2019-11-04) Percona XtraBackup 2.4.15 (2019-07-10) Percona XtraBackup 2.4.14 (2019-05-01) Percona XtraBackup 2.4.13 (2019-01-18) Percona XtraBackup 2.4.12 (2018-06-22) Percona XtraBackup 2.4.11 (2018-04-23) Percona XtraBackup 2.4.10 (2018-03-30) Percona XtraBackup 2.4.9 (2017-11-29) Percona XtraBackup 2.4.8 (2017-07-24) Percona XtraBackup 2.4.7-2 (2017-05-29) Percona XtraBackup 2.4.7 (2017-04-17) Percona XtraBackup 2.4.6 (2017-02-22) Percona XtraBackup 2.4.5 (2016-11-29) Percona XtraBackup 2.4.4 (2016-07-25) Percona XtraBackup 2.4.3 (2016-05-23) Percona XtraBackup 2.4.2 (2016-04-01) Percona XtraBackup 2.4.1 (2016-02-16)","title":"Percona XtraBackup 2.4 Release Notes"},{"location":"trademark-policy.html","text":"Trademark policy \u00b6 This Trademark Policy is to ensure that users of Percona-branded products or services know that what they receive has really been developed, approved, tested and maintained by Percona. Trademarks help to prevent confusion in the marketplace, by distinguishing one company\u2019s or person\u2019s products and services from another\u2019s. Percona owns a number of marks, including but not limited to Percona, XtraDB, Percona XtraDB, XtraBackup, Percona XtraBackup, Percona Server, and Percona Live, plus the distinctive visual icons and logos associated with these marks. Both the unregistered and registered marks of Percona are protected. Use of any Percona trademark in the name, URL, or other identifying characteristic of any product, service, website, or other use is not permitted without Percona\u2019s written permission with the following three limited exceptions. First , you may use the appropriate Percona mark when making a nominative fair use reference to a bona fide Percona product. Second , when Percona has released a product under a version of the GNU General Public License (\u201cGPL\u201d), you may use the appropriate Percona mark when distributing a verbatim copy of that product in accordance with the terms and conditions of the GPL. Third , you may use the appropriate Percona mark to refer to a distribution of GPL-released Percona software that has been modified with minor changes for the sole purpose of allowing the software to operate on an operating system or hardware platform for which Percona has not yet released the software, provided that those third party changes do not affect the behavior, functionality, features, design or performance of the software. Users who acquire this Percona-branded software receive substantially exact implementations of the Percona software. Percona reserves the right to revoke this authorization at any time in its sole discretion. For example, if Percona believes that your modification is beyond the scope of the limited license granted in this Policy or that your use of the Percona mark is detrimental to Percona, Percona will revoke this authorization. Upon revocation, you must immediately cease using the applicable Percona mark. If you do not immediately cease using the Percona mark upon revocation, Percona may take action to protect its rights and interests in the Percona mark. Percona does not grant any license to use any Percona mark for any other modified versions of Percona software; such use will require our prior written permission. Neither trademark law nor any of the exceptions set forth in this Trademark Policy permit you to truncate, modify or otherwise use any Percona mark as part of your own brand. For example, if XYZ creates a modified version of the Percona Server, XYZ may not brand that modification as \u201cXYZ Percona Server\u201d or \u201cPercona XYZ Server\u201d, even if that modification otherwise complies with the third exception noted above. In all cases, you must comply with applicable law, the underlying license, and this Trademark Policy, as amended from time to time. For instance, any mention of Percona trademarks should include the full trademarked name, with proper spelling and capitalization, along with attribution of ownership to Percona LLC and/or its affiliates. For example, the full proper name for XtraBackup is Percona XtraBackup. However, it is acceptable to omit the word \u201cPercona\u201d for brevity on the second and subsequent uses, where such omission does not cause confusion. In the event of doubt as to any of the conditions or exceptions outlined in this Trademark Policy, please contact trademarks@percona.com for assistance and we will do our very best to be helpful.","title":"Trademark policy"},{"location":"trademark-policy.html#trademark-policy","text":"This Trademark Policy is to ensure that users of Percona-branded products or services know that what they receive has really been developed, approved, tested and maintained by Percona. Trademarks help to prevent confusion in the marketplace, by distinguishing one company\u2019s or person\u2019s products and services from another\u2019s. Percona owns a number of marks, including but not limited to Percona, XtraDB, Percona XtraDB, XtraBackup, Percona XtraBackup, Percona Server, and Percona Live, plus the distinctive visual icons and logos associated with these marks. Both the unregistered and registered marks of Percona are protected. Use of any Percona trademark in the name, URL, or other identifying characteristic of any product, service, website, or other use is not permitted without Percona\u2019s written permission with the following three limited exceptions. First , you may use the appropriate Percona mark when making a nominative fair use reference to a bona fide Percona product. Second , when Percona has released a product under a version of the GNU General Public License (\u201cGPL\u201d), you may use the appropriate Percona mark when distributing a verbatim copy of that product in accordance with the terms and conditions of the GPL. Third , you may use the appropriate Percona mark to refer to a distribution of GPL-released Percona software that has been modified with minor changes for the sole purpose of allowing the software to operate on an operating system or hardware platform for which Percona has not yet released the software, provided that those third party changes do not affect the behavior, functionality, features, design or performance of the software. Users who acquire this Percona-branded software receive substantially exact implementations of the Percona software. Percona reserves the right to revoke this authorization at any time in its sole discretion. For example, if Percona believes that your modification is beyond the scope of the limited license granted in this Policy or that your use of the Percona mark is detrimental to Percona, Percona will revoke this authorization. Upon revocation, you must immediately cease using the applicable Percona mark. If you do not immediately cease using the Percona mark upon revocation, Percona may take action to protect its rights and interests in the Percona mark. Percona does not grant any license to use any Percona mark for any other modified versions of Percona software; such use will require our prior written permission. Neither trademark law nor any of the exceptions set forth in this Trademark Policy permit you to truncate, modify or otherwise use any Percona mark as part of your own brand. For example, if XYZ creates a modified version of the Percona Server, XYZ may not brand that modification as \u201cXYZ Percona Server\u201d or \u201cPercona XYZ Server\u201d, even if that modification otherwise complies with the third exception noted above. In all cases, you must comply with applicable law, the underlying license, and this Trademark Policy, as amended from time to time. For instance, any mention of Percona trademarks should include the full trademarked name, with proper spelling and capitalization, along with attribution of ownership to Percona LLC and/or its affiliates. For example, the full proper name for XtraBackup is Percona XtraBackup. However, it is acceptable to omit the word \u201cPercona\u201d for brevity on the second and subsequent uses, where such omission does not cause confusion. In the event of doubt as to any of the conditions or exceptions outlined in this Trademark Policy, please contact trademarks@percona.com for assistance and we will do our very best to be helpful.","title":"Trademark policy"},{"location":"version-check.html","text":"Version Checking \u00b6 Some Percona software contains \u201cversion checking\u201d functionality which is a feature that enables Percona software users to be notified of available software updates to improve your environment security and performance. Alongside this, the version check functionality also provides Percona with information relating to which software versions you are running, coupled with the environment confirmation which the software is running within. This helps enable Percona to focus our development effort accordingly based on trends within our customer community. The purpose of this document is to articulate the information that is collected, as well as to provide guidance on how to disable this functionality if desired. Usage \u00b6 Version Check was implemented in Percona Toolkit 2.1.4, and was enabled by default in version 2.2.1. Currently it is supported as a --[no]version-check option by a number of tools in Percona Toolkit , Percona XtraBackup, and Percona Monitoring and Management (PMM). When launched with Version Check enabled, the tool that supports this feature connects to a Percona\u2019s version check service via a secure HTTPS channel. It compares the locally installed version for possible updates, and also checks versions of the following software: Operating System Percona Monitoring and Management (PMM) MySQL Perl MySQL driver for Perl (DBD::mysql) Percona Toolkit Then it checks for and warns about versions with known problems if they are identified as running in the environment. Each version check request is logged by the server. Stored information consists of the checked system unique ID followed by the software name and version. The ID is generated either at installation or when the version checking query is submitted for the first time. Note Prior to version 3.0.7 of Percona Toolkit, the system ID was calculated as an MD5 hash of a hostname, and starting from Percona Toolkit 3.0.7 it is generated as an MD5 hash of a random number. Percona XtraBackup continues to use hostname-based MD5 hash. As a result, the content of the sent query is as follows: 85624f3fb5d2af8816178ea1493ed41a;DBD::mysql;4.044 c2b6d625ef3409164cbf8af4985c48d3;MySQL;MySQL Community Server (GPL) 5.7.22-log 85624f3fb5d2af8816178ea1493ed41a;OS;Manjaro Linux 85624f3fb5d2af8816178ea1493ed41a;Percona::Toolkit;3.0.11-dev 85624f3fb5d2af8816178ea1493ed41a;Perl;5.26.2 Disabling Version Check \u00b6 Although the version checking feature does not collect any personal information, you might prefer to disable this feature, either one time or permanently. To disable it one time, use --no-version-check option when invoking the tool from a Percona product which supports it. Here is a simple example which shows running pt-diskstats tool from the Percona Toolkit with version checking turned off: pt-diskstats --no-version-check Disabling version checking permanently can be done by placing no-version-check option into the configuration file of a Percona product (see correspondent documentation for exact file name and syntax). For example, in case of Percona Toolkit this can be done in a global configuration file /etc/percona-toolkit/percona-toolkit.conf : # Disable Version Check for all tools: no-version-check In case of Percona XtraBackup this can be done in its configuration file in a similar way: [xtrabackup] no-version-check Frequently Asked Questions \u00b6 Why is this functionality enabled by default? \u00b6 We believe having this functionality enabled improves security and performance of environments running Percona Software and it is good choice for majority of the users. Why not rely on Operating System\u2019s built in functionality for software updates? \u00b6 In many environments the Operating Systems repositories may not carry the latest version of software and newer versions of software often installed manually, so not being covered by operating system wide check for updates. Why do you send more information than just the version of software being run as a part of version check service? \u00b6 Compatibility problems can be caused by versions of various components in the environment, for example problematic versions of Perl, DBD or MySQL could cause operational problems with Percona Toolkit.","title":"Version Checking"},{"location":"version-check.html#version-checking","text":"Some Percona software contains \u201cversion checking\u201d functionality which is a feature that enables Percona software users to be notified of available software updates to improve your environment security and performance. Alongside this, the version check functionality also provides Percona with information relating to which software versions you are running, coupled with the environment confirmation which the software is running within. This helps enable Percona to focus our development effort accordingly based on trends within our customer community. The purpose of this document is to articulate the information that is collected, as well as to provide guidance on how to disable this functionality if desired.","title":"Version Checking"},{"location":"version-check.html#usage","text":"Version Check was implemented in Percona Toolkit 2.1.4, and was enabled by default in version 2.2.1. Currently it is supported as a --[no]version-check option by a number of tools in Percona Toolkit , Percona XtraBackup, and Percona Monitoring and Management (PMM). When launched with Version Check enabled, the tool that supports this feature connects to a Percona\u2019s version check service via a secure HTTPS channel. It compares the locally installed version for possible updates, and also checks versions of the following software: Operating System Percona Monitoring and Management (PMM) MySQL Perl MySQL driver for Perl (DBD::mysql) Percona Toolkit Then it checks for and warns about versions with known problems if they are identified as running in the environment. Each version check request is logged by the server. Stored information consists of the checked system unique ID followed by the software name and version. The ID is generated either at installation or when the version checking query is submitted for the first time. Note Prior to version 3.0.7 of Percona Toolkit, the system ID was calculated as an MD5 hash of a hostname, and starting from Percona Toolkit 3.0.7 it is generated as an MD5 hash of a random number. Percona XtraBackup continues to use hostname-based MD5 hash. As a result, the content of the sent query is as follows: 85624f3fb5d2af8816178ea1493ed41a;DBD::mysql;4.044 c2b6d625ef3409164cbf8af4985c48d3;MySQL;MySQL Community Server (GPL) 5.7.22-log 85624f3fb5d2af8816178ea1493ed41a;OS;Manjaro Linux 85624f3fb5d2af8816178ea1493ed41a;Percona::Toolkit;3.0.11-dev 85624f3fb5d2af8816178ea1493ed41a;Perl;5.26.2","title":"Usage"},{"location":"version-check.html#disabling-version-check","text":"Although the version checking feature does not collect any personal information, you might prefer to disable this feature, either one time or permanently. To disable it one time, use --no-version-check option when invoking the tool from a Percona product which supports it. Here is a simple example which shows running pt-diskstats tool from the Percona Toolkit with version checking turned off: pt-diskstats --no-version-check Disabling version checking permanently can be done by placing no-version-check option into the configuration file of a Percona product (see correspondent documentation for exact file name and syntax). For example, in case of Percona Toolkit this can be done in a global configuration file /etc/percona-toolkit/percona-toolkit.conf : # Disable Version Check for all tools: no-version-check In case of Percona XtraBackup this can be done in its configuration file in a similar way: [xtrabackup] no-version-check","title":"Disabling Version Check"},{"location":"version-check.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"version-check.html#why-is-this-functionality-enabled-by-default","text":"We believe having this functionality enabled improves security and performance of environments running Percona Software and it is good choice for majority of the users.","title":"Why is this functionality enabled by default?"},{"location":"version-check.html#why-not-rely-on-operating-systems-built-in-functionality-for-software-updates","text":"In many environments the Operating Systems repositories may not carry the latest version of software and newer versions of software often installed manually, so not being covered by operating system wide check for updates.","title":"Why not rely on Operating System\u2019s built in functionality for software updates?"},{"location":"version-check.html#why-do-you-send-more-information-than-just-the-version-of-software-being-run-as-a-part-of-version-check-service","text":"Compatibility problems can be caused by versions of various components in the environment, for example problematic versions of Perl, DBD or MySQL could cause operational problems with Percona Toolkit.","title":"Why do you send more information than just the version of software being run as a part of version check service?"},{"location":"xtrabackup-files.html","text":"Index of files created by Percona XtraBackup \u00b6 Information related to the backup and the server backup-my.cnf This file contains information to start the mini instance of InnoDB during the xtrabackup --prepare . This is NOT a backup of original my.cnf . The InnoDB configuration is read from the file backup-my.cnf created by innobackupex when the backup was made. xtrabackup --prepare uses InnoDB configuration from backup-my.cnf by default, or from xtrabackup --defaults-file , if specified. InnoDB configuration in this context means server variables that affect data format, i.e. innodb_page_size option, innodb_log_block_size , etc. Location-related variables, like innodb_log_group_home_dir or innodb_data_file_path are always ignored by xtrabackup --prepare , so preparing a backup always works with data files from the backup directory, rather than any external ones. xtrabackup_checkpoints The type of the backup (e.g. full or incremental), its state (e.g. prepared) and the LSN range contained in it. This information is used for incremental backups. Example of the xtrabackup_checkpoints after taking a full backup: backup_type = full-backuped from_lsn = 0 to_lsn = 15188961605 last_lsn = 15188961605 Example of the xtrabackup_checkpoints after taking an incremental backup: backup_type = incremental from_lsn = 15188961605 to_lsn = 15189350111 last_lsn = 15189350111 xtrabackup_binlog_info The binary log file used by the server and its position at the moment of the backup. Result of the SHOW MASTER STATUS . xtrabackup_binlog_pos_innodb The binary log file and its current position for InnoDB or XtraDB tables. xtrabackup_binary The xtrabackup binary used in the process. xtrabackup_logfile Contains data needed for running the: xtrabackup --prepare . The bigger this file is the xtrabackup --prepare process will take longer to finish. <table_name>.delta.meta This file is going to be created when performing the incremental backup. It contains the per-table delta metadata: page size, size of compressed page (if the value is 0 it means the tablespace isn\u2019t compressed) and space id. Example of this file could looks like this: page_size = 16384 zip_size = 0 space_id = 0 Information related to the replication environment (if using the xtrabackup --slave-info option): xtrabackup_slave_info The CHANGE MASTER statement needed for setting up a slave. Information related to the Galera and Percona XtraDB Cluster (if using the xtrabackup --galera-info option): xtrabackup_galera_info Contains the values of wsrep_local_state_uuid and wsrep_last_committed status variables","title":"Index of files created by Percona XtraBackup"},{"location":"xtrabackup-files.html#index-of-files-created-by-percona-xtrabackup","text":"Information related to the backup and the server backup-my.cnf This file contains information to start the mini instance of InnoDB during the xtrabackup --prepare . This is NOT a backup of original my.cnf . The InnoDB configuration is read from the file backup-my.cnf created by innobackupex when the backup was made. xtrabackup --prepare uses InnoDB configuration from backup-my.cnf by default, or from xtrabackup --defaults-file , if specified. InnoDB configuration in this context means server variables that affect data format, i.e. innodb_page_size option, innodb_log_block_size , etc. Location-related variables, like innodb_log_group_home_dir or innodb_data_file_path are always ignored by xtrabackup --prepare , so preparing a backup always works with data files from the backup directory, rather than any external ones. xtrabackup_checkpoints The type of the backup (e.g. full or incremental), its state (e.g. prepared) and the LSN range contained in it. This information is used for incremental backups. Example of the xtrabackup_checkpoints after taking a full backup: backup_type = full-backuped from_lsn = 0 to_lsn = 15188961605 last_lsn = 15188961605 Example of the xtrabackup_checkpoints after taking an incremental backup: backup_type = incremental from_lsn = 15188961605 to_lsn = 15189350111 last_lsn = 15189350111 xtrabackup_binlog_info The binary log file used by the server and its position at the moment of the backup. Result of the SHOW MASTER STATUS . xtrabackup_binlog_pos_innodb The binary log file and its current position for InnoDB or XtraDB tables. xtrabackup_binary The xtrabackup binary used in the process. xtrabackup_logfile Contains data needed for running the: xtrabackup --prepare . The bigger this file is the xtrabackup --prepare process will take longer to finish. <table_name>.delta.meta This file is going to be created when performing the incremental backup. It contains the per-table delta metadata: page size, size of compressed page (if the value is 0 it means the tablespace isn\u2019t compressed) and space id. Example of this file could looks like this: page_size = 16384 zip_size = 0 space_id = 0 Information related to the replication environment (if using the xtrabackup --slave-info option): xtrabackup_slave_info The CHANGE MASTER statement needed for setting up a slave. Information related to the Galera and Percona XtraDB Cluster (if using the xtrabackup --galera-info option): xtrabackup_galera_info Contains the values of wsrep_local_state_uuid and wsrep_last_committed status variables","title":"Index of files created by Percona XtraBackup"},{"location":"advanced/encrypted_innodb_tablespace_backups.html","text":"Encrypted InnoDB Tablespace Backups \u00b6 As of MySQL 5.7.11, InnoDB supports data encryption for InnoDB tables stored in file-per-table tablespaces. This feature provides at-rest encryption for physical tablespace data files. For authenticated user or application to access encrypted tablespace, InnoDB will use master encryption key to decrypt the tablespace key. The master encryption key is stored in a keyring. Two keyring plugins supported by xtrabackup are keyring_file and keyring_vault . These plugins are installed into the plugin directory. Making a Backup Using keyring_file Plugin \u00b6 Support for encrypted InnoDB tablespace backups with keyring_file has been implemented in Percona XtraBackup 2.4.2 by implementing xtrabackup --keyring-file-data option (and also xtrabackup --server-id option, needed for MySQL prior to 5.7.13). These options are only recognized by xtrabackup binary i.e., innobackupex will not be able to backup and prepare encrypted tablespaces. Creating Backup \u00b6 In order to backup and prepare database containing encrypted InnoDB tablespaces, you must specify the path to keyring file by using the xtrabackup --keyring-file-data option. $ xtrabackup --backup --target-dir = /data/backup/ --user = root \\ --keyring-file-data = /var/lib/mysql-keyring/keyring With MySQL prior to 5.7.13, use xtrabackup --server-id in the backup creation command: $ xtrabackup --backup --target-dir = /data/backup/ --user = root \\ --keyring-file-data = /var/lib/mysql-keyring/keyring --server-id = 1 After xtrabackup is finished taking the backup you should see the following message: xtrabackup: Transaction log of lsn (5696709) to (5696718) was copied. 160401 10:25:51 completed OK! Warning xtrabackup will not copy keyring file into the backup directory. In order to be prepare the backup, you must make a copy of keyring file yourself. Preparing Backup \u00b6 In order to prepare the backup you\u2019ll need to specify the keyring-file-data (server-id is stored in backup-my.cnf file, so it can be omitted when preparing the backup, regardless of the MySQL version used). $ xtrabackup --prepare --target-dir = /data/backup \\ --keyring-file-data = /var/lib/mysql-keyring/keyring After xtrabackup is finished preparing the backup you should see the following message: InnoDB: Shutdown completed; log sequence number 5697064 160401 10:34:28 completed OK! Backup is now prepared and can be restored with xtrabackup \u2013copy-back option. In case the keyring has been rotated you\u2019ll need to restore the keyring which was used to take and prepare the backup. Making Backup Using keyring_vault Plugin \u00b6 Support for encrypted InnoDB tablespace backups with keyring_vault has been implemented in Percona XtraBackup 2.4.11. Keyring vault plugin settings are described here . Creating Backup \u00b6 The following command creates a backup in the /data/backup directory: $ xtrabackup --backup --target-dir = /data/backup --user = root After xtrabackup completes taking the backup you should see the following message: xtrabackup: Transaction log of lsn (5696709) to (5696718) was copied. 160401 10:25:51 completed OK! Preparing the Backup \u00b6 In order to prepare the backup xtrabackup will need an access to the keyring. Since xtrabackup doesn\u2019t talk to MySQL server and doesn\u2019t read default my.cnf configuration file during prepare, user will need to specify keyring settings via the command line: $ xtrabackup --prepare --target-dir = /data/backup \\ --keyring-vault-config = /etc/vault.cnf See also Data at Rest Encryption for Percona Server [keyring vault plugin settings] (https://www.percona.com/doc/percona-server/LATEST/management/data_at_rest_encryption.html#keyring-vault-plugin). After xtrabackup completes preparing the backup you should see the following message: InnoDB: Shutdown completed; log sequence number 5697064 160401 10:34:28 completed OK! The backup is now prepared and can be restored with xtrabackup --copy-back option: $ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql Incremental Encrypted InnoDB Tablespace Backups with keyring_file \u00b6 The process of taking incremental backups with InnoDB tablespace encryption is similar to taking the Incremental Backups with unencrypted tablespace. Creating an Incremental Backup \u00b6 To make an incremental backup, begin with a full backup. The xtrabackup binary writes a file called xtrabackup_checkpoints into the backup\u2019s target directory. This file contains a line showing the to_lsn , which is the database\u2019s LSN at the end of the backup. First you need to create a full backup with the following command: $ xtrabackup --backup --target-dir = /data/backups/base \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Warning xtrabackup will not copy keyring file into the backup directory. In order to be prepare the backup, you must make a copy of keyring file yourself. If you try to restore the backup after the keyring has been changed you\u2019ll see errors like ERROR 3185 (HY000): Can't find master key from keyring, please check keyring plugin is loaded. when trying to access encrypted table. If you look at the xtrabackup_checkpoints file, you should see some contents similar to the following: backup_type = full-backuped from_lsn = 0 to_lsn = 7666625 last_lsn = 7666634 compact = 0 recover_binlog_info = 1 Now that you have a full backup, you can make an incremental backup based on it. Use a command such as the following: $ xtrabackup --backup --target-dir = /data/backups/inc1 \\ --incremental-basedir = /data/backups/base \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Warning xtrabackup will not copy keyring file into the backup directory. In order to be prepare the backup, you must make a copy of keyring file yourself. If the keyring hasn\u2019t been rotated you can use the same as the one you\u2019ve backed-up with the base backup. If the keyring has been rotated you\u2019ll need to back it up otherwise you won\u2019t be able to prepare the backup. The /data/backups/inc1/ directory should now contain delta files, such as ibdata1.delta and test/table1.ibd.delta . These represent the changes since the LSN 7666625 . If you examine the xtrabackup_checkpoints file in this directory, you should see something similar to the following: backup_type = incremental from_lsn = 7666625 to_lsn = 8873920 last_lsn = 8873929 compact = 0 recover_binlog_info = 1 The meaning should be self-evident. It\u2019s now possible to use this directory as the base for yet another incremental backup: $ xtrabackup --backup --target-dir = /data/backups/inc2 \\ --incremental-basedir = /data/backups/inc1 \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Preparing the Incremental Backups \u00b6 The xtrabackup --prepare step for incremental backups is not the same as for normal backups. In normal backups, two types of operations are performed to make the database consistent: committed transactions are replayed from the log file against the data files, and uncommitted transactions are rolled back. You must skip the rollback of uncommitted transactions when preparing a backup, because transactions that were uncommitted at the time of your backup may be in progress, and it\u2019s likely that they will be committed in the next incremental backup. You should use the xtrabackup --apply-log-only option to prevent the rollback phase. Warning If you do not use the xtrabackup --apply-log-only option to prevent the rollback phase, then your incremental backups will be useless. After transactions have been rolled back, further incremental backups cannot be applied. Beginning with the full backup you created, you can prepare it, and then apply the incremental differences to it. Recall that you have the following backups: /data/backups/base /data/backups/inc1 /data/backups/inc2 To prepare the base backup, you need to run xtrabackup --prepare as usual, but prevent the rollback phase: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base \\ --keyring-file-data = /var/lib/mysql-keyring/keyring The output should end with some text such as the following: InnoDB: Shutdown completed; log sequence number 7666643 InnoDB: Number of pools: 1 160401 12:31:11 completed OK! To apply the first incremental backup to the full backup, you should use the following command: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc1 \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Warning Backup should be prepared with the keyring that was used when backup was being taken. This means that if the keyring has been rotated between the base and incremental backup that you\u2019ll need to use the keyring that was in use when the first incremental backup has been taken. Preparing the second incremental backup is a similar process: apply the deltas to the (modified) base backup, and you will roll its data forward in time to the point of the second incremental backup: $ xtrabackup --prepare --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc2 \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Note xtrabackup --apply-log-only should be used when merging each incremental backup except the last one. That\u2019s why the previous line doesn\u2019t contain xtrabackup --apply-log-only . Even if the xtrabackup --apply-log-only were used on the last step, backup would still be consistent but in that case server would perform the rollback phase. The backup is now prepared and can be restored with xtrabackup --copy-back . In case the keyring has been rotated you\u2019ll need to restore the keyring which was used to take and prepare the backup. Restoring backup when keyring is not available \u00b6 While described restore method works, it requires an access to the same keyring which server is using. It may not be possible if backup is prepared on different server or at the much later time, when keys in the keyring have been purged, or in case of malfunction when keyring vault server is not available at all. A xtrabackup --transition-key should be used to make it possible for xtrabackup to process the backup without access to the keyring vault server. In this case xtrabackup will derive AES encryption key from specified passphrase and will use it to encrypt tablespace keys of tablespaces being backed up. Creating the Backup with a Passphrase \u00b6 The following example illustrates how a backup can be created in this case: $ xtrabackup --backup --user = root -p --target-dir = /data/backup \\ --transition-key = MySecretKey If xtrabackup --transition-key is specified without a value, xtrabackup will ask for it. Note xtrabackup --transition-key scrapes the supplied value so that it does not appear in the ps command output. Preparing the Backup with a Passphrase \u00b6 The same passphrase should be specified for the prepare command: $ xtrabackup --prepare --target-dir = /data/backup \\ --transition-key = MySecretKey There is no keyring-vault or keyring-file here, because xtrabackup does not talk to the keyring in this case. Restoring the Backup with a Generated Key \u00b6 When restoring a backup you will need to generate new master key. Here is the example for keyring_file : $ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql \\ --transition-key = MySecetKey --generate-new-master-key \\ --keyring-file-data = /var/lib/mysql-keyring/keyring In case of keyring_vault it will look like this: $ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql \\ --transition-key = MySecetKey --generate-new-master-key \\ --keyring-vault-config = /etc/vault.cnf xtrabackup will generate new master key, store it into target keyring vault server and re-encrypt tablespace keys using this key. Making the Backup with a Stored Transition Key \u00b6 Finally, there is an option to store transition key in the keyring. In this case xtrabackup will need access to the same keyring file or vault server during prepare and copy-back, but does not depend on whether the server keys have been purged. The three stages of the backup process are the following: Backup \u00b6 $ xtrabackup --backup --user = root -p --target-dir = /data/backup \\ --generate-transition-key Prepare \u00b6 keyring_file variant \u00b6 $ xtrabackup --prepare --target-dir = /data/backup \\ --keyring-file-data = /var/lib/mysql-keyring/keyring keyring_vault variant \u00b6 $ xtrabackup --prepare --target-dir = /data/backup \\ --keyring-vault-config = /etc/vault.cnf Copy-back \u00b6 keyring_file variant \u00b6 $ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql \\ --generate-new-master-key --keyring-file-data = /var/lib/mysql-keyring/keyring keyring_vault variant \u00b6 $ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql \\ --generate-new-master-key --keyring-vault-config = /etc/vault.cnf","title":"Encrypted InnoDB Tablespace Backups"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#encrypted-innodb-tablespace-backups","text":"As of MySQL 5.7.11, InnoDB supports data encryption for InnoDB tables stored in file-per-table tablespaces. This feature provides at-rest encryption for physical tablespace data files. For authenticated user or application to access encrypted tablespace, InnoDB will use master encryption key to decrypt the tablespace key. The master encryption key is stored in a keyring. Two keyring plugins supported by xtrabackup are keyring_file and keyring_vault . These plugins are installed into the plugin directory.","title":"Encrypted InnoDB Tablespace Backups"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#making-a-backup-using-keyring_file-plugin","text":"Support for encrypted InnoDB tablespace backups with keyring_file has been implemented in Percona XtraBackup 2.4.2 by implementing xtrabackup --keyring-file-data option (and also xtrabackup --server-id option, needed for MySQL prior to 5.7.13). These options are only recognized by xtrabackup binary i.e., innobackupex will not be able to backup and prepare encrypted tablespaces.","title":"Making a Backup Using keyring_file Plugin"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#creating-backup","text":"In order to backup and prepare database containing encrypted InnoDB tablespaces, you must specify the path to keyring file by using the xtrabackup --keyring-file-data option. $ xtrabackup --backup --target-dir = /data/backup/ --user = root \\ --keyring-file-data = /var/lib/mysql-keyring/keyring With MySQL prior to 5.7.13, use xtrabackup --server-id in the backup creation command: $ xtrabackup --backup --target-dir = /data/backup/ --user = root \\ --keyring-file-data = /var/lib/mysql-keyring/keyring --server-id = 1 After xtrabackup is finished taking the backup you should see the following message: xtrabackup: Transaction log of lsn (5696709) to (5696718) was copied. 160401 10:25:51 completed OK! Warning xtrabackup will not copy keyring file into the backup directory. In order to be prepare the backup, you must make a copy of keyring file yourself.","title":"Creating Backup"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#preparing-backup","text":"In order to prepare the backup you\u2019ll need to specify the keyring-file-data (server-id is stored in backup-my.cnf file, so it can be omitted when preparing the backup, regardless of the MySQL version used). $ xtrabackup --prepare --target-dir = /data/backup \\ --keyring-file-data = /var/lib/mysql-keyring/keyring After xtrabackup is finished preparing the backup you should see the following message: InnoDB: Shutdown completed; log sequence number 5697064 160401 10:34:28 completed OK! Backup is now prepared and can be restored with xtrabackup \u2013copy-back option. In case the keyring has been rotated you\u2019ll need to restore the keyring which was used to take and prepare the backup.","title":"Preparing Backup"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#making-backup-using-keyring_vault-plugin","text":"Support for encrypted InnoDB tablespace backups with keyring_vault has been implemented in Percona XtraBackup 2.4.11. Keyring vault plugin settings are described here .","title":"Making Backup Using keyring_vault Plugin"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#creating-backup_1","text":"The following command creates a backup in the /data/backup directory: $ xtrabackup --backup --target-dir = /data/backup --user = root After xtrabackup completes taking the backup you should see the following message: xtrabackup: Transaction log of lsn (5696709) to (5696718) was copied. 160401 10:25:51 completed OK!","title":"Creating Backup"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#preparing-the-backup","text":"In order to prepare the backup xtrabackup will need an access to the keyring. Since xtrabackup doesn\u2019t talk to MySQL server and doesn\u2019t read default my.cnf configuration file during prepare, user will need to specify keyring settings via the command line: $ xtrabackup --prepare --target-dir = /data/backup \\ --keyring-vault-config = /etc/vault.cnf See also Data at Rest Encryption for Percona Server [keyring vault plugin settings] (https://www.percona.com/doc/percona-server/LATEST/management/data_at_rest_encryption.html#keyring-vault-plugin). After xtrabackup completes preparing the backup you should see the following message: InnoDB: Shutdown completed; log sequence number 5697064 160401 10:34:28 completed OK! The backup is now prepared and can be restored with xtrabackup --copy-back option: $ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql","title":"Preparing the Backup"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#incremental-encrypted-innodb-tablespace-backups-with-keyring_file","text":"The process of taking incremental backups with InnoDB tablespace encryption is similar to taking the Incremental Backups with unencrypted tablespace.","title":"Incremental Encrypted InnoDB Tablespace Backups with keyring_file"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#creating-an-incremental-backup","text":"To make an incremental backup, begin with a full backup. The xtrabackup binary writes a file called xtrabackup_checkpoints into the backup\u2019s target directory. This file contains a line showing the to_lsn , which is the database\u2019s LSN at the end of the backup. First you need to create a full backup with the following command: $ xtrabackup --backup --target-dir = /data/backups/base \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Warning xtrabackup will not copy keyring file into the backup directory. In order to be prepare the backup, you must make a copy of keyring file yourself. If you try to restore the backup after the keyring has been changed you\u2019ll see errors like ERROR 3185 (HY000): Can't find master key from keyring, please check keyring plugin is loaded. when trying to access encrypted table. If you look at the xtrabackup_checkpoints file, you should see some contents similar to the following: backup_type = full-backuped from_lsn = 0 to_lsn = 7666625 last_lsn = 7666634 compact = 0 recover_binlog_info = 1 Now that you have a full backup, you can make an incremental backup based on it. Use a command such as the following: $ xtrabackup --backup --target-dir = /data/backups/inc1 \\ --incremental-basedir = /data/backups/base \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Warning xtrabackup will not copy keyring file into the backup directory. In order to be prepare the backup, you must make a copy of keyring file yourself. If the keyring hasn\u2019t been rotated you can use the same as the one you\u2019ve backed-up with the base backup. If the keyring has been rotated you\u2019ll need to back it up otherwise you won\u2019t be able to prepare the backup. The /data/backups/inc1/ directory should now contain delta files, such as ibdata1.delta and test/table1.ibd.delta . These represent the changes since the LSN 7666625 . If you examine the xtrabackup_checkpoints file in this directory, you should see something similar to the following: backup_type = incremental from_lsn = 7666625 to_lsn = 8873920 last_lsn = 8873929 compact = 0 recover_binlog_info = 1 The meaning should be self-evident. It\u2019s now possible to use this directory as the base for yet another incremental backup: $ xtrabackup --backup --target-dir = /data/backups/inc2 \\ --incremental-basedir = /data/backups/inc1 \\ --keyring-file-data = /var/lib/mysql-keyring/keyring","title":"Creating an Incremental Backup"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#preparing-the-incremental-backups","text":"The xtrabackup --prepare step for incremental backups is not the same as for normal backups. In normal backups, two types of operations are performed to make the database consistent: committed transactions are replayed from the log file against the data files, and uncommitted transactions are rolled back. You must skip the rollback of uncommitted transactions when preparing a backup, because transactions that were uncommitted at the time of your backup may be in progress, and it\u2019s likely that they will be committed in the next incremental backup. You should use the xtrabackup --apply-log-only option to prevent the rollback phase. Warning If you do not use the xtrabackup --apply-log-only option to prevent the rollback phase, then your incremental backups will be useless. After transactions have been rolled back, further incremental backups cannot be applied. Beginning with the full backup you created, you can prepare it, and then apply the incremental differences to it. Recall that you have the following backups: /data/backups/base /data/backups/inc1 /data/backups/inc2 To prepare the base backup, you need to run xtrabackup --prepare as usual, but prevent the rollback phase: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base \\ --keyring-file-data = /var/lib/mysql-keyring/keyring The output should end with some text such as the following: InnoDB: Shutdown completed; log sequence number 7666643 InnoDB: Number of pools: 1 160401 12:31:11 completed OK! To apply the first incremental backup to the full backup, you should use the following command: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc1 \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Warning Backup should be prepared with the keyring that was used when backup was being taken. This means that if the keyring has been rotated between the base and incremental backup that you\u2019ll need to use the keyring that was in use when the first incremental backup has been taken. Preparing the second incremental backup is a similar process: apply the deltas to the (modified) base backup, and you will roll its data forward in time to the point of the second incremental backup: $ xtrabackup --prepare --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc2 \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Note xtrabackup --apply-log-only should be used when merging each incremental backup except the last one. That\u2019s why the previous line doesn\u2019t contain xtrabackup --apply-log-only . Even if the xtrabackup --apply-log-only were used on the last step, backup would still be consistent but in that case server would perform the rollback phase. The backup is now prepared and can be restored with xtrabackup --copy-back . In case the keyring has been rotated you\u2019ll need to restore the keyring which was used to take and prepare the backup.","title":"Preparing the Incremental Backups"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#restoring-backup-when-keyring-is-not-available","text":"While described restore method works, it requires an access to the same keyring which server is using. It may not be possible if backup is prepared on different server or at the much later time, when keys in the keyring have been purged, or in case of malfunction when keyring vault server is not available at all. A xtrabackup --transition-key should be used to make it possible for xtrabackup to process the backup without access to the keyring vault server. In this case xtrabackup will derive AES encryption key from specified passphrase and will use it to encrypt tablespace keys of tablespaces being backed up.","title":"Restoring backup when keyring is not available"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#creating-the-backup-with-a-passphrase","text":"The following example illustrates how a backup can be created in this case: $ xtrabackup --backup --user = root -p --target-dir = /data/backup \\ --transition-key = MySecretKey If xtrabackup --transition-key is specified without a value, xtrabackup will ask for it. Note xtrabackup --transition-key scrapes the supplied value so that it does not appear in the ps command output.","title":"Creating the Backup with a Passphrase"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#preparing-the-backup-with-a-passphrase","text":"The same passphrase should be specified for the prepare command: $ xtrabackup --prepare --target-dir = /data/backup \\ --transition-key = MySecretKey There is no keyring-vault or keyring-file here, because xtrabackup does not talk to the keyring in this case.","title":"Preparing the Backup with a Passphrase"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#restoring-the-backup-with-a-generated-key","text":"When restoring a backup you will need to generate new master key. Here is the example for keyring_file : $ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql \\ --transition-key = MySecetKey --generate-new-master-key \\ --keyring-file-data = /var/lib/mysql-keyring/keyring In case of keyring_vault it will look like this: $ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql \\ --transition-key = MySecetKey --generate-new-master-key \\ --keyring-vault-config = /etc/vault.cnf xtrabackup will generate new master key, store it into target keyring vault server and re-encrypt tablespace keys using this key.","title":"Restoring the Backup with a Generated Key"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#making-the-backup-with-a-stored-transition-key","text":"Finally, there is an option to store transition key in the keyring. In this case xtrabackup will need access to the same keyring file or vault server during prepare and copy-back, but does not depend on whether the server keys have been purged. The three stages of the backup process are the following:","title":"Making the Backup with a Stored Transition Key"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#backup","text":"$ xtrabackup --backup --user = root -p --target-dir = /data/backup \\ --generate-transition-key","title":"Backup"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#prepare","text":"","title":"Prepare"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#keyring_file-variant","text":"$ xtrabackup --prepare --target-dir = /data/backup \\ --keyring-file-data = /var/lib/mysql-keyring/keyring","title":"keyring_file variant"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#keyring_vault-variant","text":"$ xtrabackup --prepare --target-dir = /data/backup \\ --keyring-vault-config = /etc/vault.cnf","title":"keyring_vault variant"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#copy-back","text":"","title":"Copy-back"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#keyring_file-variant_1","text":"$ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql \\ --generate-new-master-key --keyring-file-data = /var/lib/mysql-keyring/keyring","title":"keyring_file variant"},{"location":"advanced/encrypted_innodb_tablespace_backups.html#keyring_vault-variant_1","text":"$ xtrabackup --copy-back --target-dir = /data/backup --datadir = /data/mysql \\ --generate-new-master-key --keyring-vault-config = /etc/vault.cnf","title":"keyring_vault variant"},{"location":"advanced/lockless_bin-log.html","text":"Lockless binary log information \u00b6 This feature is exclusive to Percona Server for MySQL starting with version 5.6.26-74.0. It is also used in Percona XtraDB Cluster when the node is being backed up without xtrabackup --galera-info . When the Lockless binary log information feature is available on the server, Percona XtraBackup can trust binary log information stored in the InnoDB system header and avoid executing LOCK BINLOG FOR BACKUP (and thus, blocking commits for the duration of finalizing the REDO log copy) under a number of circumstances: when the server is not a GTID-enabled Galera cluster node when the replication I/O thread information should not be stored as a part of the backup (i.e. when the xtrabackup --slave-info option is not specified) If all of the above conditions hold, Percona XtraBackup does not execute the SHOW MASTER STATUS as a part of the backup procedure, does not create the xtrabackup_binlog_info file on backup. Instead, that information is retrieved and the file is created after preparing the backup, along with creating xtrabackup_binlog_pos_innodb , which in this case contains exactly the same information as in xtrabackup_binlog_info and is thus redundant. To make this new functionality configurable, there is now a new Percona XtraBackup option, xtrabackup --binlog-info , which can accept the following values: OFF - This means that Percona XtraBackup will not attempt to retrieve the binary log information at all, neither during the backup creation, nor after preparing it. This can help when a user just wants to copy data without any meta information like binary log or replication coordinates. In this case, xtrabackup --binlog-info=OFF can be passed to Percona XtraBackup and LOCK BINLOG FOR BACKUP will not be executed, even if the backup-safe binlog info feature is not provided by the server (but the backup locks feature is still a requirement). ON - This matches the old behavior, i.e. the one before this Percona XtraBackup feature had been implemented. When specified, Percona XtraBackup retrieves the binary log information and uses LOCK BINLOG FOR BACKUP (if available) to ensure its consistency. LOCKLESS - This corresponds to the functionality explained above: Percona XtraBackup will not retrieve binary log information during the backup process, will not execute LOCK BINLOG FOR BACKUP , and the xtrabackup_binlog_info file will not be created. The file will be created after preparing the backup using the information stored in the InnoDB system header. If the required server-side functionality is not provided by the server, specifying this xtrabackup --binlog-info value will result in an error. If one of the above mentioned conditions does not hold, LOCK BINLOG FOR BACKUP will still be executed to ensure consistency of other meta data. AUTO - This is the default value. When used, Percona XtraBackup will automatically switch to either ON or LOCKLESS , depending on the server-side feature availability, i.e., whether the have_backup_safe_binlog_info server variable is available.","title":"Lockless binary log information"},{"location":"advanced/lockless_bin-log.html#lockless-binary-log-information","text":"This feature is exclusive to Percona Server for MySQL starting with version 5.6.26-74.0. It is also used in Percona XtraDB Cluster when the node is being backed up without xtrabackup --galera-info . When the Lockless binary log information feature is available on the server, Percona XtraBackup can trust binary log information stored in the InnoDB system header and avoid executing LOCK BINLOG FOR BACKUP (and thus, blocking commits for the duration of finalizing the REDO log copy) under a number of circumstances: when the server is not a GTID-enabled Galera cluster node when the replication I/O thread information should not be stored as a part of the backup (i.e. when the xtrabackup --slave-info option is not specified) If all of the above conditions hold, Percona XtraBackup does not execute the SHOW MASTER STATUS as a part of the backup procedure, does not create the xtrabackup_binlog_info file on backup. Instead, that information is retrieved and the file is created after preparing the backup, along with creating xtrabackup_binlog_pos_innodb , which in this case contains exactly the same information as in xtrabackup_binlog_info and is thus redundant. To make this new functionality configurable, there is now a new Percona XtraBackup option, xtrabackup --binlog-info , which can accept the following values: OFF - This means that Percona XtraBackup will not attempt to retrieve the binary log information at all, neither during the backup creation, nor after preparing it. This can help when a user just wants to copy data without any meta information like binary log or replication coordinates. In this case, xtrabackup --binlog-info=OFF can be passed to Percona XtraBackup and LOCK BINLOG FOR BACKUP will not be executed, even if the backup-safe binlog info feature is not provided by the server (but the backup locks feature is still a requirement). ON - This matches the old behavior, i.e. the one before this Percona XtraBackup feature had been implemented. When specified, Percona XtraBackup retrieves the binary log information and uses LOCK BINLOG FOR BACKUP (if available) to ensure its consistency. LOCKLESS - This corresponds to the functionality explained above: Percona XtraBackup will not retrieve binary log information during the backup process, will not execute LOCK BINLOG FOR BACKUP , and the xtrabackup_binlog_info file will not be created. The file will be created after preparing the backup using the information stored in the InnoDB system header. If the required server-side functionality is not provided by the server, specifying this xtrabackup --binlog-info value will result in an error. If one of the above mentioned conditions does not hold, LOCK BINLOG FOR BACKUP will still be executed to ensure consistency of other meta data. AUTO - This is the default value. When used, Percona XtraBackup will automatically switch to either ON or LOCKLESS , depending on the server-side feature availability, i.e., whether the have_backup_safe_binlog_info server variable is available.","title":"Lockless binary log information"},{"location":"advanced/locks.html","text":"lock-ddl-per-table Option Improvements \u00b6 MySQL 5.7 has made improvements to bulk loading, one of these improvements is the ability to disable redo logging, which increased the speed of ADD INDEX operations. To block DDL statements on an instance, Percona Server implemented LOCK TABLES FOR BACKUP. Percona XtraBackup uses this lock for the duration of the backup. This lock does not affect DML statements. Percona XtraBackup has also implemented --lock-ddl-per-table , which blocks DDL statements by using metadata locks (MDL). The following procedures describe a simplified backup operation when using --lock-ddl-per-table : Parse and copy all redo logs after the checkpoint mark Fork a dedicated thread to continue following new redo log entries List the tablespaces required to copy Iterate through the list. The following steps occur with each listed tablespace: Query INFORMATION_SCHEMA.INNODB_SYS_TABLES to find which tables belong to the tablespace ID and take a MDL on the underlying table or tables in case there is a shared tablespace. Copy the tablespace .ibd files. The backup process may encounter a redo log event, generated by the bulk load operations, which notifies backup tools that data file changes have been omitted from the redo log. This event is an MLOG_INDEX_LOAD . If this event is found by the redo follow thread, the backup continues and assumes the backup is safe because the MDL protects tablespaces already copied and the MLOG_INDEX_LOAD event is for a tablespace that is not copied. These assumptions may not be correct and may lead to inconsistent backups. --lock-ddl-per-table redesign \u00b6 Implemented in Percona XtraBackup version 2.4.21, the --lock-ddl-per-table has been redesigned to minimize inconsistent backups. The following procedure reorders the steps: Acquire the MDL lock at the beginning of the backup Scan the redo logs. An MLOG_INDEX_LOAD event may be recorded if a CREATE INDEX statement has occurred before the backup starts. At this time, the backup process is safe and can parse and accept the event. After the first scan has completed, the follow redo log thread is initiated This thread stops the backup process if an MLOG_INDEX_LOAD event is found. Gather the tablespace list to copy Copy the .ibd files. Other Improvements \u00b6 The following improvements have been added: If the .ibd file belongs to a temporary table, the SELECT query is skipped. For a FullText Index, an MDL is acquired on the base table. A SELECT query that acquires an MDL retrieves no data.","title":"`lock-ddl-per-table` Option Improvements"},{"location":"advanced/locks.html#lock-ddl-per-table-option-improvements","text":"MySQL 5.7 has made improvements to bulk loading, one of these improvements is the ability to disable redo logging, which increased the speed of ADD INDEX operations. To block DDL statements on an instance, Percona Server implemented LOCK TABLES FOR BACKUP. Percona XtraBackup uses this lock for the duration of the backup. This lock does not affect DML statements. Percona XtraBackup has also implemented --lock-ddl-per-table , which blocks DDL statements by using metadata locks (MDL). The following procedures describe a simplified backup operation when using --lock-ddl-per-table : Parse and copy all redo logs after the checkpoint mark Fork a dedicated thread to continue following new redo log entries List the tablespaces required to copy Iterate through the list. The following steps occur with each listed tablespace: Query INFORMATION_SCHEMA.INNODB_SYS_TABLES to find which tables belong to the tablespace ID and take a MDL on the underlying table or tables in case there is a shared tablespace. Copy the tablespace .ibd files. The backup process may encounter a redo log event, generated by the bulk load operations, which notifies backup tools that data file changes have been omitted from the redo log. This event is an MLOG_INDEX_LOAD . If this event is found by the redo follow thread, the backup continues and assumes the backup is safe because the MDL protects tablespaces already copied and the MLOG_INDEX_LOAD event is for a tablespace that is not copied. These assumptions may not be correct and may lead to inconsistent backups.","title":"lock-ddl-per-table Option Improvements"},{"location":"advanced/locks.html#-lock-ddl-per-table-redesign","text":"Implemented in Percona XtraBackup version 2.4.21, the --lock-ddl-per-table has been redesigned to minimize inconsistent backups. The following procedure reorders the steps: Acquire the MDL lock at the beginning of the backup Scan the redo logs. An MLOG_INDEX_LOAD event may be recorded if a CREATE INDEX statement has occurred before the backup starts. At this time, the backup process is safe and can parse and accept the event. After the first scan has completed, the follow redo log thread is initiated This thread stops the backup process if an MLOG_INDEX_LOAD event is found. Gather the tablespace list to copy Copy the .ibd files.","title":"--lock-ddl-per-table redesign"},{"location":"advanced/locks.html#other-improvements","text":"The following improvements have been added: If the .ibd file belongs to a temporary table, the SELECT query is skipped. For a FullText Index, an MDL is acquired on the base table. A SELECT query that acquires an MDL retrieves no data.","title":"Other Improvements"},{"location":"advanced/throttling_backups.html","text":"Throttling Backups \u00b6 Although xtrabackup does not block your database\u2019s operation, any backup can add load to the system being backed up. On systems that do not have much spare I/O capacity, it might be helpful to throttle the rate at which xtrabackup reads and writes data. You can do this with the xtrabackup --throttle option. This option limits the number of chunks copied per second. The chunk size is 10 MB . The image below shows how throttling works when xtrabackup --throttle is set to 1. By default, there is no throttling, and xtrabackup reads and writes data as quickly as possible. If you set too strict of a limit on the IOPS, the backup may slow down so much that it will never catch up with the transaction logs that InnoDB is writing, and the backup might never be complete.","title":"Throttling Backups"},{"location":"advanced/throttling_backups.html#throttling-backups","text":"Although xtrabackup does not block your database\u2019s operation, any backup can add load to the system being backed up. On systems that do not have much spare I/O capacity, it might be helpful to throttle the rate at which xtrabackup reads and writes data. You can do this with the xtrabackup --throttle option. This option limits the number of chunks copied per second. The chunk size is 10 MB . The image below shows how throttling works when xtrabackup --throttle is set to 1. By default, there is no throttling, and xtrabackup reads and writes data as quickly as possible. If you set too strict of a limit on the IOPS, the backup may slow down so much that it will never catch up with the transaction logs that InnoDB is writing, and the backup might never be complete.","title":"Throttling Backups"},{"location":"backup_scenarios/compressed_backup.html","text":"Compressed Backup \u00b6 Percona XtraBackup supports compressed backups: a local or streaming backup can be compressed or decompressed with xbstream. Creating Compressed Backups \u00b6 In order to make a compressed backup you\u2019ll need to use the xtrabackup \u2013compress option: $ xtrabackup --backup --compress --target-dir = /data/compressed/ The xtrabackup --compress uses the qpress tool that you can install via the percona-release package configuration tool as follows: $ sudo percona-release enable tools $ sudo apt update $ sudo apt install qpress Note Enable the repository: percona-release enable-only tools release . If you intend to use Percona XtraBackup in combination with the upstream MySQL Server, you only need to enable the tools repository: percona-release enable-only tools . If you want to speed up the compression you can use the parallel compression, which can be enabled with xtrabackup \u2013compress-threads option. Following example will use four threads for compression: $ xtrabackup --backup --compress --compress-threads = 4 \\ --target-dir = /data/compressed/ Output should look like this ... 170223 13:00:38 [01] Compressing ./test/sbtest1.frm to /tmp/compressed/test/sbtest1.frm.qp 170223 13:00:38 [01] ...done 170223 13:00:38 [01] Compressing ./test/sbtest2.frm to /tmp/compressed/test/sbtest2.frm.qp 170223 13:00:38 [01] ...done ... 170223 13:00:39 [00] Compressing xtrabackup_info 170223 13:00:39 [00] ...done xtrabackup: Transaction log of lsn (9291934) to (9291934) was copied. 170223 13:00:39 completed OK! Preparing the backup \u00b6 Before you can prepare the backup you must uncompress all the files. Percona XtraBackup has implemented xtrabackup --decompress option that can be used to decompress the backup. $ xtrabackup --decompress --target-dir = /data/compressed/ Note xtrabackup --parallel can be used with xtrabackup --decompress option to decompress multiple files simultaneously. Percona XtraBackup does not automatically remove the compressed files. In order to clean up the backup directory, use the xtrabackup --remove-original option. If the files not removed they are not copied or moved to the datadir if xtrabackup --copy-back or xtrabackup --move-back are used. When the files are uncompressed you can prepare the backup with the xtrabackup --prepare option: $ xtrabackup --prepare --target-dir = /data/compressed/ Check for a confirmation message: InnoDB: Starting shutdown... InnoDB: Shutdown completed; log sequence number 9293846 170223 13:39:31 completed OK! Now the files in /data/compressed/ are ready to be used by the server. Restoring the backup \u00b6 xtrabackup has a xtrabackup --copy-back option, which performs the restoration of a backup to the server\u2019s datadir: $ xtrabackup --copy-back --target-dir = /data/backups/ The option copies all the data-related files back to the server\u2019s datadir , determined by the server\u2019s my.cnf configuration file. Check the last line of the output for a success message: 170223 13:49:13 completed OK! Verify the file permissions after copying the data back. You may need to adjust the permissions. For example, the following command changes the owner of the file location: $ chown -R mysql:mysql /var/lib/mysql Now that the datadir contains the restored data. You are ready to start the server.","title":"Compressed Backup"},{"location":"backup_scenarios/compressed_backup.html#compressed-backup","text":"Percona XtraBackup supports compressed backups: a local or streaming backup can be compressed or decompressed with xbstream.","title":"Compressed Backup"},{"location":"backup_scenarios/compressed_backup.html#creating-compressed-backups","text":"In order to make a compressed backup you\u2019ll need to use the xtrabackup \u2013compress option: $ xtrabackup --backup --compress --target-dir = /data/compressed/ The xtrabackup --compress uses the qpress tool that you can install via the percona-release package configuration tool as follows: $ sudo percona-release enable tools $ sudo apt update $ sudo apt install qpress Note Enable the repository: percona-release enable-only tools release . If you intend to use Percona XtraBackup in combination with the upstream MySQL Server, you only need to enable the tools repository: percona-release enable-only tools . If you want to speed up the compression you can use the parallel compression, which can be enabled with xtrabackup \u2013compress-threads option. Following example will use four threads for compression: $ xtrabackup --backup --compress --compress-threads = 4 \\ --target-dir = /data/compressed/ Output should look like this ... 170223 13:00:38 [01] Compressing ./test/sbtest1.frm to /tmp/compressed/test/sbtest1.frm.qp 170223 13:00:38 [01] ...done 170223 13:00:38 [01] Compressing ./test/sbtest2.frm to /tmp/compressed/test/sbtest2.frm.qp 170223 13:00:38 [01] ...done ... 170223 13:00:39 [00] Compressing xtrabackup_info 170223 13:00:39 [00] ...done xtrabackup: Transaction log of lsn (9291934) to (9291934) was copied. 170223 13:00:39 completed OK!","title":"Creating Compressed Backups"},{"location":"backup_scenarios/compressed_backup.html#preparing-the-backup","text":"Before you can prepare the backup you must uncompress all the files. Percona XtraBackup has implemented xtrabackup --decompress option that can be used to decompress the backup. $ xtrabackup --decompress --target-dir = /data/compressed/ Note xtrabackup --parallel can be used with xtrabackup --decompress option to decompress multiple files simultaneously. Percona XtraBackup does not automatically remove the compressed files. In order to clean up the backup directory, use the xtrabackup --remove-original option. If the files not removed they are not copied or moved to the datadir if xtrabackup --copy-back or xtrabackup --move-back are used. When the files are uncompressed you can prepare the backup with the xtrabackup --prepare option: $ xtrabackup --prepare --target-dir = /data/compressed/ Check for a confirmation message: InnoDB: Starting shutdown... InnoDB: Shutdown completed; log sequence number 9293846 170223 13:39:31 completed OK! Now the files in /data/compressed/ are ready to be used by the server.","title":"Preparing the backup"},{"location":"backup_scenarios/compressed_backup.html#restoring-the-backup","text":"xtrabackup has a xtrabackup --copy-back option, which performs the restoration of a backup to the server\u2019s datadir: $ xtrabackup --copy-back --target-dir = /data/backups/ The option copies all the data-related files back to the server\u2019s datadir , determined by the server\u2019s my.cnf configuration file. Check the last line of the output for a success message: 170223 13:49:13 completed OK! Verify the file permissions after copying the data back. You may need to adjust the permissions. For example, the following command changes the owner of the file location: $ chown -R mysql:mysql /var/lib/mysql Now that the datadir contains the restored data. You are ready to start the server.","title":"Restoring the backup"},{"location":"backup_scenarios/encrypted_backup.html","text":"Encrypted Backup \u00b6 Percona XtraBackup has implemented support for encrypted backups. It can be used to encrypt/decrypt local or streaming backup with xbstream option (streaming tar backups are not supported) in order to add another layer of protection to the backups. Encryption is done with the libgcrypt library. Creating Encrypted Backups \u00b6 To make an encrypted backup following options need to be specified (options xtrabackup --encrypt-key and xtrabackup --encrypt-key-file are mutually exclusive, i.e., just one of them needs to be provided): --encrypt=ALGORITHM - currently supported algorithms are: AES128 , AES192 and AES256 --encrypt-key=ENCRYPTION_KEY - proper length encryption key to use. It is not recommended to use this option where there is uncontrolled access to the machine as the command line and thus the key can be viewed as part of the process info. --encrypt-key-file=KEYFILE - the name of a file where the raw key of the appropriate length can be read from. The file must be a simple binary (or text) file that contains exactly the key to be used. Both xtrabackup --encrypt-key option and xtrabackup --encrypt-key-file option can be used to specify the encryption key. Encryption key can be generated with command like: $ openssl rand -base64 24 Example output of that command should look like this: GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs This value then can be used as the encryption key Using the --encrypt-key option \u00b6 Example of the xtrabackup command using the xtrabackup --encrypt-key should look like this: $ xtrabackup --backup --target-dir = /data/backups --encrypt = AES256 \\ --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" Using the --encrypt-key-file option \u00b6 Example of the xtrabackup command using the xtrabackup --encrypt-key-file should look like this: $ xtrabackup --backup --target-dir = /data/backups/ --encrypt = AES256 \\ --encrypt-key-file = /data/backups/keyfile Note Depending on the text editor used for making the KEYFILE , text file in some cases can contain the CRLF and this will cause the key size to grow and thus making it invalid. Suggested way to do this would be to create the file with: echo -n \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" > /data/backups/keyfile Optimizing the encryption process \u00b6 Two options have been introduced with the encrypted backups that can be used to speed up the encryption process. These are xtrabackup --encrypt-threads and xtrabackup --encrypt-chunk-size . By using the xtrabackup --encrypt-threads option multiple threads can be specified to be used for encryption in parallel. Option xtrabackup --encrypt-chunk-size can be used to specify the size (in bytes) of the working encryption buffer for each encryption thread (default is 64K). Decrypting Encrypted Backups \u00b6 Percona XtraBackup xtrabackup --decrypt option has been implemented that can be used to decrypt the backups: $ xtrabackup --decrypt = AES256 --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" \\ --target-dir = /data/backups/ Percona XtraBackup doesn\u2019t automatically remove the encrypted files. In order to clean up the backup directory users should remove the *.xbcrypt files. In Percona XtraBackup 2.4.6 xtrabackup --remove-original option has been implemented that you can use to remove the encrypted files once they\u2019ve been decrypted. To remove the files once they\u2019re decrypted you should run: $ xtrabackup --decrypt = AES256 --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" \\ --target-dir = /data/backups/ --remove-original Note xtrabackup --parallel can be used with xtrabackup --decrypt option to decrypt multiple files simultaneously. When the files have been decrypted backup can be prepared. Preparing Encrypted Backups \u00b6 After the backups have been decrypted, they can be prepared the same way as the standard full backups with the xtrabackup --prepare option: $ xtrabackup --prepare --target-dir = /data/backups/ Restoring Encrypted Backups \u00b6 xtrabackup has a xtrabackup --copy-back option, which performs the restoration of a backup to the server\u2019s datadir: $ xtrabackup --copy-back --target-dir = /data/backups/ It will copy all the data-related files back to the server\u2019s datadir , determined by the server\u2019s my.cnf configuration file. You should check the last line of the output for a success message: 170214 12:37:01 completed OK! Other Reading \u00b6 The Libgcrypt Reference Manual","title":"Encrypted Backup"},{"location":"backup_scenarios/encrypted_backup.html#encrypted-backup","text":"Percona XtraBackup has implemented support for encrypted backups. It can be used to encrypt/decrypt local or streaming backup with xbstream option (streaming tar backups are not supported) in order to add another layer of protection to the backups. Encryption is done with the libgcrypt library.","title":"Encrypted Backup"},{"location":"backup_scenarios/encrypted_backup.html#creating-encrypted-backups","text":"To make an encrypted backup following options need to be specified (options xtrabackup --encrypt-key and xtrabackup --encrypt-key-file are mutually exclusive, i.e., just one of them needs to be provided): --encrypt=ALGORITHM - currently supported algorithms are: AES128 , AES192 and AES256 --encrypt-key=ENCRYPTION_KEY - proper length encryption key to use. It is not recommended to use this option where there is uncontrolled access to the machine as the command line and thus the key can be viewed as part of the process info. --encrypt-key-file=KEYFILE - the name of a file where the raw key of the appropriate length can be read from. The file must be a simple binary (or text) file that contains exactly the key to be used. Both xtrabackup --encrypt-key option and xtrabackup --encrypt-key-file option can be used to specify the encryption key. Encryption key can be generated with command like: $ openssl rand -base64 24 Example output of that command should look like this: GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs This value then can be used as the encryption key","title":"Creating Encrypted Backups"},{"location":"backup_scenarios/encrypted_backup.html#using-the-encrypt-key-option","text":"Example of the xtrabackup command using the xtrabackup --encrypt-key should look like this: $ xtrabackup --backup --target-dir = /data/backups --encrypt = AES256 \\ --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\"","title":"Using the --encrypt-key option"},{"location":"backup_scenarios/encrypted_backup.html#using-the-encrypt-key-file-option","text":"Example of the xtrabackup command using the xtrabackup --encrypt-key-file should look like this: $ xtrabackup --backup --target-dir = /data/backups/ --encrypt = AES256 \\ --encrypt-key-file = /data/backups/keyfile Note Depending on the text editor used for making the KEYFILE , text file in some cases can contain the CRLF and this will cause the key size to grow and thus making it invalid. Suggested way to do this would be to create the file with: echo -n \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" > /data/backups/keyfile","title":"Using the --encrypt-key-file option"},{"location":"backup_scenarios/encrypted_backup.html#optimizing-the-encryption-process","text":"Two options have been introduced with the encrypted backups that can be used to speed up the encryption process. These are xtrabackup --encrypt-threads and xtrabackup --encrypt-chunk-size . By using the xtrabackup --encrypt-threads option multiple threads can be specified to be used for encryption in parallel. Option xtrabackup --encrypt-chunk-size can be used to specify the size (in bytes) of the working encryption buffer for each encryption thread (default is 64K).","title":"Optimizing the encryption process"},{"location":"backup_scenarios/encrypted_backup.html#decrypting-encrypted-backups","text":"Percona XtraBackup xtrabackup --decrypt option has been implemented that can be used to decrypt the backups: $ xtrabackup --decrypt = AES256 --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" \\ --target-dir = /data/backups/ Percona XtraBackup doesn\u2019t automatically remove the encrypted files. In order to clean up the backup directory users should remove the *.xbcrypt files. In Percona XtraBackup 2.4.6 xtrabackup --remove-original option has been implemented that you can use to remove the encrypted files once they\u2019ve been decrypted. To remove the files once they\u2019re decrypted you should run: $ xtrabackup --decrypt = AES256 --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" \\ --target-dir = /data/backups/ --remove-original Note xtrabackup --parallel can be used with xtrabackup --decrypt option to decrypt multiple files simultaneously. When the files have been decrypted backup can be prepared.","title":"Decrypting Encrypted Backups"},{"location":"backup_scenarios/encrypted_backup.html#preparing-encrypted-backups","text":"After the backups have been decrypted, they can be prepared the same way as the standard full backups with the xtrabackup --prepare option: $ xtrabackup --prepare --target-dir = /data/backups/","title":"Preparing Encrypted Backups"},{"location":"backup_scenarios/encrypted_backup.html#restoring-encrypted-backups","text":"xtrabackup has a xtrabackup --copy-back option, which performs the restoration of a backup to the server\u2019s datadir: $ xtrabackup --copy-back --target-dir = /data/backups/ It will copy all the data-related files back to the server\u2019s datadir , determined by the server\u2019s my.cnf configuration file. You should check the last line of the output for a success message: 170214 12:37:01 completed OK!","title":"Restoring Encrypted Backups"},{"location":"backup_scenarios/encrypted_backup.html#other-reading","text":"The Libgcrypt Reference Manual","title":"Other Reading"},{"location":"backup_scenarios/full_backup.html","text":"The Backup Cycle - Full Backups \u00b6 Creating a backup \u00b6 To create a backup, run xtrabackup with the xtrabackup --backup option . You also need to specify a xtrabackup --target-dir option, which is where the backup will be stored, if the InnoDB data or log files aren\u2019t stored in the same directory, you might need to specify the location of those, too. If the target directory does not exist, xtrabackup creates it. If the directory does exist and is empty, xtrabackup will succeed. xtrabackup will not overwrite existing files, it will fail with operating system error 17, file exists . To start the backup process run: $ xtrabackup --backup --target-dir = /data/backups/ This will store the backup at /data/backups/ . If you specify a relative path, the target directory will be relative to the current directory. During the backup process, you should see a lot of output showing the data files being copied, as well as the log file thread repeatedly scanning the log files and copying from it. Here is an example that shows the log thread scanning the log in the background, and a file copying thread working on the ibdata1 file: 160906 10:19:17 Finished backing up non-InnoDB tables and files 160906 10:19:17 Executing FLUSH NO_WRITE_TO_BINLOG ENGINE LOGS... xtrabackup: The latest check point (for incremental): '62988944' xtrabackup: Stopping log copying thread. .160906 10:19:18 >> log scanned up to (137343534) 160906 10:19:18 Executing UNLOCK TABLES 160906 10:19:18 All tables unlocked 160906 10:19:18 Backup created in directory '/data/backups/' 160906 10:19:18 [00] Writing backup-my.cnf 160906 10:19:18 [00] ...done 160906 10:19:18 [00] Writing xtrabackup_info 160906 10:19:18 [00] ...done xtrabackup: Transaction log of lsn (26970807) to (137343534) was copied. 160906 10:19:18 completed OK! The last thing you should see is something like the following, where the value of the <LSN> will be a number that depends on your system: xtrabackup: Transaction log of lsn (<SLN>) to (<LSN>) was copied. Note Log copying thread checks the transactional log every second to see if there were any new log records written that need to be copied, but there is a chance that the log copying thread might not be able to keep up with the amount of writes that go to the transactional logs, and will hit an error when the log records are overwritten before they could be read. After the backup is finished, the target directory will contain files such as the following, assuming you have a single InnoDB table test.tbl1 and you are using MySQL\u2019s innodb_file_per_table option: $ ls -lh /data/backups/ The results are as follows: total 182M drwx------ 7 root root 4.0K Sep 6 10:19 . drwxrwxrwt 11 root root 4.0K Sep 6 11:05 .. -rw-r----- 1 root root 387 Sep 6 10:19 backup-my.cnf -rw-r----- 1 root root 76M Sep 6 10:19 ibdata1 drwx------ 2 root root 4.0K Sep 6 10:19 mysql drwx------ 2 root root 4.0K Sep 6 10:19 performance_schema drwx------ 2 root root 4.0K Sep 6 10:19 sbtest drwx------ 2 root root 4.0K Sep 6 10:19 test drwx------ 2 root root 4.0K Sep 6 10:19 world2 -rw-r----- 1 root root 116 Sep 6 10:19 xtrabackup_checkpoints -rw-r----- 1 root root 433 Sep 6 10:19 xtrabackup_info -rw-r----- 1 root root 106M Sep 6 10:19 xtrabackup_logfile The backup can take a long time, depending on how large the database is. It is safe to cancel at any time, because it does not modify the database. The next step is getting your backup ready to be restored. Preparing a backup \u00b6 After you made a backup with the xtrabackup --backup option, you\u2019ll first need to prepare it in order to restore it. Data files are not point-in-time consistent until they\u2019ve been prepared, because they were copied at different times as the program ran, and they might have been changed while this was happening. If you try to start InnoDB with these data files, it will detect corruption and crash itself to prevent you from running on damaged data. The xtrabackup --prepare step makes the files perfectly consistent at a single instant in time, so you can run InnoDB on them. You can run the prepare operation on any machine; it does not need to be on the originating server or the server to which you intend to restore. You can copy the backup to a utility server and prepare it there. Note You can prepare a backup created with older Percona XtraBackup version with a newer one, but not vice versa. Preparing a backup on an unsupported server version should be done with the latest Percona XtraBackup release which supports that server version. For example, if one has a backup of MySQL 5.0 created with Percona XtraBackup 1.6, then preparing the backup with Percona XtraBackup 2.3 is not supported, because support for MySQL 5.0 was removed in Percona XtraBackup 2.1. Instead, the latest release in the 2.0 series should be used. During the prepare operation, xtrabackup boots up a kind of modified InnoDB that\u2019s embedded inside it (the libraries it was linked against). The modifications are necessary to disable InnoDB\u2019s standard safety checks, such as complaining that the log file isn\u2019t the right size, which aren\u2019t appropriate for working with backups. These modifications are only for the xtrabackup binary; you don\u2019t need a modified InnoDB to use xtrabackup for your backups. The prepare step uses this embedded InnoDB to perform crash recovery on the copied data files, using the copied log file. The prepare step is very simple to use: you simply run xtrabackup --prepare option and tell it which directory to prepare, for example, to prepare the previously taken backup run: $ xtrabackup --prepare --target-dir = /data/backups/ When this finishes, you should see an InnoDB shutdown with a message such as the following, where again the value of LSN will depend on your system: InnoDB: Shutdown completed; log sequence number 137345046 160906 11:21:01 completed OK! All following prepares will not change the already prepared data files, you\u2019ll see that output says: xtrabackup: This target seems to be already prepared. xtrabackup: notice: xtrabackup_logfile was already used to '--prepare'. It is not recommended to interrupt xtrabackup process while preparing backup because it may cause data files corruption and backup will become unusable. Backup validity is not guaranteed if prepare process was interrupted. Note If you intend the backup to be the basis for further incremental backups, you should use the xtrabackup --apply-log-only option when preparing the backup, or you will not be able to apply incremental backups to it. See the documentation on preparing [incremental backup] (incremental_backup.md#incremental-backup) for more details. Restoring a Backup \u00b6 Warning Backup needs to be prepared before it can be restored. $ xtrabackup --copy-back --target-dir = /data/backups/ If you don\u2019t want to save your backup, you can use the xtrabackup --move-back option which will move the backed up data to the datadir . If you don\u2019t want to use any of the above options, you can additionally use rsync or cp to restore the files. Note The datadir must be empty before restoring the backup. Also it\u2019s important to note that MySQL server needs to be shut down before restore is performed. You can\u2019t restore to a datadir of a running mysqld instance (except when importing a partial backup). Example of the rsync command that can be used to restore the backup can look like this: $ rsync -avrP /data/backup/ /var/lib/mysql/ You should check that the restored files have the correct ownership and permissions. As files\u2019 attributes will be preserved, in most cases you will need to change the files\u2019 ownership to mysql before starting the database server, as they will be owned by the user who created the backup: $ chown -R mysql:mysql /var/lib/mysql Data is now restored and you can start the server. Note When relay-log-info-repository=TABLE is enabled, the instance recovered from the backup has errors in the error log, like the following: 2019-08-09 12:40:02 69297 [ERROR] Failed to open the relay log '/data/mysql-relay-bin.004349' (relay_log_pos 5534092) To avoid these types of issues, enable relay_log_recovery or execute RESET SLAVE prior to CHANGE MASTER TO . The relay log information was backed up, but a new relay log has been created, which creates a mismatch during the restore.","title":"The Backup Cycle - Full Backups"},{"location":"backup_scenarios/full_backup.html#the-backup-cycle-full-backups","text":"","title":"The Backup Cycle - Full Backups"},{"location":"backup_scenarios/full_backup.html#creating-a-backup","text":"To create a backup, run xtrabackup with the xtrabackup --backup option . You also need to specify a xtrabackup --target-dir option, which is where the backup will be stored, if the InnoDB data or log files aren\u2019t stored in the same directory, you might need to specify the location of those, too. If the target directory does not exist, xtrabackup creates it. If the directory does exist and is empty, xtrabackup will succeed. xtrabackup will not overwrite existing files, it will fail with operating system error 17, file exists . To start the backup process run: $ xtrabackup --backup --target-dir = /data/backups/ This will store the backup at /data/backups/ . If you specify a relative path, the target directory will be relative to the current directory. During the backup process, you should see a lot of output showing the data files being copied, as well as the log file thread repeatedly scanning the log files and copying from it. Here is an example that shows the log thread scanning the log in the background, and a file copying thread working on the ibdata1 file: 160906 10:19:17 Finished backing up non-InnoDB tables and files 160906 10:19:17 Executing FLUSH NO_WRITE_TO_BINLOG ENGINE LOGS... xtrabackup: The latest check point (for incremental): '62988944' xtrabackup: Stopping log copying thread. .160906 10:19:18 >> log scanned up to (137343534) 160906 10:19:18 Executing UNLOCK TABLES 160906 10:19:18 All tables unlocked 160906 10:19:18 Backup created in directory '/data/backups/' 160906 10:19:18 [00] Writing backup-my.cnf 160906 10:19:18 [00] ...done 160906 10:19:18 [00] Writing xtrabackup_info 160906 10:19:18 [00] ...done xtrabackup: Transaction log of lsn (26970807) to (137343534) was copied. 160906 10:19:18 completed OK! The last thing you should see is something like the following, where the value of the <LSN> will be a number that depends on your system: xtrabackup: Transaction log of lsn (<SLN>) to (<LSN>) was copied. Note Log copying thread checks the transactional log every second to see if there were any new log records written that need to be copied, but there is a chance that the log copying thread might not be able to keep up with the amount of writes that go to the transactional logs, and will hit an error when the log records are overwritten before they could be read. After the backup is finished, the target directory will contain files such as the following, assuming you have a single InnoDB table test.tbl1 and you are using MySQL\u2019s innodb_file_per_table option: $ ls -lh /data/backups/ The results are as follows: total 182M drwx------ 7 root root 4.0K Sep 6 10:19 . drwxrwxrwt 11 root root 4.0K Sep 6 11:05 .. -rw-r----- 1 root root 387 Sep 6 10:19 backup-my.cnf -rw-r----- 1 root root 76M Sep 6 10:19 ibdata1 drwx------ 2 root root 4.0K Sep 6 10:19 mysql drwx------ 2 root root 4.0K Sep 6 10:19 performance_schema drwx------ 2 root root 4.0K Sep 6 10:19 sbtest drwx------ 2 root root 4.0K Sep 6 10:19 test drwx------ 2 root root 4.0K Sep 6 10:19 world2 -rw-r----- 1 root root 116 Sep 6 10:19 xtrabackup_checkpoints -rw-r----- 1 root root 433 Sep 6 10:19 xtrabackup_info -rw-r----- 1 root root 106M Sep 6 10:19 xtrabackup_logfile The backup can take a long time, depending on how large the database is. It is safe to cancel at any time, because it does not modify the database. The next step is getting your backup ready to be restored.","title":"Creating a backup"},{"location":"backup_scenarios/full_backup.html#preparing-a-backup","text":"After you made a backup with the xtrabackup --backup option, you\u2019ll first need to prepare it in order to restore it. Data files are not point-in-time consistent until they\u2019ve been prepared, because they were copied at different times as the program ran, and they might have been changed while this was happening. If you try to start InnoDB with these data files, it will detect corruption and crash itself to prevent you from running on damaged data. The xtrabackup --prepare step makes the files perfectly consistent at a single instant in time, so you can run InnoDB on them. You can run the prepare operation on any machine; it does not need to be on the originating server or the server to which you intend to restore. You can copy the backup to a utility server and prepare it there. Note You can prepare a backup created with older Percona XtraBackup version with a newer one, but not vice versa. Preparing a backup on an unsupported server version should be done with the latest Percona XtraBackup release which supports that server version. For example, if one has a backup of MySQL 5.0 created with Percona XtraBackup 1.6, then preparing the backup with Percona XtraBackup 2.3 is not supported, because support for MySQL 5.0 was removed in Percona XtraBackup 2.1. Instead, the latest release in the 2.0 series should be used. During the prepare operation, xtrabackup boots up a kind of modified InnoDB that\u2019s embedded inside it (the libraries it was linked against). The modifications are necessary to disable InnoDB\u2019s standard safety checks, such as complaining that the log file isn\u2019t the right size, which aren\u2019t appropriate for working with backups. These modifications are only for the xtrabackup binary; you don\u2019t need a modified InnoDB to use xtrabackup for your backups. The prepare step uses this embedded InnoDB to perform crash recovery on the copied data files, using the copied log file. The prepare step is very simple to use: you simply run xtrabackup --prepare option and tell it which directory to prepare, for example, to prepare the previously taken backup run: $ xtrabackup --prepare --target-dir = /data/backups/ When this finishes, you should see an InnoDB shutdown with a message such as the following, where again the value of LSN will depend on your system: InnoDB: Shutdown completed; log sequence number 137345046 160906 11:21:01 completed OK! All following prepares will not change the already prepared data files, you\u2019ll see that output says: xtrabackup: This target seems to be already prepared. xtrabackup: notice: xtrabackup_logfile was already used to '--prepare'. It is not recommended to interrupt xtrabackup process while preparing backup because it may cause data files corruption and backup will become unusable. Backup validity is not guaranteed if prepare process was interrupted. Note If you intend the backup to be the basis for further incremental backups, you should use the xtrabackup --apply-log-only option when preparing the backup, or you will not be able to apply incremental backups to it. See the documentation on preparing [incremental backup] (incremental_backup.md#incremental-backup) for more details.","title":"Preparing a backup"},{"location":"backup_scenarios/full_backup.html#restoring-a-backup","text":"Warning Backup needs to be prepared before it can be restored. $ xtrabackup --copy-back --target-dir = /data/backups/ If you don\u2019t want to save your backup, you can use the xtrabackup --move-back option which will move the backed up data to the datadir . If you don\u2019t want to use any of the above options, you can additionally use rsync or cp to restore the files. Note The datadir must be empty before restoring the backup. Also it\u2019s important to note that MySQL server needs to be shut down before restore is performed. You can\u2019t restore to a datadir of a running mysqld instance (except when importing a partial backup). Example of the rsync command that can be used to restore the backup can look like this: $ rsync -avrP /data/backup/ /var/lib/mysql/ You should check that the restored files have the correct ownership and permissions. As files\u2019 attributes will be preserved, in most cases you will need to change the files\u2019 ownership to mysql before starting the database server, as they will be owned by the user who created the backup: $ chown -R mysql:mysql /var/lib/mysql Data is now restored and you can start the server. Note When relay-log-info-repository=TABLE is enabled, the instance recovered from the backup has errors in the error log, like the following: 2019-08-09 12:40:02 69297 [ERROR] Failed to open the relay log '/data/mysql-relay-bin.004349' (relay_log_pos 5534092) To avoid these types of issues, enable relay_log_recovery or execute RESET SLAVE prior to CHANGE MASTER TO . The relay log information was backed up, but a new relay log has been created, which creates a mismatch during the restore.","title":"Restoring a Backup"},{"location":"backup_scenarios/incremental_backup.html","text":"Incremental Backup \u00b6 Both the xtrabackup tool and the innobackupex tool support incremental backups. An incremental backup backs up only data that has changed since the last backup. You can take multiple incremental backups between each full backup. For example, you can take a full backup once a week and an incremental backup every day, or a full backup every day and incremental backups each hour. Incremental backups work because each InnoDB page contains a log sequence number (LSN). The LSN is the system version number for the entire database. Each page\u2019s LSN shows how recently it was changed. An incremental backup copies each page whose LSN is newer than the previous incremental or full backup\u2019s LSN . There are two algorithms in use to find the set of such pages to be copied. The first one, available with all the server types and versions, checks the page LSN directly by reading all the data pages. The second one, available with Percona Server for MySQL , enables the changed page tracking feature on the server, which will note the pages as they are being changed. This information will be then written out in a compact separate so-called bitmap file. The xtrabackup binary uses that file to read only the data pages it needs for the incremental backup. This features potentially saves many read requests. The latter algorithm is enabled by default if the xtrabackup binary finds the bitmap file. It is possible to specify xtrabackup --incremental-force-scan to read all the pages even if the bitmap data is available. Important Incremental backups do not compare the data files to the previous backup\u2019s data files. For this reason, running an incremental backup after a partial backup may lead to inconsistent data. Incremental backups read the pages and compare their LSN to the last backup\u2019s LSN . You must have a full backup to recover the incremental changes. Without a full backup to act as a base, the incremental backups are useless. You can use the --incremental-lsn option to perform an incremental backup without even having the previous backup, if you know its LSN . See also : Partial Backups Creating an Incremental Backup \u00b6 To make an incremental backup, begin with a full backup as usual. The xtrabackup binary writes a file called xtrabackup_checkpoints into the backup\u2019s target directory. This file contains a line showing the to_lsn , which is the database\u2019s LSN at the end of the backup. Create the full backup with a following command: $ xtrabackup --backup --target-dir = /data/backups/base If you look at the xtrabackup_checkpoints file, you should see similar content depending on your LSN nuber: backup_type = full-backuped from_lsn = 0 to_lsn = 1626007 last_lsn = 1626007 compact = 0 recover_binlog_info = 1 Now that you have a full backup, you can make an incremental backup based on it. Use the following command: $ xtrabackup --backup --target-dir = /data/backups/inc1 \\ --incremental-basedir = /data/backups/base The /data/backups/inc1/ directory should now contain delta files, such as ibdata1.delta and test/table1.ibd.delta. These represent the changes since the LSN 1626007 . If you examine the xtrabackup_checkpoints file in this directory, you should see similar content to the following: backup_type = incremental from_lsn = 1626007 to_lsn = 4124244 last_lsn = 4124244 compact = 0 recover_binlog_info = 1 from_lsn is the starting LSN of the backup and for incremental it has to be the same as to_lsn (if it is the last checkpoint) of the previous/base backup. It\u2019s now possible to use this directory as the base for yet another incremental backup: $ xtrabackup --backup --target-dir = /data/backups/inc2 \\ --incremental-basedir = /data/backups/inc1 This folder also contains the xtrabackup_checkpoints : backup_type = incremental from_lsn = 4124244 to_lsn = 6938371 last_lsn = 7110572 compact = 0 recover_binlog_info = 1 Note In this case you can see that there is a difference between the to_lsn (last checkpoint LSN) and last_lsn (last copied LSN), this means that there was some traffic on the server during the backup process. Preparing the Incremental Backups \u00b6 The xtrabackup --prepare step for incremental backups is not the same as for full backups. In full backups, two types of operations are performed to make the database consistent: committed transactions are replayed from the log file against the data files, and uncommitted transactions are rolled back. You must skip the rollback of uncommitted transactions when preparing an incremental backup, because transactions that were uncommitted at the time of your backup may be in progress, and it\u2019s likely that they will be committed in the next incremental backup. You should use the xtrabackup --apply-log-only option to prevent the rollback phase. Note If you do not use the xtrabackup --apply-log-only option to prevent the rollback phase, then your incremental backup is useless .After the transactions have been rolled back, further incremental backups cannot be applied. Beginning with the full backup you created, you can prepare it, and then apply the incremental differences to it. Recall that you have the following backups: /data/backups/base /data/backups/inc1 /data/backups/inc2 To prepare the base backup, you need to run xtrabackup \u2013prepare as usual, but prevent the rollback phase: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base The output should end with text similar to the following: InnoDB: Shutdown completed; log sequence number 1626007 161011 12:41:04 completed OK! The log sequence number should match the to_lsn of the base backup, which you saw previously. Note This backup is safe to restore , even though the operation skipped the rollback phase. If you restore it and start MySQL , InnoDB detects that the rollback phase was not performed, and it will do that in the background. This operation is the same as a crash recovery upon start. In addition, MySQL notifies you that the database was not shut down normally. To apply the first incremental backup to the full backup, run the following command: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc1 This applies the delta files to the files in /data/backups/base , which rolls them forward in time to the time of the incremental backup. It then applies the redo log as usual to the result. The final data is in /data/backups/base , not in the incremental directory. You should see an output similar to: incremental backup from 1626007 is enabled. xtrabackup: cd to /data/backups/base xtrabackup: This target seems to be already prepared with --apply-log-only. xtrabackup: xtrabackup_logfile detected: size=2097152, start_lsn=(4124244) ... xtrabackup: page size for /tmp/backups/inc1/ibdata1.delta is 16384 bytes Applying /tmp/backups/inc1/ibdata1.delta to ./ibdata1... ... 161011 12:45:56 completed OK! Again, the LSN should match what you saw from your earlier inspection of the first incremental backup. If you restore the files from /data/backups/base , you should see the state of the database as of the first incremental backup. Warning Percona XtraBackup does not support using the same incremental backup directory to prepare two copies of backup. Do not run xtrabackup --prepare with the same incremental backup directory (the value of --incremental-dir ) more than once. Preparing the second incremental backup is a similar process: apply the deltas to the (modified) base backup, and you will roll its data forward in time to the point of the second incremental backup: $ xtrabackup --prepare --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc2 Note xtrabackup --apply-log-only should be used when merging all incrementals except the last one. That\u2019s why the previous line doesn\u2019t contain the xtrabackup --apply-log-only option. Even if the xtrabackup --apply-log-only was used on the last step, backup would still be consistent but in that case server would perform the rollback phase. Once prepared, incremental backups are the same as the full backups and they can be restored in the same way.","title":"Incremental Backup"},{"location":"backup_scenarios/incremental_backup.html#incremental-backup","text":"Both the xtrabackup tool and the innobackupex tool support incremental backups. An incremental backup backs up only data that has changed since the last backup. You can take multiple incremental backups between each full backup. For example, you can take a full backup once a week and an incremental backup every day, or a full backup every day and incremental backups each hour. Incremental backups work because each InnoDB page contains a log sequence number (LSN). The LSN is the system version number for the entire database. Each page\u2019s LSN shows how recently it was changed. An incremental backup copies each page whose LSN is newer than the previous incremental or full backup\u2019s LSN . There are two algorithms in use to find the set of such pages to be copied. The first one, available with all the server types and versions, checks the page LSN directly by reading all the data pages. The second one, available with Percona Server for MySQL , enables the changed page tracking feature on the server, which will note the pages as they are being changed. This information will be then written out in a compact separate so-called bitmap file. The xtrabackup binary uses that file to read only the data pages it needs for the incremental backup. This features potentially saves many read requests. The latter algorithm is enabled by default if the xtrabackup binary finds the bitmap file. It is possible to specify xtrabackup --incremental-force-scan to read all the pages even if the bitmap data is available. Important Incremental backups do not compare the data files to the previous backup\u2019s data files. For this reason, running an incremental backup after a partial backup may lead to inconsistent data. Incremental backups read the pages and compare their LSN to the last backup\u2019s LSN . You must have a full backup to recover the incremental changes. Without a full backup to act as a base, the incremental backups are useless. You can use the --incremental-lsn option to perform an incremental backup without even having the previous backup, if you know its LSN . See also : Partial Backups","title":"Incremental Backup"},{"location":"backup_scenarios/incremental_backup.html#creating-an-incremental-backup","text":"To make an incremental backup, begin with a full backup as usual. The xtrabackup binary writes a file called xtrabackup_checkpoints into the backup\u2019s target directory. This file contains a line showing the to_lsn , which is the database\u2019s LSN at the end of the backup. Create the full backup with a following command: $ xtrabackup --backup --target-dir = /data/backups/base If you look at the xtrabackup_checkpoints file, you should see similar content depending on your LSN nuber: backup_type = full-backuped from_lsn = 0 to_lsn = 1626007 last_lsn = 1626007 compact = 0 recover_binlog_info = 1 Now that you have a full backup, you can make an incremental backup based on it. Use the following command: $ xtrabackup --backup --target-dir = /data/backups/inc1 \\ --incremental-basedir = /data/backups/base The /data/backups/inc1/ directory should now contain delta files, such as ibdata1.delta and test/table1.ibd.delta. These represent the changes since the LSN 1626007 . If you examine the xtrabackup_checkpoints file in this directory, you should see similar content to the following: backup_type = incremental from_lsn = 1626007 to_lsn = 4124244 last_lsn = 4124244 compact = 0 recover_binlog_info = 1 from_lsn is the starting LSN of the backup and for incremental it has to be the same as to_lsn (if it is the last checkpoint) of the previous/base backup. It\u2019s now possible to use this directory as the base for yet another incremental backup: $ xtrabackup --backup --target-dir = /data/backups/inc2 \\ --incremental-basedir = /data/backups/inc1 This folder also contains the xtrabackup_checkpoints : backup_type = incremental from_lsn = 4124244 to_lsn = 6938371 last_lsn = 7110572 compact = 0 recover_binlog_info = 1 Note In this case you can see that there is a difference between the to_lsn (last checkpoint LSN) and last_lsn (last copied LSN), this means that there was some traffic on the server during the backup process.","title":"Creating an Incremental Backup"},{"location":"backup_scenarios/incremental_backup.html#preparing-the-incremental-backups","text":"The xtrabackup --prepare step for incremental backups is not the same as for full backups. In full backups, two types of operations are performed to make the database consistent: committed transactions are replayed from the log file against the data files, and uncommitted transactions are rolled back. You must skip the rollback of uncommitted transactions when preparing an incremental backup, because transactions that were uncommitted at the time of your backup may be in progress, and it\u2019s likely that they will be committed in the next incremental backup. You should use the xtrabackup --apply-log-only option to prevent the rollback phase. Note If you do not use the xtrabackup --apply-log-only option to prevent the rollback phase, then your incremental backup is useless .After the transactions have been rolled back, further incremental backups cannot be applied. Beginning with the full backup you created, you can prepare it, and then apply the incremental differences to it. Recall that you have the following backups: /data/backups/base /data/backups/inc1 /data/backups/inc2 To prepare the base backup, you need to run xtrabackup \u2013prepare as usual, but prevent the rollback phase: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base The output should end with text similar to the following: InnoDB: Shutdown completed; log sequence number 1626007 161011 12:41:04 completed OK! The log sequence number should match the to_lsn of the base backup, which you saw previously. Note This backup is safe to restore , even though the operation skipped the rollback phase. If you restore it and start MySQL , InnoDB detects that the rollback phase was not performed, and it will do that in the background. This operation is the same as a crash recovery upon start. In addition, MySQL notifies you that the database was not shut down normally. To apply the first incremental backup to the full backup, run the following command: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc1 This applies the delta files to the files in /data/backups/base , which rolls them forward in time to the time of the incremental backup. It then applies the redo log as usual to the result. The final data is in /data/backups/base , not in the incremental directory. You should see an output similar to: incremental backup from 1626007 is enabled. xtrabackup: cd to /data/backups/base xtrabackup: This target seems to be already prepared with --apply-log-only. xtrabackup: xtrabackup_logfile detected: size=2097152, start_lsn=(4124244) ... xtrabackup: page size for /tmp/backups/inc1/ibdata1.delta is 16384 bytes Applying /tmp/backups/inc1/ibdata1.delta to ./ibdata1... ... 161011 12:45:56 completed OK! Again, the LSN should match what you saw from your earlier inspection of the first incremental backup. If you restore the files from /data/backups/base , you should see the state of the database as of the first incremental backup. Warning Percona XtraBackup does not support using the same incremental backup directory to prepare two copies of backup. Do not run xtrabackup --prepare with the same incremental backup directory (the value of --incremental-dir ) more than once. Preparing the second incremental backup is a similar process: apply the deltas to the (modified) base backup, and you will roll its data forward in time to the point of the second incremental backup: $ xtrabackup --prepare --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc2 Note xtrabackup --apply-log-only should be used when merging all incrementals except the last one. That\u2019s why the previous line doesn\u2019t contain the xtrabackup --apply-log-only option. Even if the xtrabackup --apply-log-only was used on the last step, backup would still be consistent but in that case server would perform the rollback phase. Once prepared, incremental backups are the same as the full backups and they can be restored in the same way.","title":"Preparing the Incremental Backups"},{"location":"howtos/backup_verification.html","text":"Verifying Backups with replication and pt-checksum \u00b6 One way to verify if the backup is consistent is by setting up the replication and running pt-table-checksum . This can be used to verify any type of backups, but before setting up replication, backup should be prepared and be able to run (this means that incremental backups should be merged to full backups, encrypted backups decrypted etc.). Setting up the replication \u00b6 How to setup a replica for replication in 6 simple steps with Percona XtraBackup guide provides a detailed instructions on how to take the backup and set up the replication. For checking the backup consistency you can use either the original server where the backup was taken, or another test server created by using a different backup method (such as cold backup, mysqldump or LVM snapshots) as the source server in the replication setup. Using pt-table-checksum \u00b6 This tool is part of the Percona Toolkit . It performs an online replication consistency check by executing checksum queries on the source, which produces different results on replicas that are inconsistent with the source. After you confirmed that replication has been set up successfully, you can install or download pt-table-checksum . This example shows downloading the latest version of pt-table-checksum : $ wget percona.com/get/pt-table-checksum Note In order for pt-table-checksum to work correctly libdbd-mysql-perl will need to be installed on Debian/Ubuntu systems or perl-DBD-MySQL on RHEL/CentOS . If you installed the percona-toolkit package from the Percona repositories package manager should install those libraries automatically. After this command has been run, pt-table-checksum will be downloaded to your current working directory. Running the pt-table-checksum on the source will create percona database with the checksums table which will be replicated to the replicas as well. Example of the pt-table-checksum will look like this: $ ./pt-table-checksum You should see results similar to the following: TS ERRORS DIFFS ROWS CHUNKS SKIPPED TIME TABLE 04-30T11:31:50 0 0 633135 8 0 5.400 exampledb.aka_name 04-30T11:31:52 0 0 290859 1 0 2.692 exampledb.aka_title Checksumming exampledb.user_info: 16% 02:27 remain Checksumming exampledb.user_info: 34% 01:58 remain Checksumming exampledb.user_info: 50% 01:29 remain Checksumming exampledb.user_info: 68% 00:56 remain Checksumming exampledb.user_info: 86% 00:24 remain 04-30T11:34:38 0 0 22187768 126 0 165.216 exampledb.user_info 04-30T11:38:09 0 0 0 1 0 0.033 mysql.time_zone_name 04-30T11:38:09 0 0 0 1 0 0.052 mysql.time_zone_transition 04-30T11:38:09 0 0 0 1 0 0.054 mysql.time_zone_transition_type 04-30T11:38:09 0 0 8 1 0 0.064 mysql.user If all the values in the DIFFS column are 0 that means that backup is consistent with the current setup. In case backup wasn\u2019t consistent pt-table-checksum should spot the difference and point to the table that doesn\u2019t match. Following example shows adding new user on the backed up replica in order to simulate the inconsistent backup: mysql > GRANT usage ON exampledb . * to exampledb @ localhost IDENTIFIED BY 'thisisnewpassword' ; If we run the pt-table-checksum now difference should be spotted $ ./pt-table-checksum You should see results similar to the following: TS ERRORS DIFFS ROWS CHUNKS SKIPPED TIME TABLE 04-30T11:31:50 0 0 633135 8 0 5.400 exampledb.aka_name 04-30T11:31:52 0 0 290859 1 0 2.692 exampledb.aka_title Checksumming exampledb.user_info: 16% 02:27 remain Checksumming exampledb.user_info: 34% 01:58 remain Checksumming exampledb.user_info: 50% 01:29 remain Checksumming exampledb.user_info: 68% 00:56 remain Checksumming exampledb.user_info: 86% 00:24 remain 04-30T11:34:38 0 0 22187768 126 0 165.216 exampledb.user_info 04-30T11:38:09 0 0 0 1 0 0.033 mysql.time_zone_name 04-30T11:38:09 0 0 0 1 0 0.052 mysql.time_zone_transition 04-30T11:38:09 0 0 0 1 0 0.054 mysql.time_zone_transition_type 04-30T11:38:09 1 0 8 1 0 0.064 mysql.user This output shows that source and the replica are not in a consistent state and that the difference is in the mysql.user table. More information on different options that pt-table-checksum provides can be found in the pt-table-checksum documentation .","title":"Verifying Backups with replication and pt-checksum"},{"location":"howtos/backup_verification.html#verifying-backups-with-replication-and-pt-checksum","text":"One way to verify if the backup is consistent is by setting up the replication and running pt-table-checksum . This can be used to verify any type of backups, but before setting up replication, backup should be prepared and be able to run (this means that incremental backups should be merged to full backups, encrypted backups decrypted etc.).","title":"Verifying Backups with replication and pt-checksum"},{"location":"howtos/backup_verification.html#setting-up-the-replication","text":"How to setup a replica for replication in 6 simple steps with Percona XtraBackup guide provides a detailed instructions on how to take the backup and set up the replication. For checking the backup consistency you can use either the original server where the backup was taken, or another test server created by using a different backup method (such as cold backup, mysqldump or LVM snapshots) as the source server in the replication setup.","title":"Setting up the replication"},{"location":"howtos/backup_verification.html#using-pt-table-checksum","text":"This tool is part of the Percona Toolkit . It performs an online replication consistency check by executing checksum queries on the source, which produces different results on replicas that are inconsistent with the source. After you confirmed that replication has been set up successfully, you can install or download pt-table-checksum . This example shows downloading the latest version of pt-table-checksum : $ wget percona.com/get/pt-table-checksum Note In order for pt-table-checksum to work correctly libdbd-mysql-perl will need to be installed on Debian/Ubuntu systems or perl-DBD-MySQL on RHEL/CentOS . If you installed the percona-toolkit package from the Percona repositories package manager should install those libraries automatically. After this command has been run, pt-table-checksum will be downloaded to your current working directory. Running the pt-table-checksum on the source will create percona database with the checksums table which will be replicated to the replicas as well. Example of the pt-table-checksum will look like this: $ ./pt-table-checksum You should see results similar to the following: TS ERRORS DIFFS ROWS CHUNKS SKIPPED TIME TABLE 04-30T11:31:50 0 0 633135 8 0 5.400 exampledb.aka_name 04-30T11:31:52 0 0 290859 1 0 2.692 exampledb.aka_title Checksumming exampledb.user_info: 16% 02:27 remain Checksumming exampledb.user_info: 34% 01:58 remain Checksumming exampledb.user_info: 50% 01:29 remain Checksumming exampledb.user_info: 68% 00:56 remain Checksumming exampledb.user_info: 86% 00:24 remain 04-30T11:34:38 0 0 22187768 126 0 165.216 exampledb.user_info 04-30T11:38:09 0 0 0 1 0 0.033 mysql.time_zone_name 04-30T11:38:09 0 0 0 1 0 0.052 mysql.time_zone_transition 04-30T11:38:09 0 0 0 1 0 0.054 mysql.time_zone_transition_type 04-30T11:38:09 0 0 8 1 0 0.064 mysql.user If all the values in the DIFFS column are 0 that means that backup is consistent with the current setup. In case backup wasn\u2019t consistent pt-table-checksum should spot the difference and point to the table that doesn\u2019t match. Following example shows adding new user on the backed up replica in order to simulate the inconsistent backup: mysql > GRANT usage ON exampledb . * to exampledb @ localhost IDENTIFIED BY 'thisisnewpassword' ; If we run the pt-table-checksum now difference should be spotted $ ./pt-table-checksum You should see results similar to the following: TS ERRORS DIFFS ROWS CHUNKS SKIPPED TIME TABLE 04-30T11:31:50 0 0 633135 8 0 5.400 exampledb.aka_name 04-30T11:31:52 0 0 290859 1 0 2.692 exampledb.aka_title Checksumming exampledb.user_info: 16% 02:27 remain Checksumming exampledb.user_info: 34% 01:58 remain Checksumming exampledb.user_info: 50% 01:29 remain Checksumming exampledb.user_info: 68% 00:56 remain Checksumming exampledb.user_info: 86% 00:24 remain 04-30T11:34:38 0 0 22187768 126 0 165.216 exampledb.user_info 04-30T11:38:09 0 0 0 1 0 0.033 mysql.time_zone_name 04-30T11:38:09 0 0 0 1 0 0.052 mysql.time_zone_transition 04-30T11:38:09 0 0 0 1 0 0.054 mysql.time_zone_transition_type 04-30T11:38:09 1 0 8 1 0 0.064 mysql.user This output shows that source and the replica are not in a consistent state and that the difference is in the mysql.user table. More information on different options that pt-table-checksum provides can be found in the pt-table-checksum documentation .","title":"Using pt-table-checksum"},{"location":"howtos/enabling_tcp.html","text":"Enabling the server to communicate via TCP/IP \u00b6 Most of the Linux distributions do not enable by default to accept TCP/IP connections from outside in their MySQL or Percona Server packages. You can check it with netstat on a shell: $ netstat -lnp | grep mysql You should see results similar to the following: tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 2480/mysqld unix 2 [ ACC ] STREAM LISTENING 8101 2480/mysqld /tmp/mysql.sock You should check two things: there is a line starting with tcp (the server is indeed accepting TCP connections) and the first address ( 0.0.0.0:3306 in this example) is different than 127.0.0.1:3306 (the bind address is not localhost\u2019s). In the first case, the first place to look is the my.cnf file. If you find the option skip-networking , comment it out or just delete it. Also check that if the variable bind_address is set, then it shouldn\u2019t be set to localhost\u2019s but to the host\u2019s IP. Then restart the MySQL server and check it again with netstat . If the changes you did had no effect, then you should look at your distribution\u2019s startup scripts (like rc.mysqld ). You should comment out flags like --skip-networking and/or change the bind-address . After you get the server listening to remote TCP connections properly, the last thing to do is checking that the port (3306 by default) is indeed open. Check your firewall configurations ( iptables -L ) and that you are allowing remote hosts on that port (in /etc/hosts.allow ). And we\u2019re done! We have a MySQL server running which is able to communicate with the world through TCP/IP.","title":"Enabling the server to communicate via TCP/IP"},{"location":"howtos/enabling_tcp.html#enabling-the-server-to-communicate-via-tcpip","text":"Most of the Linux distributions do not enable by default to accept TCP/IP connections from outside in their MySQL or Percona Server packages. You can check it with netstat on a shell: $ netstat -lnp | grep mysql You should see results similar to the following: tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 2480/mysqld unix 2 [ ACC ] STREAM LISTENING 8101 2480/mysqld /tmp/mysql.sock You should check two things: there is a line starting with tcp (the server is indeed accepting TCP connections) and the first address ( 0.0.0.0:3306 in this example) is different than 127.0.0.1:3306 (the bind address is not localhost\u2019s). In the first case, the first place to look is the my.cnf file. If you find the option skip-networking , comment it out or just delete it. Also check that if the variable bind_address is set, then it shouldn\u2019t be set to localhost\u2019s but to the host\u2019s IP. Then restart the MySQL server and check it again with netstat . If the changes you did had no effect, then you should look at your distribution\u2019s startup scripts (like rc.mysqld ). You should comment out flags like --skip-networking and/or change the bind-address . After you get the server listening to remote TCP connections properly, the last thing to do is checking that the port (3306 by default) is indeed open. Check your firewall configurations ( iptables -L ) and that you are allowing remote hosts on that port (in /etc/hosts.allow ). And we\u2019re done! We have a MySQL server running which is able to communicate with the world through TCP/IP.","title":"Enabling the server to communicate via TCP/IP"},{"location":"howtos/permissions.html","text":"Privileges and Permissions for Users \u00b6 We will be referring to permissions to the ability of a user to access and perform changes on the relevant parts of the host\u2019s filesystem, starting/stopping services and installing software. By privileges we refer to the abilities of a database user to perform different kinds of actions on the database server. At a system level \u00b6 There are many ways for checking the permission on a file or directory. For example, ls -ls /path/to/file or stat /path/to/file | grep Access will do the job: $ stat /etc/mysql | grep Access You should see results similar to the following: Access: (0755/drwxr-xr-x) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2011-05-12 21:19:07.129850437 -0300 $ ls -ld /etc/mysql/my.cnf -rw-r--r-- 1 root root 4703 Apr 5 06:26 /etc/mysql/my.cnf As in this example, my.cnf is owned by root and not writable for anyone else. Assuming that you do not have root \u2018s password, you can check what permissions you have on this types of files with sudo -l : $ sudo -l You should see results similar to the following: Password: You may run the following commands on this host: (root) /usr/bin/ (root) NOPASSWD: /etc/init.d/mysqld (root) NOPASSWD: /bin/vi /etc/mysql/my.cnf (root) NOPASSWD: /usr/local/bin/top (root) NOPASSWD: /usr/bin/ls (root) /bin/tail Being able to execute with sudo scripts in /etc/init.d/ , /etc/rc.d/ or /sbin/service is the ability to start and stop services. Also, If you can execute the package manager of your distribution, you can install or remove software with it. If not, having rwx permission over a directory will let you do a local installation of software by compiling it there. This is a typical situation in many hosting companies\u2019 services. There are other ways for managing permissions, such as using PolicyKit , Extended ACLs or SELinux , which may be preventing or allowing your access. You should check them in that case. At a database server level \u00b6 To query the privileges that your database user has been granted, at a console of the server execute: mysql > SHOW GRANTS ; or for a particular user with: mysql > SHOW GRANTS FOR 'db-user' @ 'host' ; It will display the privileges using the same format as for the SHOW-GRANT statement . Note that privileges may vary across versions of the server. To list the exact list of privileges that your server support (and a brief description of them) execute: mysql > SHOW PRIVILEGES ;","title":"Privileges and Permissions for Users"},{"location":"howtos/permissions.html#privileges-and-permissions-for-users","text":"We will be referring to permissions to the ability of a user to access and perform changes on the relevant parts of the host\u2019s filesystem, starting/stopping services and installing software. By privileges we refer to the abilities of a database user to perform different kinds of actions on the database server.","title":"Privileges and Permissions for Users"},{"location":"howtos/permissions.html#at-a-system-level","text":"There are many ways for checking the permission on a file or directory. For example, ls -ls /path/to/file or stat /path/to/file | grep Access will do the job: $ stat /etc/mysql | grep Access You should see results similar to the following: Access: (0755/drwxr-xr-x) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2011-05-12 21:19:07.129850437 -0300 $ ls -ld /etc/mysql/my.cnf -rw-r--r-- 1 root root 4703 Apr 5 06:26 /etc/mysql/my.cnf As in this example, my.cnf is owned by root and not writable for anyone else. Assuming that you do not have root \u2018s password, you can check what permissions you have on this types of files with sudo -l : $ sudo -l You should see results similar to the following: Password: You may run the following commands on this host: (root) /usr/bin/ (root) NOPASSWD: /etc/init.d/mysqld (root) NOPASSWD: /bin/vi /etc/mysql/my.cnf (root) NOPASSWD: /usr/local/bin/top (root) NOPASSWD: /usr/bin/ls (root) /bin/tail Being able to execute with sudo scripts in /etc/init.d/ , /etc/rc.d/ or /sbin/service is the ability to start and stop services. Also, If you can execute the package manager of your distribution, you can install or remove software with it. If not, having rwx permission over a directory will let you do a local installation of software by compiling it there. This is a typical situation in many hosting companies\u2019 services. There are other ways for managing permissions, such as using PolicyKit , Extended ACLs or SELinux , which may be preventing or allowing your access. You should check them in that case.","title":"At a system level"},{"location":"howtos/permissions.html#at-a-database-server-level","text":"To query the privileges that your database user has been granted, at a console of the server execute: mysql > SHOW GRANTS ; or for a particular user with: mysql > SHOW GRANTS FOR 'db-user' @ 'host' ; It will display the privileges using the same format as for the SHOW-GRANT statement . Note that privileges may vary across versions of the server. To list the exact list of privileges that your server support (and a brief description of them) execute: mysql > SHOW PRIVILEGES ;","title":"At a database server level"},{"location":"howtos/recipes_ibkx_compressed.html","text":"Making a Compressed Backup \u00b6 In order to make a compressed backup you\u2019ll need to use innobackupex --compress $ innobackupex --compress /data/backup If you want to speed up the compression you can use the parallel compression, which can be enabled with innobackupex --compress-threads option. Following example will use four threads for compression: $ innobackupex --compress --compress-threads = 4 /data/backup Output should look like this ... [01] Compressing ./imdb/comp_cast_type.ibd to /data/backup/2013-08-01_11-24-04/./imdb/comp_cast_type.ibd.qp [01] ...done [01] Compressing ./imdb/aka_name.ibd to /data/backup/2013-08-01_11-24-04/./imdb/aka_name.ibd.qp [01] ...done ... 130801 11:50:24 innobackupex: completed OK Preparing the backup \u00b6 Before you can prepare the backup you\u2019ll need to uncompress all the files with qpress (which is available from Percona Software repositories ). You can use following one-liner to uncompress all the files: $ for bf in ` find . -iname \"*\\.qp\" ` ; do qpress -d $bf $( dirname $bf ) && rm $bf ; done In Percona XtraBackup 2.1.4 new innobackupex --decompress option has been implemented that can be used to decompress the backup: $ innobackupex --decompress /data/backup/2013-08-01_11-24-04/ Note In order to successfully use the innobackupex --decompress option,qpress binary needs to installed and within the path. innobackupex --parallel can be used with innobackupex --decompress option to decompress multiple files simultaneously. When the files are uncompressed you can prepare the backup with innobackupex \u2013apply-log: $ innobackupex --apply-log /data/backup/2013-08-01_11-24-04/ You should check for a confirmation message: 130802 02:51:02 innobackupex: completed OK! Now the files in /data/backups/2013-08-01_11-24-04 is ready to be used by the server. Note Percona XtraBackup doesn\u2019t automatically remove the compressed files. In order to clean up the backup directory users should remove the \\*.qp files. Restoring the backup \u00b6 Once the backup has been prepared you can use the innobackupex --copy-back to restore the backup. $ innobackupex --copy-back /data/backups/2013-08-01_11-24-04/ This will copy the prepared data back to its original location as defined by the datadir in your my.cnf . After the confirmation message: 130802 02:58:44 innobackupex: completed OK! you should check the file permissions after copying the data back. You may need to adjust them with something like: $ chown -R mysql:mysql /var/lib/mysql Now the datadir contains the restored data. You are ready to start the server.","title":"Making a Compressed Backup"},{"location":"howtos/recipes_ibkx_compressed.html#making-a-compressed-backup","text":"In order to make a compressed backup you\u2019ll need to use innobackupex --compress $ innobackupex --compress /data/backup If you want to speed up the compression you can use the parallel compression, which can be enabled with innobackupex --compress-threads option. Following example will use four threads for compression: $ innobackupex --compress --compress-threads = 4 /data/backup Output should look like this ... [01] Compressing ./imdb/comp_cast_type.ibd to /data/backup/2013-08-01_11-24-04/./imdb/comp_cast_type.ibd.qp [01] ...done [01] Compressing ./imdb/aka_name.ibd to /data/backup/2013-08-01_11-24-04/./imdb/aka_name.ibd.qp [01] ...done ... 130801 11:50:24 innobackupex: completed OK","title":"Making a Compressed Backup"},{"location":"howtos/recipes_ibkx_compressed.html#preparing-the-backup","text":"Before you can prepare the backup you\u2019ll need to uncompress all the files with qpress (which is available from Percona Software repositories ). You can use following one-liner to uncompress all the files: $ for bf in ` find . -iname \"*\\.qp\" ` ; do qpress -d $bf $( dirname $bf ) && rm $bf ; done In Percona XtraBackup 2.1.4 new innobackupex --decompress option has been implemented that can be used to decompress the backup: $ innobackupex --decompress /data/backup/2013-08-01_11-24-04/ Note In order to successfully use the innobackupex --decompress option,qpress binary needs to installed and within the path. innobackupex --parallel can be used with innobackupex --decompress option to decompress multiple files simultaneously. When the files are uncompressed you can prepare the backup with innobackupex \u2013apply-log: $ innobackupex --apply-log /data/backup/2013-08-01_11-24-04/ You should check for a confirmation message: 130802 02:51:02 innobackupex: completed OK! Now the files in /data/backups/2013-08-01_11-24-04 is ready to be used by the server. Note Percona XtraBackup doesn\u2019t automatically remove the compressed files. In order to clean up the backup directory users should remove the \\*.qp files.","title":"Preparing the backup"},{"location":"howtos/recipes_ibkx_compressed.html#restoring-the-backup","text":"Once the backup has been prepared you can use the innobackupex --copy-back to restore the backup. $ innobackupex --copy-back /data/backups/2013-08-01_11-24-04/ This will copy the prepared data back to its original location as defined by the datadir in your my.cnf . After the confirmation message: 130802 02:58:44 innobackupex: completed OK! you should check the file permissions after copying the data back. You may need to adjust them with something like: $ chown -R mysql:mysql /var/lib/mysql Now the datadir contains the restored data. You are ready to start the server.","title":"Restoring the backup"},{"location":"howtos/recipes_ibkx_gtid.html","text":"How to create a new (or repair a broken) GTID based slave \u00b6 MySQL 5.6 introduced the new Global Transaction ID ( GTID ) support in replication. Percona XtraBackup automatically stores the GTID value in the xtrabackup_binlog_info when doing the backup of MySQL and Percona Server for MySQL 5.7 with the GTID mode enabled. This information can be used to create a new (or repair a broken) GTID based replica. STEP 1: Take a backup from any server on the replication environment, source or replica. \u00b6 The following command takes a backup and saves it in the /data/backups/$TIMESTAMP folder: $ innobackupex /data/backups/ In the destination folder, there will be a file with the name xtrabackup_binlog_info . This file contains both binary log coordinates and the GTID information. $ cat xtrabackup_binlog_info The results should be similar to the following: mysql-bin.000002 1232 c777888a-b6df-11e2-a604-080027635ef5:1-4 That information is also printed by innobackupex after taking the backup: innobackupex: MySQL binlog position: filename 'mysql-bin.000002', position 1232, GTID of the last change 'c777888a-b6df-11e2-a604-080027635ef5:1-4' STEP 2: Prepare the backup \u00b6 The backup will be prepared with the following command: TheMaster$ innobackupex --apply-log /data/backups/ $TIMESTAMP / You need to select the path where your snapshot has been taken, for example /data/backups/2013-05-07_08-33-33 . If everything is ok you should get the same OK message. Now, the transaction logs are applied to the data files, and new ones are created: your data files are ready to be used by the MySQL server. STEP 3: Move the backup to the destination server \u00b6 Use rsync or scp to copy the data to the destination server. If you are synchronizing the data directly to the already running replica\u2019s data directory it is advised to stop the MySQL server there. $ rsync -avprP -e ssh /path/to/backupdir/ $TIMESTAMP NewSlave:/path/to/mysql/ After you copy the data over, make sure MySQL has proper permissions to access them. $ chown mysql:mysql /path/to/mysql/datadir STEP 4: Configure and start replication \u00b6 Set the gtid_purged variable to the GTID from xtrabackup_binlog_info . Then, update the information about the source node and, finally, start the replica. # Using the mysql shell NewSlave > SET SESSION wsrep_on = 0 ; NewSlave > RESET MASTER ; NewSlave > SET SESSION wsrep_on = 1 ; NewSlave > SET GLOBAL gtid_purged = '<gtid_string_found_in_xtrabackup_binlog_info>' ; NewSlave > CHANGE MASTER TO MASTER_HOST = \"$masterip\" , MASTER_USER = \"repl\" , MASTER_PASSWORD = \"$slavepass\" , MASTER_AUTO_POSITION = 1 ; NewSlave > START SLAVE ; Note The example above is applicable to Percona XtraDB Cluster . The wsrep_on variable is set to 0 before resetting the source ( RESET MASTER ). The reason is that Percona XtraDB Cluster will not allow resetting the source if wsrep_on=1 . STEP 5: Check the replication status \u00b6 Following command will show the replica status: NewSlave > SHOW SLAVE STATUS \\G The results should be similar to as follows: Slave_IO_Running: Yes Slave_SQL_Running: Yes [...] Retrieved_Gtid_Set: c777888a-b6df-11e2-a604-080027635ef5:5 Executed_Gtid_Set: c777888a-b6df-11e2-a604-080027635ef5:1-5 We can see that the replica has retrieved a new transaction with number 5, so transactions from 1 to 5 are already on this replica. That\u2019s all, we have created a new replica in our GTID based replication environment.","title":"How to create a new (or repair a broken) GTID based slave"},{"location":"howtos/recipes_ibkx_gtid.html#how-to-create-a-new-or-repair-a-broken-gtid-based-slave","text":"MySQL 5.6 introduced the new Global Transaction ID ( GTID ) support in replication. Percona XtraBackup automatically stores the GTID value in the xtrabackup_binlog_info when doing the backup of MySQL and Percona Server for MySQL 5.7 with the GTID mode enabled. This information can be used to create a new (or repair a broken) GTID based replica.","title":"How to create a new (or repair a broken) GTID based slave"},{"location":"howtos/recipes_ibkx_gtid.html#step-1-take-a-backup-from-any-server-on-the-replication-environment-source-or-replica","text":"The following command takes a backup and saves it in the /data/backups/$TIMESTAMP folder: $ innobackupex /data/backups/ In the destination folder, there will be a file with the name xtrabackup_binlog_info . This file contains both binary log coordinates and the GTID information. $ cat xtrabackup_binlog_info The results should be similar to the following: mysql-bin.000002 1232 c777888a-b6df-11e2-a604-080027635ef5:1-4 That information is also printed by innobackupex after taking the backup: innobackupex: MySQL binlog position: filename 'mysql-bin.000002', position 1232, GTID of the last change 'c777888a-b6df-11e2-a604-080027635ef5:1-4'","title":"STEP 1: Take a backup from any server on the replication environment, source or replica."},{"location":"howtos/recipes_ibkx_gtid.html#step-2-prepare-the-backup","text":"The backup will be prepared with the following command: TheMaster$ innobackupex --apply-log /data/backups/ $TIMESTAMP / You need to select the path where your snapshot has been taken, for example /data/backups/2013-05-07_08-33-33 . If everything is ok you should get the same OK message. Now, the transaction logs are applied to the data files, and new ones are created: your data files are ready to be used by the MySQL server.","title":"STEP 2: Prepare the backup"},{"location":"howtos/recipes_ibkx_gtid.html#step-3-move-the-backup-to-the-destination-server","text":"Use rsync or scp to copy the data to the destination server. If you are synchronizing the data directly to the already running replica\u2019s data directory it is advised to stop the MySQL server there. $ rsync -avprP -e ssh /path/to/backupdir/ $TIMESTAMP NewSlave:/path/to/mysql/ After you copy the data over, make sure MySQL has proper permissions to access them. $ chown mysql:mysql /path/to/mysql/datadir","title":"STEP 3: Move the backup to the destination server"},{"location":"howtos/recipes_ibkx_gtid.html#step-4-configure-and-start-replication","text":"Set the gtid_purged variable to the GTID from xtrabackup_binlog_info . Then, update the information about the source node and, finally, start the replica. # Using the mysql shell NewSlave > SET SESSION wsrep_on = 0 ; NewSlave > RESET MASTER ; NewSlave > SET SESSION wsrep_on = 1 ; NewSlave > SET GLOBAL gtid_purged = '<gtid_string_found_in_xtrabackup_binlog_info>' ; NewSlave > CHANGE MASTER TO MASTER_HOST = \"$masterip\" , MASTER_USER = \"repl\" , MASTER_PASSWORD = \"$slavepass\" , MASTER_AUTO_POSITION = 1 ; NewSlave > START SLAVE ; Note The example above is applicable to Percona XtraDB Cluster . The wsrep_on variable is set to 0 before resetting the source ( RESET MASTER ). The reason is that Percona XtraDB Cluster will not allow resetting the source if wsrep_on=1 .","title":"STEP 4: Configure and start replication"},{"location":"howtos/recipes_ibkx_gtid.html#step-5-check-the-replication-status","text":"Following command will show the replica status: NewSlave > SHOW SLAVE STATUS \\G The results should be similar to as follows: Slave_IO_Running: Yes Slave_SQL_Running: Yes [...] Retrieved_Gtid_Set: c777888a-b6df-11e2-a604-080027635ef5:5 Executed_Gtid_Set: c777888a-b6df-11e2-a604-080027635ef5:1-5 We can see that the replica has retrieved a new transaction with number 5, so transactions from 1 to 5 are already on this replica. That\u2019s all, we have created a new replica in our GTID based replication environment.","title":"STEP 5: Check the replication status"},{"location":"howtos/recipes_ibkx_inc.html","text":"Making an Incremental Backup \u00b6 Every incremental backup starts with a full one, which we will call the base backup : $ innobackupex --user = USER --password = PASSWORD /path/to/backup/dir/ Note that the full backup will be in a timestamped subdirectory of /path/to/backup/dir/ (e.g., /path/to/backup/dir/2011-12-24_23-01-00/ ). Assuming that variable $FULLBACKUP contains /path/to/backup/dir/2011-5-23_23-01-18, let\u2019s do an incremental backup an hour later: innobackupex --incremental /path/to/inc/dir \\ --incremental-basedir = $FULLBACKUP --user = USER --password = PASSWORD Now, the incremental backup should be in /path/to/inc/dir/2011-12-25_00-01-00/ . Let\u2019s call $INCREMENTALBACKUP=2011-5-23_23-50-10 . Preparing incremental backups is a bit different from full ones: First you have to replay the committed transactions on each backup, $ innobackupex --apply-log --redo-only $FULLBACKUP \\ --use-memory = 1G --user = USER --password = PASSWORD Note In the prepare phase, the --use-memory parameter speeds up the process if the amount of RAM assigned to the option is available. Use the parameter only in the prepare phase. In the other phases the parameter makes the application lazy allocate this memory (reserve) but does not affect database pages. If everything went fine, you should see an output similar to: 111225 01:10:12 InnoDB: Shutdown completed; log sequence number 91514213 Now apply the incremental backup to the base backup, by issuing: $ innobackupex --apply-log --redo-only $FULLBACKUP --incremental-dir = $INCREMENTALBACKUP --use-memory = 1G --user = DVADER --password = D4RKS1D3 Note the $INCREMENTALBACKUP . The final data will be in the base backup directory , not in the incremental one. In this example, /path/to/backup/dir/2011-12-24_23-01-00 or $FULLBACKUP . If you want to apply more incremental backups, repeat this step with the next one. It is important that you do this in the chronological order in which the backups were done. You can check the file xtrabackup_checkpoints at the directory of each one. They should look like: (in the base backup) backup_type = full-backuped from_lsn = 0 to_lsn = 1291135 and in the incremental ones: backup_type = incremental from_lsn = 1291135 to_lsn = 1291340 The to_lsn number must match the from_lsn of the next one. Once you put all the parts together, you can prepare again the full backup (base + incrementals) once again to rollback the pending transactions: $ innobackupex-1.5.1 --apply-log $FULLBACKUP --use-memory = 1G \\ --user = $USERNAME --password = $PASSWORD Now your backup is ready to be used immediately after restoring it. This preparation step is optional, as if you restore it without doing it, the database server will assume that a crash occurred and will begin to roll back the uncommitted transaction (causing some downtime which can be avoided).","title":"Making an Incremental Backup"},{"location":"howtos/recipes_ibkx_inc.html#making-an-incremental-backup","text":"Every incremental backup starts with a full one, which we will call the base backup : $ innobackupex --user = USER --password = PASSWORD /path/to/backup/dir/ Note that the full backup will be in a timestamped subdirectory of /path/to/backup/dir/ (e.g., /path/to/backup/dir/2011-12-24_23-01-00/ ). Assuming that variable $FULLBACKUP contains /path/to/backup/dir/2011-5-23_23-01-18, let\u2019s do an incremental backup an hour later: innobackupex --incremental /path/to/inc/dir \\ --incremental-basedir = $FULLBACKUP --user = USER --password = PASSWORD Now, the incremental backup should be in /path/to/inc/dir/2011-12-25_00-01-00/ . Let\u2019s call $INCREMENTALBACKUP=2011-5-23_23-50-10 . Preparing incremental backups is a bit different from full ones: First you have to replay the committed transactions on each backup, $ innobackupex --apply-log --redo-only $FULLBACKUP \\ --use-memory = 1G --user = USER --password = PASSWORD Note In the prepare phase, the --use-memory parameter speeds up the process if the amount of RAM assigned to the option is available. Use the parameter only in the prepare phase. In the other phases the parameter makes the application lazy allocate this memory (reserve) but does not affect database pages. If everything went fine, you should see an output similar to: 111225 01:10:12 InnoDB: Shutdown completed; log sequence number 91514213 Now apply the incremental backup to the base backup, by issuing: $ innobackupex --apply-log --redo-only $FULLBACKUP --incremental-dir = $INCREMENTALBACKUP --use-memory = 1G --user = DVADER --password = D4RKS1D3 Note the $INCREMENTALBACKUP . The final data will be in the base backup directory , not in the incremental one. In this example, /path/to/backup/dir/2011-12-24_23-01-00 or $FULLBACKUP . If you want to apply more incremental backups, repeat this step with the next one. It is important that you do this in the chronological order in which the backups were done. You can check the file xtrabackup_checkpoints at the directory of each one. They should look like: (in the base backup) backup_type = full-backuped from_lsn = 0 to_lsn = 1291135 and in the incremental ones: backup_type = incremental from_lsn = 1291135 to_lsn = 1291340 The to_lsn number must match the from_lsn of the next one. Once you put all the parts together, you can prepare again the full backup (base + incrementals) once again to rollback the pending transactions: $ innobackupex-1.5.1 --apply-log $FULLBACKUP --use-memory = 1G \\ --user = $USERNAME --password = $PASSWORD Now your backup is ready to be used immediately after restoring it. This preparation step is optional, as if you restore it without doing it, the database server will assume that a crash occurred and will begin to roll back the uncommitted transaction (causing some downtime which can be avoided).","title":"Making an Incremental Backup"},{"location":"howtos/recipes_ibkx_local.html","text":"Make a Local Full Backup (Create, Prepare and Restore) \u00b6 Create the Backup \u00b6 This is the simplest use case. It copies all your MySQL data into the specified directory. Here is how to make a backup of all the databases in the datadir specified in your my.cnf . It will put the backup in a time stamped subdirectory of /data/backups/, in this case, /data/backups/2010-03-13_02-42-44 , $ innobackupex /data/backups There is a lot of output, but you need to make sure you see this at the end of the backup. If you don\u2019t see this output, then your backup failed: 100313 02:43:07 innobackupex: completed OK! Prepare the Backup \u00b6 To prepare the backup use the innobackupex --apply-log option and specify the timestamped subdirectory of the backup. To speed up the apply-log process, use the innobackupex --use-memory : $ innobackupex --use-memory = 4G --apply-log /data/backups/2010-03-13_02-42-44/ You should check for a confirmation message: 100313 02:51:02 innobackupex: completed OK! Now the files in /data/backups/2010-03-13_02-42-44 is ready to be used by the server. Restore the Backup \u00b6 To restore the already-prepared backup, first stop the server and then use the innobackupex --copy-back function of innobackupex: $ innobackupex --copy-back /data/backups/2010-03-13_02-42-44/ ## Use chmod to correct the permissions, if necessary! This will copy the prepared data back to its original location as defined by the datadir in your my.cnf . Note The datadir must be empty; Percona XtraBackup innobackupex --copy-back option will not copy over existing files unless innobackupex --force-non-empty-directories option is specified. Also it\u2019s important to note that MySQL server needs to be shut down before restore is performed. You can\u2019t restore to a datadir of a running mysqld instance (except when importing a partial backup). After the confirmation message: 100313 02:58:44 innobackupex: completed OK! you should check the file permissions after copying the data back. You may need to adjust them with something like: $ chown -R mysql:mysql /var/lib/mysql Now the datadir contains the restored data. You are ready to start the server.","title":"Make a Local Full Backup (Create, Prepare and Restore)"},{"location":"howtos/recipes_ibkx_local.html#make-a-local-full-backup-create-prepare-and-restore","text":"","title":"Make a Local Full Backup (Create, Prepare and Restore)"},{"location":"howtos/recipes_ibkx_local.html#create-the-backup","text":"This is the simplest use case. It copies all your MySQL data into the specified directory. Here is how to make a backup of all the databases in the datadir specified in your my.cnf . It will put the backup in a time stamped subdirectory of /data/backups/, in this case, /data/backups/2010-03-13_02-42-44 , $ innobackupex /data/backups There is a lot of output, but you need to make sure you see this at the end of the backup. If you don\u2019t see this output, then your backup failed: 100313 02:43:07 innobackupex: completed OK!","title":"Create the Backup"},{"location":"howtos/recipes_ibkx_local.html#prepare-the-backup","text":"To prepare the backup use the innobackupex --apply-log option and specify the timestamped subdirectory of the backup. To speed up the apply-log process, use the innobackupex --use-memory : $ innobackupex --use-memory = 4G --apply-log /data/backups/2010-03-13_02-42-44/ You should check for a confirmation message: 100313 02:51:02 innobackupex: completed OK! Now the files in /data/backups/2010-03-13_02-42-44 is ready to be used by the server.","title":"Prepare the Backup"},{"location":"howtos/recipes_ibkx_local.html#restore-the-backup","text":"To restore the already-prepared backup, first stop the server and then use the innobackupex --copy-back function of innobackupex: $ innobackupex --copy-back /data/backups/2010-03-13_02-42-44/ ## Use chmod to correct the permissions, if necessary! This will copy the prepared data back to its original location as defined by the datadir in your my.cnf . Note The datadir must be empty; Percona XtraBackup innobackupex --copy-back option will not copy over existing files unless innobackupex --force-non-empty-directories option is specified. Also it\u2019s important to note that MySQL server needs to be shut down before restore is performed. You can\u2019t restore to a datadir of a running mysqld instance (except when importing a partial backup). After the confirmation message: 100313 02:58:44 innobackupex: completed OK! you should check the file permissions after copying the data back. You may need to adjust them with something like: $ chown -R mysql:mysql /var/lib/mysql Now the datadir contains the restored data. You are ready to start the server.","title":"Restore the Backup"},{"location":"howtos/recipes_ibkx_partition.html","text":"Backing Up and Restoring Individual Partitions \u00b6 Percona XtraBackup features partial backups , which means that you may backup individual partitions as well because from the storage engines perspective partitions are regular tables with specially formatted names. The only requirement for this feature is having the innodb_file_per_table option enabled in the server. There is only one caveat about using this kind of backup: you can\u2019t copy back the prepared backup. Restoring partial backups should be done by importing the tables, and not by using the traditional innobackupex --copy-back option. Although there are some scenarios where restoring can be done by copying back the files, this may be lead to database inconsistencies in many cases and it is not the recommended way to do it. Creating the backup \u00b6 There are three ways of specifying which part of the whole data will be backed up: regular expressions ( innobackupex --include ), enumerating the tables in a file ( innobackupex --tables-file ) or providing a list of databases ( innobackupex --databases ). In this example innobackupex --include option will be used. The regular expression provided to this option will be matched against the fully qualified tablename, including the database name, in the form databasename.tablename . For example, this will back up the partition p4 from the table name located in the database imdb : $ innobackupex --include = '^imdb[.]name#P#p4' /mnt/backup/ This will create a timestamped directory with the usual files that innobackupex creates, but only the data files related to the tables matched. Output of the innobackupex will list the skipped tables ... [01] Skipping ./imdb/person_info.ibd [01] Skipping ./imdb/name#P#p5.ibd [01] Skipping ./imdb/name#P#p6.ibd ... imdb.person_info.frm is skipped because it does not match ^imdb[.]name#P#p4. imdb.title.frm is skipped because it does not match ^imdb[.]name#P#p4. imdb.company_type.frm is skipped because it does not match ^imdb[.]name#P#p4. ... Note that this option is passed to xtrabackup --tables and is matched against each table of each database, the directories of each database will be created even if they are empty. Preparing the backup \u00b6 For preparing partial backups, the procedure is analogous to restoring individual tables : apply the logs and use innobackupex --export : $ innobackupex --apply-log --export /mnt/backup/2012-08-28_10-29-09 You may see warnings in the output about tables that don\u2019t exist. This is because InnoDB -based engines stores its data dictionary inside the tablespace files besides the .frm files. innobackupex will use xtrabackup to remove the missing tables (those that haven\u2019t been selected in the partial backup) from the data dictionary in order to avoid future warnings or errors: InnoDB: in InnoDB data dictionary has tablespace id 51, InnoDB: but tablespace with that id or name does not exist. It will be removed from data dictionary. 120828 10:25:28 InnoDB: Waiting for the background threads to start 120828 10:25:29 Percona XtraDB (http://www.percona.com) 1.1.8-20.1 started; log sequence number 10098323731 xtrabackup: export option is specified. xtrabackup: export metadata of table 'imdb/name#P#p4' to file `./imdb/name#P#p4.exp` (1 indexes) xtrabackup: name=PRIMARY, id.low=73, page=3 You should also see the notification of the creation of a file needed for importing ( .exp file) for each table included in the partial backup: xtrabackup: export option is specified. xtrabackup: export metadata of table 'imdb/name#P#p4' to file `./imdb/name#P#p4.exp` (1 indexes) xtrabackup: name=PRIMARY, id.low=73, page=3 Note that you can use innobackupex --export with innobackupex --apply-log to an already-prepared backup in order to create the .exp files. Finally, check the for the confirmation message in the output: 120828 19:25:38 innobackupex: completed OK! Restoring from the backups \u00b6 Restoring should be done by importing the tables in the partial backup to the server. Note Improved table/partition import is only available in Percona Server for MySQL and MySQL 5.6, this means that partitions which were backed up from different server can be imported as well. For versions older than MySQL 5.6 only partitions from that server can be imported with some important limitations. There should be no DROP/CREATE/TRUNCATE/ALTER TABLE commands issued between taking the backup and importing the partition. First step is to create new table in which data will be restored > mysql > CREATE TABLE name_p4 ( > id int ( 11 ) NOT NULL AUTO_INCREMENT , > name text NOT NULL , > imdb_index varchar ( 12 ) DEFAULT NULL , > imdb_id int ( 11 ) DEFAULT NULL , > name_pcode_cf varchar ( 5 ) DEFAULT NULL , > name_pcode_nf varchar ( 5 ) DEFAULT NULL , > surname_pcode varchar ( 5 ) DEFAULT NULL , > PRIMARY KEY ( id ) > ) ENGINE = InnoDB AUTO_INCREMENT = 2812744 DEFAULT CHARSET = utf8 To restore the partition from the backup tablespace needs to be discarded for that table: mysql > ALTER TABLE name_p4 DISCARD TABLESPACE ; The next step is to copy the .exp and ibd files from the backup to MySQL data directory: $ cp /mnt/backup/2012-08-28_10-29-09/imdb/name#P#p4.exp /var/lib/mysql/imdb/name_p4.exp $ cp /mnt/backup/2012-08-28_10-29-09/imdb/name#P#p4.ibd /var/lib/mysql/imdb/name_p4.ibd Note Make sure that the copied files can be accessed by the user running the MySQL . If you are running the Percona Server for MySQL make sure that variable innodb_import_table_from_xtrabackup is enabled: mysql > SET GLOBAL innodb_import_table_from_xtrabackup = 1 ; The last step is to import the tablespace: mysql > ALTER TABLE name_p4 IMPORT TABLESPACE ; Restoring from the backups in version 5.6 \u00b6 The problem with server versions up to 5.5 is that there is no server support to import either individual partitions or all partitions of a partitioned table, so partitions could only be imported as independent tables. In MySQL and Percona Server for MySQL 5.6 it is possible to exchange individual partitions with independent tables through ALTER TABLE \u2026 EXCHANGE PARTITION command. Note In Percona Server for MySQL 5.6, the variable innodb_import_table_from_xtrabackup was been removed in favor of MySQL Transportable Tablespaces implementation. When importing an entire partitioned table, first import all (sub)partitions as independent tables: mysql > CREATE TABLE ` name_p4 ` ( ` id ` int ( 11 ) NOT NULL AUTO_INCREMENT , ` name ` text NOT NULL , ` imdb_index ` varchar ( 12 ) DEFAULT NULL , ` imdb_id ` int ( 11 ) DEFAULT NULL , ` name_pcode_cf ` varchar ( 5 ) DEFAULT NULL , ` name_pcode_nf ` varchar ( 5 ) DEFAULT NULL , ` surname_pcode ` varchar ( 5 ) DEFAULT NULL , PRIMARY KEY ( ` id ` ) ) ENGINE = InnoDB AUTO_INCREMENT = 2812744 DEFAULT CHARSET = utf8 To restore the partition from the backup tablespace needs to be discarded for that table: mysql > ALTER TABLE name_p4 DISCARD TABLESPACE ; The next step is to copy the .cfg and .ibd files from the backup to MySQL data directory: $ cp /mnt/backup/2013-07-18_10-29-09/imdb/name#P#p4.cfg /var/lib/mysql/imdb/name_p4.cfg $ cp /mnt/backup/2013-07-18_10-29-09/imdb/name#P#p4.ibd /var/lib/mysql/imdb/name_p4.ibd The last step is to import the tablespace: mysql > ALTER TABLE name_p4 IMPORT TABLESPACE ; We can now create the empty partitioned table with exactly the same schema as the table being imported: mysql > CREATE TABLE name2 LIKE name ; Then swap empty partitions from the newly created table with individual tables corresponding to partitions that have been exported/imported on the previous steps: mysql > ALTER TABLE name2 EXCHANGE PARTITION p4 WITH TABLE name_p4 ; In order for this operation to be successful following conditions have to be met.","title":"Backing Up and Restoring Individual Partitions"},{"location":"howtos/recipes_ibkx_partition.html#backing-up-and-restoring-individual-partitions","text":"Percona XtraBackup features partial backups , which means that you may backup individual partitions as well because from the storage engines perspective partitions are regular tables with specially formatted names. The only requirement for this feature is having the innodb_file_per_table option enabled in the server. There is only one caveat about using this kind of backup: you can\u2019t copy back the prepared backup. Restoring partial backups should be done by importing the tables, and not by using the traditional innobackupex --copy-back option. Although there are some scenarios where restoring can be done by copying back the files, this may be lead to database inconsistencies in many cases and it is not the recommended way to do it.","title":"Backing Up and Restoring Individual Partitions"},{"location":"howtos/recipes_ibkx_partition.html#creating-the-backup","text":"There are three ways of specifying which part of the whole data will be backed up: regular expressions ( innobackupex --include ), enumerating the tables in a file ( innobackupex --tables-file ) or providing a list of databases ( innobackupex --databases ). In this example innobackupex --include option will be used. The regular expression provided to this option will be matched against the fully qualified tablename, including the database name, in the form databasename.tablename . For example, this will back up the partition p4 from the table name located in the database imdb : $ innobackupex --include = '^imdb[.]name#P#p4' /mnt/backup/ This will create a timestamped directory with the usual files that innobackupex creates, but only the data files related to the tables matched. Output of the innobackupex will list the skipped tables ... [01] Skipping ./imdb/person_info.ibd [01] Skipping ./imdb/name#P#p5.ibd [01] Skipping ./imdb/name#P#p6.ibd ... imdb.person_info.frm is skipped because it does not match ^imdb[.]name#P#p4. imdb.title.frm is skipped because it does not match ^imdb[.]name#P#p4. imdb.company_type.frm is skipped because it does not match ^imdb[.]name#P#p4. ... Note that this option is passed to xtrabackup --tables and is matched against each table of each database, the directories of each database will be created even if they are empty.","title":"Creating the backup"},{"location":"howtos/recipes_ibkx_partition.html#preparing-the-backup","text":"For preparing partial backups, the procedure is analogous to restoring individual tables : apply the logs and use innobackupex --export : $ innobackupex --apply-log --export /mnt/backup/2012-08-28_10-29-09 You may see warnings in the output about tables that don\u2019t exist. This is because InnoDB -based engines stores its data dictionary inside the tablespace files besides the .frm files. innobackupex will use xtrabackup to remove the missing tables (those that haven\u2019t been selected in the partial backup) from the data dictionary in order to avoid future warnings or errors: InnoDB: in InnoDB data dictionary has tablespace id 51, InnoDB: but tablespace with that id or name does not exist. It will be removed from data dictionary. 120828 10:25:28 InnoDB: Waiting for the background threads to start 120828 10:25:29 Percona XtraDB (http://www.percona.com) 1.1.8-20.1 started; log sequence number 10098323731 xtrabackup: export option is specified. xtrabackup: export metadata of table 'imdb/name#P#p4' to file `./imdb/name#P#p4.exp` (1 indexes) xtrabackup: name=PRIMARY, id.low=73, page=3 You should also see the notification of the creation of a file needed for importing ( .exp file) for each table included in the partial backup: xtrabackup: export option is specified. xtrabackup: export metadata of table 'imdb/name#P#p4' to file `./imdb/name#P#p4.exp` (1 indexes) xtrabackup: name=PRIMARY, id.low=73, page=3 Note that you can use innobackupex --export with innobackupex --apply-log to an already-prepared backup in order to create the .exp files. Finally, check the for the confirmation message in the output: 120828 19:25:38 innobackupex: completed OK!","title":"Preparing the backup"},{"location":"howtos/recipes_ibkx_partition.html#restoring-from-the-backups","text":"Restoring should be done by importing the tables in the partial backup to the server. Note Improved table/partition import is only available in Percona Server for MySQL and MySQL 5.6, this means that partitions which were backed up from different server can be imported as well. For versions older than MySQL 5.6 only partitions from that server can be imported with some important limitations. There should be no DROP/CREATE/TRUNCATE/ALTER TABLE commands issued between taking the backup and importing the partition. First step is to create new table in which data will be restored > mysql > CREATE TABLE name_p4 ( > id int ( 11 ) NOT NULL AUTO_INCREMENT , > name text NOT NULL , > imdb_index varchar ( 12 ) DEFAULT NULL , > imdb_id int ( 11 ) DEFAULT NULL , > name_pcode_cf varchar ( 5 ) DEFAULT NULL , > name_pcode_nf varchar ( 5 ) DEFAULT NULL , > surname_pcode varchar ( 5 ) DEFAULT NULL , > PRIMARY KEY ( id ) > ) ENGINE = InnoDB AUTO_INCREMENT = 2812744 DEFAULT CHARSET = utf8 To restore the partition from the backup tablespace needs to be discarded for that table: mysql > ALTER TABLE name_p4 DISCARD TABLESPACE ; The next step is to copy the .exp and ibd files from the backup to MySQL data directory: $ cp /mnt/backup/2012-08-28_10-29-09/imdb/name#P#p4.exp /var/lib/mysql/imdb/name_p4.exp $ cp /mnt/backup/2012-08-28_10-29-09/imdb/name#P#p4.ibd /var/lib/mysql/imdb/name_p4.ibd Note Make sure that the copied files can be accessed by the user running the MySQL . If you are running the Percona Server for MySQL make sure that variable innodb_import_table_from_xtrabackup is enabled: mysql > SET GLOBAL innodb_import_table_from_xtrabackup = 1 ; The last step is to import the tablespace: mysql > ALTER TABLE name_p4 IMPORT TABLESPACE ;","title":"Restoring from the backups"},{"location":"howtos/recipes_ibkx_partition.html#restoring-from-the-backups-in-version-56","text":"The problem with server versions up to 5.5 is that there is no server support to import either individual partitions or all partitions of a partitioned table, so partitions could only be imported as independent tables. In MySQL and Percona Server for MySQL 5.6 it is possible to exchange individual partitions with independent tables through ALTER TABLE \u2026 EXCHANGE PARTITION command. Note In Percona Server for MySQL 5.6, the variable innodb_import_table_from_xtrabackup was been removed in favor of MySQL Transportable Tablespaces implementation. When importing an entire partitioned table, first import all (sub)partitions as independent tables: mysql > CREATE TABLE ` name_p4 ` ( ` id ` int ( 11 ) NOT NULL AUTO_INCREMENT , ` name ` text NOT NULL , ` imdb_index ` varchar ( 12 ) DEFAULT NULL , ` imdb_id ` int ( 11 ) DEFAULT NULL , ` name_pcode_cf ` varchar ( 5 ) DEFAULT NULL , ` name_pcode_nf ` varchar ( 5 ) DEFAULT NULL , ` surname_pcode ` varchar ( 5 ) DEFAULT NULL , PRIMARY KEY ( ` id ` ) ) ENGINE = InnoDB AUTO_INCREMENT = 2812744 DEFAULT CHARSET = utf8 To restore the partition from the backup tablespace needs to be discarded for that table: mysql > ALTER TABLE name_p4 DISCARD TABLESPACE ; The next step is to copy the .cfg and .ibd files from the backup to MySQL data directory: $ cp /mnt/backup/2013-07-18_10-29-09/imdb/name#P#p4.cfg /var/lib/mysql/imdb/name_p4.cfg $ cp /mnt/backup/2013-07-18_10-29-09/imdb/name#P#p4.ibd /var/lib/mysql/imdb/name_p4.ibd The last step is to import the tablespace: mysql > ALTER TABLE name_p4 IMPORT TABLESPACE ; We can now create the empty partitioned table with exactly the same schema as the table being imported: mysql > CREATE TABLE name2 LIKE name ; Then swap empty partitions from the newly created table with individual tables corresponding to partitions that have been exported/imported on the previous steps: mysql > ALTER TABLE name2 EXCHANGE PARTITION p4 WITH TABLE name_p4 ; In order for this operation to be successful following conditions have to be met.","title":"Restoring from the backups in version 5.6"},{"location":"howtos/recipes_ibkx_stream.html","text":"Make a Streaming Backup \u00b6 Stream mode sends the backup to STDOUT in the xbstream format instead of copying it to the directory named by the first argument. You can pipe the output to gzip , or across the network, to another server. To extract the resulting xbstream file, you must use the xbstream utility: xbstream -x < backup.xbstream . Here are some examples of using the xbstream option for streaming: Action Command Stream the backup into a xbstream archived named \u2018backup.xbstream\u2019 $ xtrabackup --backup --stream=xbstream ./ > backup.xbstream Stream the backup into a xbstream archived named \u2018backup.xbstream\u2019 and compress it $ xtrabackup --backup --stream=xbstream --compress ./ > backup.xbstream Encrypt the backup $ xtrabackup --backup --stream=xbstream ./ > backup.xbstream gzip - | openssl des3 -salt -k \"password\" > backup.xbstream.gz.des3 Send the backup to another server and unpack it $ xtrabackup --backup --compress --stream=xbstream ./ | ssh user@otherhost \"xbstream -x\" Send the backup to another server using netcat On the destination host: $ nc -l 9999 | cat - > /data/backups/backup.xbstream On the source host: $ xtrabackup --backup --stream=xbstream ./ | nc desthost 9999 Send the backup to another server using a one-liner $ ssh user@desthost \"( nc -l 9999 > /data/backups/backup.xbstream & )\" && xtrabackup --backup --stream=xbstream ./ | nc desthost 9999 Throttle the throughput to 10MB/sec using the pipe viewer tool 1 $ xtrabackup --backup --stream=xbstream ./ | pv -q -L10m ssh user@desthost \"cat - > /data/backups/backup.xbstream\" Checksumming the backup during the streaming On the destination host: $ nc -l 9999 | tee >(sha1sum > destination_checksum) > /data/backups/backup.xbstream On the source host: $ xtrabackup --backup --stream=xbstream ./ | tee >(sha1sum > source_checksum) | nc desthost 9999 Compare the checksums on the source host: $ cat source_checksum 65e4f916a49c1f216e0887ce54cf59bf3934dbad Compare the checksums on the destination host: $ cat destination_checksum 65e4f916a49c1f216e0887ce54cf59bf3934dbad Parallel compression with parallel copying backup $ xtrabackup --backup --compress --compress-threads=8 --stream=xbstream --parallel=4 ./ > backup.xbstream Install from the official site or from the distribution package ( apt install pv ). \u21a9","title":"Make a Streaming Backup"},{"location":"howtos/recipes_ibkx_stream.html#make-a-streaming-backup","text":"Stream mode sends the backup to STDOUT in the xbstream format instead of copying it to the directory named by the first argument. You can pipe the output to gzip , or across the network, to another server. To extract the resulting xbstream file, you must use the xbstream utility: xbstream -x < backup.xbstream . Here are some examples of using the xbstream option for streaming: Action Command Stream the backup into a xbstream archived named \u2018backup.xbstream\u2019 $ xtrabackup --backup --stream=xbstream ./ > backup.xbstream Stream the backup into a xbstream archived named \u2018backup.xbstream\u2019 and compress it $ xtrabackup --backup --stream=xbstream --compress ./ > backup.xbstream Encrypt the backup $ xtrabackup --backup --stream=xbstream ./ > backup.xbstream gzip - | openssl des3 -salt -k \"password\" > backup.xbstream.gz.des3 Send the backup to another server and unpack it $ xtrabackup --backup --compress --stream=xbstream ./ | ssh user@otherhost \"xbstream -x\" Send the backup to another server using netcat On the destination host: $ nc -l 9999 | cat - > /data/backups/backup.xbstream On the source host: $ xtrabackup --backup --stream=xbstream ./ | nc desthost 9999 Send the backup to another server using a one-liner $ ssh user@desthost \"( nc -l 9999 > /data/backups/backup.xbstream & )\" && xtrabackup --backup --stream=xbstream ./ | nc desthost 9999 Throttle the throughput to 10MB/sec using the pipe viewer tool 1 $ xtrabackup --backup --stream=xbstream ./ | pv -q -L10m ssh user@desthost \"cat - > /data/backups/backup.xbstream\" Checksumming the backup during the streaming On the destination host: $ nc -l 9999 | tee >(sha1sum > destination_checksum) > /data/backups/backup.xbstream On the source host: $ xtrabackup --backup --stream=xbstream ./ | tee >(sha1sum > source_checksum) | nc desthost 9999 Compare the checksums on the source host: $ cat source_checksum 65e4f916a49c1f216e0887ce54cf59bf3934dbad Compare the checksums on the destination host: $ cat destination_checksum 65e4f916a49c1f216e0887ce54cf59bf3934dbad Parallel compression with parallel copying backup $ xtrabackup --backup --compress --compress-threads=8 --stream=xbstream --parallel=4 ./ > backup.xbstream Install from the official site or from the distribution package ( apt install pv ). \u21a9","title":"Make a Streaming Backup"},{"location":"howtos/recipes_xbk_full.html","text":"Making a Full Backup \u00b6 Backup the InnoDB data and log files - located in /var/lib/mysql/ - to /data/backups/mysql/ (destination). Then, prepare the backup files to be ready to restore or use (make the data files consistent). Make a backup: \u00b6 $ xtrabackup --backup --target-dir = /data/backups/mysql/ Prepare the backup twice: \u00b6 $ xtrabackup --prepare --target-dir = /data/backups/mysql/ $ xtrabackup --prepare --target-dir = /data/backups/mysql/ Success Criterion \u00b6 The exit status of xtrabackup is 0. In the second xtrabackup \u2013prepare step, you should see InnoDB print messages similar to Log file ./ib_logfile0 did not exist: new to be created , followed by a line indicating the log file was created (creating new logs is the purpose of the second preparation). Notes \u00b6 You might want to set the xtrabackup --use-memory option to something similar to the size of your buffer pool, if you are on a dedicated server that has enough free memory. More details here . For a more detailed explanation, see Creating a backup","title":"Making a Full Backup"},{"location":"howtos/recipes_xbk_full.html#making-a-full-backup","text":"Backup the InnoDB data and log files - located in /var/lib/mysql/ - to /data/backups/mysql/ (destination). Then, prepare the backup files to be ready to restore or use (make the data files consistent).","title":"Making a Full Backup"},{"location":"howtos/recipes_xbk_full.html#make-a-backup","text":"$ xtrabackup --backup --target-dir = /data/backups/mysql/","title":"Make a backup:"},{"location":"howtos/recipes_xbk_full.html#prepare-the-backup-twice","text":"$ xtrabackup --prepare --target-dir = /data/backups/mysql/ $ xtrabackup --prepare --target-dir = /data/backups/mysql/","title":"Prepare the backup twice:"},{"location":"howtos/recipes_xbk_full.html#success-criterion","text":"The exit status of xtrabackup is 0. In the second xtrabackup \u2013prepare step, you should see InnoDB print messages similar to Log file ./ib_logfile0 did not exist: new to be created , followed by a line indicating the log file was created (creating new logs is the purpose of the second preparation).","title":"Success Criterion"},{"location":"howtos/recipes_xbk_full.html#notes","text":"You might want to set the xtrabackup --use-memory option to something similar to the size of your buffer pool, if you are on a dedicated server that has enough free memory. More details here . For a more detailed explanation, see Creating a backup","title":"Notes"},{"location":"howtos/recipes_xbk_inc.html","text":"Making an Incremental Backup \u00b6 Backup all the InnoDB data and log files - located in /var/lib/mysql/ - once , then make two daily incremental backups in /data/backups/mysql/ (destination). Finally, prepare the backup files to be ready to restore or use. Create one full backup \u00b6 Making an incremental backup requires a full backup as a base: $ xtrabackup --backup --target-dir = /data/backups/mysql/ It is important that you do not run the xtrabackup --prepare command yet. Create two incremental backups \u00b6 Suppose the full backup is on Monday, and you will create an incremental one on Tuesday: $ xtrabackup --backup --target-dir = /data/backups/inc/tue/ \\ --incremental-basedir = /data/backups/mysql/ and the same policy is applied on Wednesday: $ xtrabackup --backup --target-dir = /data/backups/inc/wed/ \\ --incremental-basedir = /data/backups/inc/tue/ Prepare the base backup \u00b6 Prepare the base backup (Monday\u2019s backup): $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/mysql/ Roll forward the base data to the first increment \u00b6 Roll Monday\u2019s data forward to the state on Tuesday: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/mysql/ \\ --incremental-dir = /data/backups/inc/tue/ Roll forward again to the second increment \u00b6 Roll forward again to the state on Wednesday: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/mysql/ \\ --incremental-dir = /data/backups/inc/wed/ Prepare the whole backup to be ready to use \u00b6 Create the new logs by preparing it: $ xtrabackup --prepare --target-dir = /data/backups/mysql/ Notes \u00b6 You might want to set the xtrabackup --use-memory to speed up the process if you are on a dedicated server that has enough free memory. More details here . A more detailed explanation is here .","title":"Making an Incremental Backup"},{"location":"howtos/recipes_xbk_inc.html#making-an-incremental-backup","text":"Backup all the InnoDB data and log files - located in /var/lib/mysql/ - once , then make two daily incremental backups in /data/backups/mysql/ (destination). Finally, prepare the backup files to be ready to restore or use.","title":"Making an Incremental Backup"},{"location":"howtos/recipes_xbk_inc.html#create-one-full-backup","text":"Making an incremental backup requires a full backup as a base: $ xtrabackup --backup --target-dir = /data/backups/mysql/ It is important that you do not run the xtrabackup --prepare command yet.","title":"Create one full backup"},{"location":"howtos/recipes_xbk_inc.html#create-two-incremental-backups","text":"Suppose the full backup is on Monday, and you will create an incremental one on Tuesday: $ xtrabackup --backup --target-dir = /data/backups/inc/tue/ \\ --incremental-basedir = /data/backups/mysql/ and the same policy is applied on Wednesday: $ xtrabackup --backup --target-dir = /data/backups/inc/wed/ \\ --incremental-basedir = /data/backups/inc/tue/","title":"Create two incremental backups"},{"location":"howtos/recipes_xbk_inc.html#prepare-the-base-backup","text":"Prepare the base backup (Monday\u2019s backup): $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/mysql/","title":"Prepare the base backup"},{"location":"howtos/recipes_xbk_inc.html#roll-forward-the-base-data-to-the-first-increment","text":"Roll Monday\u2019s data forward to the state on Tuesday: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/mysql/ \\ --incremental-dir = /data/backups/inc/tue/","title":"Roll forward the base data to the first increment"},{"location":"howtos/recipes_xbk_inc.html#roll-forward-again-to-the-second-increment","text":"Roll forward again to the state on Wednesday: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/mysql/ \\ --incremental-dir = /data/backups/inc/wed/","title":"Roll forward again to the second increment"},{"location":"howtos/recipes_xbk_inc.html#prepare-the-whole-backup-to-be-ready-to-use","text":"Create the new logs by preparing it: $ xtrabackup --prepare --target-dir = /data/backups/mysql/","title":"Prepare the whole backup to be ready to use"},{"location":"howtos/recipes_xbk_inc.html#notes","text":"You might want to set the xtrabackup --use-memory to speed up the process if you are on a dedicated server that has enough free memory. More details here . A more detailed explanation is here .","title":"Notes"},{"location":"howtos/recipes_xbk_restore.html","text":"Restoring the Backup \u00b6 Because xtrabackup does not copy MyISAM files, .frm files, and the rest of the database, you might need to back those up separately. To restore the InnoDB data, simply do something like the following after preparing: cd /data/backups/mysql/ rsync -rvt --exclude 'xtrabackup_checkpoints' --exclude 'xtrabackup_logfile' \\ ./ /var/lib/mysql chown -R mysql:mysql /var/lib/mysql/","title":"Restoring the Backup"},{"location":"howtos/recipes_xbk_restore.html#restoring-the-backup","text":"Because xtrabackup does not copy MyISAM files, .frm files, and the rest of the database, you might need to back those up separately. To restore the InnoDB data, simply do something like the following after preparing: cd /data/backups/mysql/ rsync -rvt --exclude 'xtrabackup_checkpoints' --exclude 'xtrabackup_logfile' \\ ./ /var/lib/mysql chown -R mysql:mysql /var/lib/mysql/","title":"Restoring the Backup"},{"location":"howtos/setting_up_replication.html","text":"How to setup a replica for replication in 6 simple steps with Percona XtraBackup \u00b6 Data is, by far, the most valuable part of a system. Having a backup done systematically and available for a rapid recovery in case of failure is admittedly essential to a system. However, it is not common practice because of its costs, infrastructure needed or even the boredom associated to the task. Percona XtraBackup is designed to solve this problem. You can have almost real-time backups in 6 simple steps by setting up a replication environment with Percona XtraBackup . Percona XtraBackup is a tool for backing up your data without interruption. It performs \u201chot backups\u201d on unmodified versions of MySQL servers (5.1, 5.5 and 5.6), as well as MariaDB and Percona Server for MySQL . It is a totally free and open source software distributed only under the GPLv2 license. All the things you will need \u00b6 Setting up a replica for replication with Percona XtraBackup is a straightforward procedure. You must have the following things to complete the steps: Source A system with a MySQL -based server installed, configured and running. This system is called the Source , and is where your data is stored and used for replication. We assume the following about this server: Communication enabled with others by the standard TCP/IP port Installed and configured SSH server Configured user account in the system with the appropriate permissions and privileges Enabled binlogs and the server-id set up to 1 Replica MySQL -based server installed on another server. This server is called the Replica . We assume the same configuration as the Source , except that the server-id on the Replica is 2. Percona XtraBackup The backup tool should be installed in both servers for convenience. Note It is not recommended to mix MySQL variants (Percona Server, MySQL, MariaDB) in your replication setup. This may produce incorrect xtrabackup_slave_info file when adding a new replica. STEP 1: Make a backup on the Source and prepare it \u00b6 On the Source , issue the following command to a shell: $ xtrabackup --backup --user = yourDBuser --password = MaGiCdB1 --target-dir = /path/to/backupdir After this is finished you should see: xtrabackup: completed OK! This action copies your MySQL data dir to the /path/to/backupdir directory. You have told Percona XtraBackup to connect to the database server using your database user and password, and do a hot backup of all your data in it (all MyISAM , InnoDB tables and indexes in them). On the Source , to make the snapshot consistent, prepare the data: $ xtrabackup --user = yourDBuser --password = MaGiCdB1 \\ --prepare --target-dir = /path/to/backupdir Select the path where your snapshot has been taken. If everything is ok you should see the same OK message. Now the transaction logs are applied to the data files, and new ones are created: your data files are ready to be used by the MySQL server. Percona XtraBackup knows where your data is by reading your my.cnf. If you have your configuration file in a non-standard place, use the flag xtrabackup --defaults-file =/location/of/my.cnf . If you want to skip writing the user name and password every time you want to access MySQL , set the information in .mylogin.cnf as follows: $ mysql_config_editor set --login-path = client --host = localhost --user = root --password This setting provides root access to MySQL. See also MySQL Documentaiton: MySQL Configuration Utility STEP 2: Copy backed up data to the Replica \u00b6 Use rsync or scp to copy the data from Source to Replica. If you\u2019re syncing the data directly to replica\u2019s data directory it\u2019s advised to stop the mysqld there. On the Source , run the following command: $ rsync -avpP -e ssh /path/to/backupdir Replica:/path/to/mysql/ After data has been copied you can back up the original or previously installed MySQL datadir ( NOTE : Make sure mysqld is shut down before you move the contents of its datadir, or move the snapshot into its datadir.). Run the following command on the Replica : $ mv /path/to/mysql/datadir /path/to/mysql/datadir_bak and, on the Replica , move the snapshot from the Source in its place: $ xtrabackup --move-back --target-dir = /path/to/mysql/backupdir After you have copied data to the Replica , make sure the Replica MySQL has the proper permissions: $ chown mysql:mysql /path/to/mysql/datadir In case the ibdata and iblog files are located in different directories outside the datadir, put them in their proper place after the logs have been applied. STEP 3: Configure the Source MySQL server \u00b6 On the Source , add the appropriate grant to allow the replica to connect to the source: > GRANT REPLICATION SLAVE ON * . * TO 'repl' @ '$replicaip' IDENTIFIED BY '$replicapass' ; Also make sure that firewall rules are correct and that the Replica can connect to the Source . Test that you can run the mysql client on the Replica , connect to the Source , and authenticate. $ mysql --host = Source --user = repl --password = $replicapass Verify the privileges. mysql > SHOW GRANTS ; STEP 4: Configure the Replica MySQL server \u00b6 Copy the my.cnf file from the Source to the Replica . On the Replica , run the following: $ scp user@Source:/etc/mysql/my.cnf /etc/mysql/my.cnf then change the following options in /etc/mysql/my.cnf: server-id=2 and start/restart mysqld on the Replica . In case you\u2019re using init script on Debian based system to start mysqld, be sure that the password for debian-sys-maint user has been updated and is the same as the user\u2019s password on the Source . This password can be seen and updated in /etc/mysql/debian.cnf . STEP 5: Start the replication \u00b6 On the Replica , look at the content of the file xtrabackup_binlog_info , it will be something like: $ cat /var/lib/mysql/xtrabackup_binlog_info Source-bin.000001 481 Execute the CHANGE MASTER statement on a MySQL console and use the username and password you\u2019ve set up in STEP 3: TheSlave * mysql > CHANGE MASTER TO MASTER_HOST = '$sourceip' , MASTER_USER = 'repl' , MASTER_PASSWORD = '$replicapass' , MASTER_LOG_FILE = 'Source-bin.000001' , MASTER_LOG_POS = 481 ; and start the replica: > START SLAVE ; STEP 6: Check \u00b6 On the Replica , check that everything went OK with: TheSlave | mysql > SHOW SLAVE STATUS \\ G The results should look like the following: ... Slave_IO_Running: Yes Slave_SQL_Running: Yes ... Seconds_Behind_Master: 13 ... Both IO and SQL threads need to be running. The Seconds_Behind_Master means the SQL currently being executed has a current_timestamp of 13 seconds ago. It is an estimation of the lag between the Source and the Replica . Note that at the beginning, a high value could be shown because the Replica has to \u201ccatch up\u201d with the Source . Adding more replicas to the Source \u00b6 You can use this procedure with slight variation to add new replicas to a source. We will use Percona XtraBackup to clone an already configured replica. We will continue using the previous scenario for convenience but we will add the NewReplica to the plot. At the Replica , do a full backup: $ xtrabackup --user = yourDBuser --password = MaGiCiGaM \\ --backup --slave-info --target-dir = /path/to/backupdir By using the xtrabackup --slave-info Percona XtraBackup creates additional file called xtrabackup_slave_info. On the Replica , apply the logs: $ xtrabackup --prepare --use-memory = 2G --target-dir = /path/to/backupdir/ Note In the prepare phase, the --use-memory parameter speeds up the process if the amount of RAM assigned to the option is available. Use the parameter only in the prepare phase. In the other phases the parameter makes the application lazy allocate this memory (reserve) but does not affect database pages. Copy the directory from the Replica to the NewReplica ( NOTE : Make sure mysqld is shut down on the NewReplica before you copy the contents the snapshot into its datadir .): rsync -avprP -e ssh /path/to/backupdir NewReplica:/path/to/mysql/datadir On the Source , add additional grant on the source: > GRANT REPLICATION SLAVE ON * . * TO 'repl' @ '$newreplicaip' IDENTIFIED BY '$replicapass' ; Copy the configuration file from the Replica . On the NewReplica , run the following command: $ scp user@Replica:/etc/mysql/my.cnf /etc/mysql/my.cnf Make sure you change the server-id variable in /etc/mysql/my.cnf to 3 and disable the replication on start: skip-slave-start server-id=3 After setting server_id , start mysqld . Fetch the master_log_file and master_log_pos from the file xtrabackup_slave_info , execute the statement for setting up the source and the log file for the NewReplica : TheNEWSlave | mysql > CHANGE MASTER TO MASTER_HOST = '$sourceip' , MASTER_USER = 'repl' , MASTER_PASSWORD = '$replicapass' , MASTER_LOG_FILE = 'Source-bin.000001' , MASTER_LOG_POS = 481 ; and start the replica: > START SLAVE ; If both IO and SQL threads are running when you check the the NewReplica , server is replicating the Source .","title":"How to setup a replica for replication in 6 simple steps with Percona XtraBackup"},{"location":"howtos/setting_up_replication.html#how-to-setup-a-replica-for-replication-in-6-simple-steps-with-percona-xtrabackup","text":"Data is, by far, the most valuable part of a system. Having a backup done systematically and available for a rapid recovery in case of failure is admittedly essential to a system. However, it is not common practice because of its costs, infrastructure needed or even the boredom associated to the task. Percona XtraBackup is designed to solve this problem. You can have almost real-time backups in 6 simple steps by setting up a replication environment with Percona XtraBackup . Percona XtraBackup is a tool for backing up your data without interruption. It performs \u201chot backups\u201d on unmodified versions of MySQL servers (5.1, 5.5 and 5.6), as well as MariaDB and Percona Server for MySQL . It is a totally free and open source software distributed only under the GPLv2 license.","title":"How to setup a replica for replication in 6 simple steps with Percona XtraBackup"},{"location":"howtos/setting_up_replication.html#all-the-things-you-will-need","text":"Setting up a replica for replication with Percona XtraBackup is a straightforward procedure. You must have the following things to complete the steps: Source A system with a MySQL -based server installed, configured and running. This system is called the Source , and is where your data is stored and used for replication. We assume the following about this server: Communication enabled with others by the standard TCP/IP port Installed and configured SSH server Configured user account in the system with the appropriate permissions and privileges Enabled binlogs and the server-id set up to 1 Replica MySQL -based server installed on another server. This server is called the Replica . We assume the same configuration as the Source , except that the server-id on the Replica is 2. Percona XtraBackup The backup tool should be installed in both servers for convenience. Note It is not recommended to mix MySQL variants (Percona Server, MySQL, MariaDB) in your replication setup. This may produce incorrect xtrabackup_slave_info file when adding a new replica.","title":"All the things you will need"},{"location":"howtos/setting_up_replication.html#step-1-make-a-backup-on-the-source-and-prepare-it","text":"On the Source , issue the following command to a shell: $ xtrabackup --backup --user = yourDBuser --password = MaGiCdB1 --target-dir = /path/to/backupdir After this is finished you should see: xtrabackup: completed OK! This action copies your MySQL data dir to the /path/to/backupdir directory. You have told Percona XtraBackup to connect to the database server using your database user and password, and do a hot backup of all your data in it (all MyISAM , InnoDB tables and indexes in them). On the Source , to make the snapshot consistent, prepare the data: $ xtrabackup --user = yourDBuser --password = MaGiCdB1 \\ --prepare --target-dir = /path/to/backupdir Select the path where your snapshot has been taken. If everything is ok you should see the same OK message. Now the transaction logs are applied to the data files, and new ones are created: your data files are ready to be used by the MySQL server. Percona XtraBackup knows where your data is by reading your my.cnf. If you have your configuration file in a non-standard place, use the flag xtrabackup --defaults-file =/location/of/my.cnf . If you want to skip writing the user name and password every time you want to access MySQL , set the information in .mylogin.cnf as follows: $ mysql_config_editor set --login-path = client --host = localhost --user = root --password This setting provides root access to MySQL. See also MySQL Documentaiton: MySQL Configuration Utility","title":"STEP 1: Make a backup on the Source and prepare it"},{"location":"howtos/setting_up_replication.html#step-2-copy-backed-up-data-to-the-replica","text":"Use rsync or scp to copy the data from Source to Replica. If you\u2019re syncing the data directly to replica\u2019s data directory it\u2019s advised to stop the mysqld there. On the Source , run the following command: $ rsync -avpP -e ssh /path/to/backupdir Replica:/path/to/mysql/ After data has been copied you can back up the original or previously installed MySQL datadir ( NOTE : Make sure mysqld is shut down before you move the contents of its datadir, or move the snapshot into its datadir.). Run the following command on the Replica : $ mv /path/to/mysql/datadir /path/to/mysql/datadir_bak and, on the Replica , move the snapshot from the Source in its place: $ xtrabackup --move-back --target-dir = /path/to/mysql/backupdir After you have copied data to the Replica , make sure the Replica MySQL has the proper permissions: $ chown mysql:mysql /path/to/mysql/datadir In case the ibdata and iblog files are located in different directories outside the datadir, put them in their proper place after the logs have been applied.","title":"STEP 2:  Copy backed up data to the Replica"},{"location":"howtos/setting_up_replication.html#step-3-configure-the-source-mysql-server","text":"On the Source , add the appropriate grant to allow the replica to connect to the source: > GRANT REPLICATION SLAVE ON * . * TO 'repl' @ '$replicaip' IDENTIFIED BY '$replicapass' ; Also make sure that firewall rules are correct and that the Replica can connect to the Source . Test that you can run the mysql client on the Replica , connect to the Source , and authenticate. $ mysql --host = Source --user = repl --password = $replicapass Verify the privileges. mysql > SHOW GRANTS ;","title":"STEP 3: Configure the Source MySQL server"},{"location":"howtos/setting_up_replication.html#step-4-configure-the-replica-mysql-server","text":"Copy the my.cnf file from the Source to the Replica . On the Replica , run the following: $ scp user@Source:/etc/mysql/my.cnf /etc/mysql/my.cnf then change the following options in /etc/mysql/my.cnf: server-id=2 and start/restart mysqld on the Replica . In case you\u2019re using init script on Debian based system to start mysqld, be sure that the password for debian-sys-maint user has been updated and is the same as the user\u2019s password on the Source . This password can be seen and updated in /etc/mysql/debian.cnf .","title":"STEP 4: Configure the Replica MySQL server"},{"location":"howtos/setting_up_replication.html#step-5-start-the-replication","text":"On the Replica , look at the content of the file xtrabackup_binlog_info , it will be something like: $ cat /var/lib/mysql/xtrabackup_binlog_info Source-bin.000001 481 Execute the CHANGE MASTER statement on a MySQL console and use the username and password you\u2019ve set up in STEP 3: TheSlave * mysql > CHANGE MASTER TO MASTER_HOST = '$sourceip' , MASTER_USER = 'repl' , MASTER_PASSWORD = '$replicapass' , MASTER_LOG_FILE = 'Source-bin.000001' , MASTER_LOG_POS = 481 ; and start the replica: > START SLAVE ;","title":"STEP 5: Start the replication"},{"location":"howtos/setting_up_replication.html#step-6-check","text":"On the Replica , check that everything went OK with: TheSlave | mysql > SHOW SLAVE STATUS \\ G The results should look like the following: ... Slave_IO_Running: Yes Slave_SQL_Running: Yes ... Seconds_Behind_Master: 13 ... Both IO and SQL threads need to be running. The Seconds_Behind_Master means the SQL currently being executed has a current_timestamp of 13 seconds ago. It is an estimation of the lag between the Source and the Replica . Note that at the beginning, a high value could be shown because the Replica has to \u201ccatch up\u201d with the Source .","title":"STEP 6: Check"},{"location":"howtos/setting_up_replication.html#adding-more-replicas-to-the-source","text":"You can use this procedure with slight variation to add new replicas to a source. We will use Percona XtraBackup to clone an already configured replica. We will continue using the previous scenario for convenience but we will add the NewReplica to the plot. At the Replica , do a full backup: $ xtrabackup --user = yourDBuser --password = MaGiCiGaM \\ --backup --slave-info --target-dir = /path/to/backupdir By using the xtrabackup --slave-info Percona XtraBackup creates additional file called xtrabackup_slave_info. On the Replica , apply the logs: $ xtrabackup --prepare --use-memory = 2G --target-dir = /path/to/backupdir/ Note In the prepare phase, the --use-memory parameter speeds up the process if the amount of RAM assigned to the option is available. Use the parameter only in the prepare phase. In the other phases the parameter makes the application lazy allocate this memory (reserve) but does not affect database pages. Copy the directory from the Replica to the NewReplica ( NOTE : Make sure mysqld is shut down on the NewReplica before you copy the contents the snapshot into its datadir .): rsync -avprP -e ssh /path/to/backupdir NewReplica:/path/to/mysql/datadir On the Source , add additional grant on the source: > GRANT REPLICATION SLAVE ON * . * TO 'repl' @ '$newreplicaip' IDENTIFIED BY '$replicapass' ; Copy the configuration file from the Replica . On the NewReplica , run the following command: $ scp user@Replica:/etc/mysql/my.cnf /etc/mysql/my.cnf Make sure you change the server-id variable in /etc/mysql/my.cnf to 3 and disable the replication on start: skip-slave-start server-id=3 After setting server_id , start mysqld . Fetch the master_log_file and master_log_pos from the file xtrabackup_slave_info , execute the statement for setting up the source and the log file for the NewReplica : TheNEWSlave | mysql > CHANGE MASTER TO MASTER_HOST = '$sourceip' , MASTER_USER = 'repl' , MASTER_PASSWORD = '$replicapass' , MASTER_LOG_FILE = 'Source-bin.000001' , MASTER_LOG_POS = 481 ; and start the replica: > START SLAVE ; If both IO and SQL threads are running when you check the the NewReplica , server is replicating the Source .","title":"Adding more replicas to the Source"},{"location":"howtos/ssh_server.html","text":"Installing and configuring a SSH server \u00b6 Many Linux distributions only install the ssh client by default. If you don\u2019t have the ssh server installed already, the easiest way of doing it is by using your distribution\u2019s packaging system. Using apt, run the following: $ sudo apt install openssh-server Using Red Hat Linux or a derivative, use the following: $ sudo yum install openssh-server Review your distribution\u2019s documentation on how to configure the server.","title":"Installing and configuring a SSH server"},{"location":"howtos/ssh_server.html#installing-and-configuring-a-ssh-server","text":"Many Linux distributions only install the ssh client by default. If you don\u2019t have the ssh server installed already, the easiest way of doing it is by using your distribution\u2019s packaging system. Using apt, run the following: $ sudo apt install openssh-server Using Red Hat Linux or a derivative, use the following: $ sudo yum install openssh-server Review your distribution\u2019s documentation on how to configure the server.","title":"Installing and configuring a SSH server"},{"location":"innobackupex/creating_a_backup_ibk.html","text":"Creating a Backup with innobackupex \u00b6 innobackupex is the tool which provides functionality to backup a whole MySQL database instance using the xtrabackup in combination with tools like xbstream and xbcrypt. To create a full backup, invoke the script with the options needed to connect to the server and only one argument: the path to the directory where the backup will be stored $ innobackupex --user = DBUSER --password = DBUSERPASS /path/to/BACKUP-DIR/ and check the last line of the output for a confirmation message: innobackupex: Backup created in directory '/path/to/BACKUP-DIR/2013-03-25_00-00-09' innobackupex: MySQL binlog position: filename 'mysql-bin.000003', position 1946 111225 00:00:53 innobackupex: completed OK! The backup will be stored within a time stamped directory created in the provided path, /path/to/BACKUP-DIR/2013-03-25_00-00-09 in this particular example. Under the hood \u00b6 innobackupex called xtrabackup binary to backup all the data of InnoDB tables (see Creating a backup for details on this process) and copied all the table definitions in the database (.frm files), data and files related to MyISAM , MERGE <.MRG> (reference to other tables), CSV <.CSV> and ARCHIVE <.ARM> tables, along with triggers <.TRG> and database configuration information <.opt> to a time stamped directory created in the provided path. It will also create the following files for convenience on the created directory. innobackupex --no-timestamp \u00b6 This option tells innobackupex not to create a time stamped directory to store the backup: innobackupex --user=DBUSER --password=DBUSERPASS /path/to/BACKUP-DIR/ --no-timestamp innobackupex will create the BACKUP-DIR subdirectory (or fail if exists) and store the backup inside of it. innobackupex --defaults-file \u00b6 You can provide another configuration file to innobackupex with this option. The only limitation is that it has to be the first option passed : $ innobackupex --defaults-file = /tmp/other-my.cnf --user = DBUSER --password = DBUSERPASS /path/to/BACKUP-DIR/","title":"Creating a Backup with innobackupex"},{"location":"innobackupex/creating_a_backup_ibk.html#creating-a-backup-with-innobackupex","text":"innobackupex is the tool which provides functionality to backup a whole MySQL database instance using the xtrabackup in combination with tools like xbstream and xbcrypt. To create a full backup, invoke the script with the options needed to connect to the server and only one argument: the path to the directory where the backup will be stored $ innobackupex --user = DBUSER --password = DBUSERPASS /path/to/BACKUP-DIR/ and check the last line of the output for a confirmation message: innobackupex: Backup created in directory '/path/to/BACKUP-DIR/2013-03-25_00-00-09' innobackupex: MySQL binlog position: filename 'mysql-bin.000003', position 1946 111225 00:00:53 innobackupex: completed OK! The backup will be stored within a time stamped directory created in the provided path, /path/to/BACKUP-DIR/2013-03-25_00-00-09 in this particular example.","title":"Creating a Backup with innobackupex"},{"location":"innobackupex/creating_a_backup_ibk.html#under-the-hood","text":"innobackupex called xtrabackup binary to backup all the data of InnoDB tables (see Creating a backup for details on this process) and copied all the table definitions in the database (.frm files), data and files related to MyISAM , MERGE <.MRG> (reference to other tables), CSV <.CSV> and ARCHIVE <.ARM> tables, along with triggers <.TRG> and database configuration information <.opt> to a time stamped directory created in the provided path. It will also create the following files for convenience on the created directory.","title":"Under the hood"},{"location":"innobackupex/creating_a_backup_ibk.html#innobackupex-no-timestamp","text":"This option tells innobackupex not to create a time stamped directory to store the backup: innobackupex --user=DBUSER --password=DBUSERPASS /path/to/BACKUP-DIR/ --no-timestamp innobackupex will create the BACKUP-DIR subdirectory (or fail if exists) and store the backup inside of it.","title":"innobackupex --no-timestamp"},{"location":"innobackupex/creating_a_backup_ibk.html#innobackupex-defaults-file","text":"You can provide another configuration file to innobackupex with this option. The only limitation is that it has to be the first option passed : $ innobackupex --defaults-file = /tmp/other-my.cnf --user = DBUSER --password = DBUSERPASS /path/to/BACKUP-DIR/","title":"innobackupex --defaults-file"},{"location":"innobackupex/encrypted_backups_innobackupex.html","text":"Encrypted Backups \u00b6 Percona XtraBackup has implemented support for encrypted backups. It can be used to encrypt/decrypt local or streaming backup with xbstream option (streaming tar backups are not supported) in order to add another layer of protection to the backups. Encryption is done with the libgcrypt library. Creating Encrypted Backups \u00b6 To make an encrypted backup following options need to be specified (options innobackupex --encrypt-key and innobackupex --encrypt-key-file are mutually exclusive, i.e. just one of them needs to be provided): innobackupex \u2013encrypt innobackupex \u2013encrypt-key innobackupex \u2013encrypt-key-file Both innobackupex --encrypt-key option and innobackupex --encrypt-key-file option can be used to specify the encryption key. Encryption key can be generated with a command like: $ openssl rand -base64 24 Example output of that command should look like this: GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs This value then can be used as the encryption key Using innobackupex --encrypt-key \u00b6 Example of the innobackupex command using the innobackupex \u2013encrypt-key should look like this $ innobackupex --encrypt = AES256 --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" /data/backups Using innobackupex --encrypt-key-file \u00b6 Example of the innobackupex command using the innobackupex --encrypt-key-file should look like this: $ innobackupex --encrypt = AES256 --encrypt-key-file = /data/backups/keyfile /data/backups Note Depending on the text editor used for making the KEYFILE , text file in some cases can contain the CRLF and this will cause the key size to grow and thus making it invalid. Suggested way to do this would be to create the file with: echo -n \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" > /data/backups/keyfile Both of these examples will create a timestamped directory in /data/backups containing the encrypted backup. Note You can use the innobackupex \u2013no-timestamp option to override this behavior and the backup will be created in the given directory. Optimizing the encryption process \u00b6 Two new options have been introduced with the encrypted backups that can be used to speed up the encryption process. These are innobackupex --encrypt-threads and innobackupex --encrypt-chunk-size . By using the innobackupex --encrypt-threads option multiple threads can be specified to be used for encryption in parallel. Option innobackupex --encrypt-chunk-size can be used to specify the size (in bytes) of the working encryption buffer for each encryption thread (default is 64K). Decrypting Encrypted Backups \u00b6 Backups can be decrypted with The xbcrypt binary . The following one-liner can be used to encrypt the whole folder: $ for i in ` find . -iname \"*\\.xbcrypt\" ` ; do xbcrypt -d --encrypt-key-file = /root/secret_key --encrypt-algo = AES256 < $i > $( dirname $i ) / $( basename $i .xbcrypt ) && rm $i ; done Percona XtraBackup innobackupex --decrypt option has been implemented that can be used to decrypt the backups: $ innobackupex --decrypt = AES256 --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" /data/backups/2015-03-18_08-31-35/ Percona XtraBackup doesn\u2019t automatically remove the encrypted files. In order to clean up the backup directory users should remove the \\*.xbcrypt files. Note innobackupex --parallel can be used with innobackupex --decrypt option to decrypt multiple files simultaneously. When the files have been decrypted backup can be prepared. Preparing Encrypted Backups \u00b6 After the backups have been decrypted, they can be prepared the same way as the standard full backups with the innobackupex --apply-log option: $ innobackupex --apply-log /data/backups/2015-03-18_08-31-35/ Note Percona XtraBackup doesn\u2019t automatically remove the encrypted files. In order to clean up the backup directory users should remove the \\*.xbcrypt files. Restoring Encrypted Backups \u00b6 innobackupex has a innobackupex --copy-back option, which performs the restoration of a backup to the server\u2019s datadir $ innobackupex --copy-back /path/to/BACKUP-DIR It will copy all the data-related files back to the server\u2019s datadir , determined by the server\u2019s my.cnf configuration file. You should check the last line of the output for a success message: innobackupex: Finished copying back files. 150318 11:08:13 innobackupex: completed OK! Other Reading \u00b6 The Libgcrypt Reference Manual","title":"Encrypted Backups"},{"location":"innobackupex/encrypted_backups_innobackupex.html#encrypted-backups","text":"Percona XtraBackup has implemented support for encrypted backups. It can be used to encrypt/decrypt local or streaming backup with xbstream option (streaming tar backups are not supported) in order to add another layer of protection to the backups. Encryption is done with the libgcrypt library.","title":"Encrypted Backups"},{"location":"innobackupex/encrypted_backups_innobackupex.html#creating-encrypted-backups","text":"To make an encrypted backup following options need to be specified (options innobackupex --encrypt-key and innobackupex --encrypt-key-file are mutually exclusive, i.e. just one of them needs to be provided): innobackupex \u2013encrypt innobackupex \u2013encrypt-key innobackupex \u2013encrypt-key-file Both innobackupex --encrypt-key option and innobackupex --encrypt-key-file option can be used to specify the encryption key. Encryption key can be generated with a command like: $ openssl rand -base64 24 Example output of that command should look like this: GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs This value then can be used as the encryption key","title":"Creating Encrypted Backups"},{"location":"innobackupex/encrypted_backups_innobackupex.html#using-innobackupex-encrypt-key","text":"Example of the innobackupex command using the innobackupex \u2013encrypt-key should look like this $ innobackupex --encrypt = AES256 --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" /data/backups","title":"Using innobackupex --encrypt-key"},{"location":"innobackupex/encrypted_backups_innobackupex.html#using-innobackupex-encrypt-key-file","text":"Example of the innobackupex command using the innobackupex --encrypt-key-file should look like this: $ innobackupex --encrypt = AES256 --encrypt-key-file = /data/backups/keyfile /data/backups Note Depending on the text editor used for making the KEYFILE , text file in some cases can contain the CRLF and this will cause the key size to grow and thus making it invalid. Suggested way to do this would be to create the file with: echo -n \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" > /data/backups/keyfile Both of these examples will create a timestamped directory in /data/backups containing the encrypted backup. Note You can use the innobackupex \u2013no-timestamp option to override this behavior and the backup will be created in the given directory.","title":"Using innobackupex --encrypt-key-file"},{"location":"innobackupex/encrypted_backups_innobackupex.html#optimizing-the-encryption-process","text":"Two new options have been introduced with the encrypted backups that can be used to speed up the encryption process. These are innobackupex --encrypt-threads and innobackupex --encrypt-chunk-size . By using the innobackupex --encrypt-threads option multiple threads can be specified to be used for encryption in parallel. Option innobackupex --encrypt-chunk-size can be used to specify the size (in bytes) of the working encryption buffer for each encryption thread (default is 64K).","title":"Optimizing the encryption process"},{"location":"innobackupex/encrypted_backups_innobackupex.html#decrypting-encrypted-backups","text":"Backups can be decrypted with The xbcrypt binary . The following one-liner can be used to encrypt the whole folder: $ for i in ` find . -iname \"*\\.xbcrypt\" ` ; do xbcrypt -d --encrypt-key-file = /root/secret_key --encrypt-algo = AES256 < $i > $( dirname $i ) / $( basename $i .xbcrypt ) && rm $i ; done Percona XtraBackup innobackupex --decrypt option has been implemented that can be used to decrypt the backups: $ innobackupex --decrypt = AES256 --encrypt-key = \"GCHFLrDFVx6UAsRb88uLVbAVWbK+Yzfs\" /data/backups/2015-03-18_08-31-35/ Percona XtraBackup doesn\u2019t automatically remove the encrypted files. In order to clean up the backup directory users should remove the \\*.xbcrypt files. Note innobackupex --parallel can be used with innobackupex --decrypt option to decrypt multiple files simultaneously. When the files have been decrypted backup can be prepared.","title":"Decrypting Encrypted Backups"},{"location":"innobackupex/encrypted_backups_innobackupex.html#preparing-encrypted-backups","text":"After the backups have been decrypted, they can be prepared the same way as the standard full backups with the innobackupex --apply-log option: $ innobackupex --apply-log /data/backups/2015-03-18_08-31-35/ Note Percona XtraBackup doesn\u2019t automatically remove the encrypted files. In order to clean up the backup directory users should remove the \\*.xbcrypt files.","title":"Preparing Encrypted Backups"},{"location":"innobackupex/encrypted_backups_innobackupex.html#restoring-encrypted-backups","text":"innobackupex has a innobackupex --copy-back option, which performs the restoration of a backup to the server\u2019s datadir $ innobackupex --copy-back /path/to/BACKUP-DIR It will copy all the data-related files back to the server\u2019s datadir , determined by the server\u2019s my.cnf configuration file. You should check the last line of the output for a success message: innobackupex: Finished copying back files. 150318 11:08:13 innobackupex: completed OK!","title":"Restoring Encrypted Backups"},{"location":"innobackupex/encrypted_backups_innobackupex.html#other-reading","text":"The Libgcrypt Reference Manual","title":"Other Reading"},{"location":"innobackupex/how_innobackupex_works.html","text":"How innobackupex Works \u00b6 From Percona XtraBackup version 2.3 innobackupex is has been rewritten in C and set up as a symlink to the xtrabackup . innobackupex supports all features and syntax as 2.2 version did, but it is now deprecated and will be removed in next major release. Syntax for new features will not be added to the innobackupex, only to the xtrabackup. The following describes the rationale behind innobackupex actions. Making a Backup \u00b6 If no mode is specified, innobackupex will assume the backup mode. By default, it runs xtrabackup and lets it copy the InnoDB data files. When xtrabackup finishes that, innobackupex sees it create the xtrabackup_suspended_2 file and executes FLUSH TABLES WITH READ LOCK . xtrabackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Then it begins copying the rest of the files. innobackupex will then check MySQL variables to determine which features are supported by server. Special interest are backup locks, changed page bitmaps, GTID mode, etc. If everything goes well, the binary is started as a child process. innobackupex will wait for replicas in a replication setup if the option innobackupex --safe-slave-backup is set and will flush all tables with READ LOCK , preventing all MyISAM tables from writing (unless option innobackupex --no-lock is specified). Note Locking is done only for MyISAM and other non-InnoDB tables, and only after Percona XtraBackup is finished backing up all InnoDB/XtraDB data and logs. Percona XtraBackup will use backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Once this is done, the backup of the files will begin. It will backup .frm , .MRG , .MYD , .MYI , .TRG , .TRN , .ARM , .ARZ , .CSM , .CSV , .par , and .opt files. When all the files are backed up, it resumes ibbackup and wait until it finishes copying the transactions done while the backup was done. Then, the tables are unlocked, the replica is started (if the option innobackupex --safe-slave-backup was used) and the connection with the server is closed. Then, it removes the xtrabackup_suspended_2 file and permits xtrabackup to exit. It will also create the following files in the directory of the backup: xtrabackup_checkpoints containing the `LSN` and the type of backup; xtrabackup_binlog_info containing the position of the binary log at the moment of backing up; xtrabackup_binlog_pos_innodb containing the position of the binary log at the moment of backing up relative to *InnoDB* transactions; xtrabackup_slave_info containing the MySQL binlog position of the source server in a replication setup via `SHOW SLAVE STATUS` if the innobackupex \u2013slave-info option is passed; backup-my.cnf containing only the my.cnf options required for the backup. For example, innodb_data_file_path, innodb_log_files_in_group, innodb_log_file_size, innodb_fast_checksum, innodb_page_size, innodb_log_block_size; xtrabackup_binary containing the binary used for the backup; mysql-stderr containing the `STDERR` of mysqld during the process and mysql-stdout containing the `STDOUT` of the server. Finally, the binary log position will be printed to STDERR and innobackupex will exit returning 0 if all went OK. Note that the STDERR of innobackupex is not written in any file. You will have to redirect it to a file, e.g., innobackupex OPTIONS 2> backupout.log . Restoring a backup \u00b6 To restore a backup with innobackupex the innobackupex --copy-back option must be used. innobackupex will read from the my.cnf the variables datadir , innodb_data_home_dir , innodb_data_file_path , innodb_log_group_home_dir and check that the directories exist. It will copy the MyISAM tables, indexes, etc. ( .frm , .MRG , .MYD , .MYI , .TRG , .TRN , .ARM , .ARZ , .CSM , .CSV , par and .opt files) first, InnoDB tables and indexes next and the log files at last. It will preserve file\u2019s attributes when copying them, you may have to change the files\u2019 ownership to mysql before starting the database server, as they will be owned by the user who created the backup. Alternatively, the innobackupex --move-back option may be used to restore a backup. This option is similar to innobackupex --copy-back with the only difference that instead of copying files it moves them to their target locations. As this option removes backup files, it must be used with caution. It is useful in cases when there is not enough free disk space to hold both data files and their backup copies.","title":"How innobackupex Works"},{"location":"innobackupex/how_innobackupex_works.html#how-innobackupex-works","text":"From Percona XtraBackup version 2.3 innobackupex is has been rewritten in C and set up as a symlink to the xtrabackup . innobackupex supports all features and syntax as 2.2 version did, but it is now deprecated and will be removed in next major release. Syntax for new features will not be added to the innobackupex, only to the xtrabackup. The following describes the rationale behind innobackupex actions.","title":"How innobackupex Works"},{"location":"innobackupex/how_innobackupex_works.html#making-a-backup","text":"If no mode is specified, innobackupex will assume the backup mode. By default, it runs xtrabackup and lets it copy the InnoDB data files. When xtrabackup finishes that, innobackupex sees it create the xtrabackup_suspended_2 file and executes FLUSH TABLES WITH READ LOCK . xtrabackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Then it begins copying the rest of the files. innobackupex will then check MySQL variables to determine which features are supported by server. Special interest are backup locks, changed page bitmaps, GTID mode, etc. If everything goes well, the binary is started as a child process. innobackupex will wait for replicas in a replication setup if the option innobackupex --safe-slave-backup is set and will flush all tables with READ LOCK , preventing all MyISAM tables from writing (unless option innobackupex --no-lock is specified). Note Locking is done only for MyISAM and other non-InnoDB tables, and only after Percona XtraBackup is finished backing up all InnoDB/XtraDB data and logs. Percona XtraBackup will use backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Once this is done, the backup of the files will begin. It will backup .frm , .MRG , .MYD , .MYI , .TRG , .TRN , .ARM , .ARZ , .CSM , .CSV , .par , and .opt files. When all the files are backed up, it resumes ibbackup and wait until it finishes copying the transactions done while the backup was done. Then, the tables are unlocked, the replica is started (if the option innobackupex --safe-slave-backup was used) and the connection with the server is closed. Then, it removes the xtrabackup_suspended_2 file and permits xtrabackup to exit. It will also create the following files in the directory of the backup: xtrabackup_checkpoints containing the `LSN` and the type of backup; xtrabackup_binlog_info containing the position of the binary log at the moment of backing up; xtrabackup_binlog_pos_innodb containing the position of the binary log at the moment of backing up relative to *InnoDB* transactions; xtrabackup_slave_info containing the MySQL binlog position of the source server in a replication setup via `SHOW SLAVE STATUS` if the innobackupex \u2013slave-info option is passed; backup-my.cnf containing only the my.cnf options required for the backup. For example, innodb_data_file_path, innodb_log_files_in_group, innodb_log_file_size, innodb_fast_checksum, innodb_page_size, innodb_log_block_size; xtrabackup_binary containing the binary used for the backup; mysql-stderr containing the `STDERR` of mysqld during the process and mysql-stdout containing the `STDOUT` of the server. Finally, the binary log position will be printed to STDERR and innobackupex will exit returning 0 if all went OK. Note that the STDERR of innobackupex is not written in any file. You will have to redirect it to a file, e.g., innobackupex OPTIONS 2> backupout.log .","title":"Making a Backup"},{"location":"innobackupex/how_innobackupex_works.html#restoring-a-backup","text":"To restore a backup with innobackupex the innobackupex --copy-back option must be used. innobackupex will read from the my.cnf the variables datadir , innodb_data_home_dir , innodb_data_file_path , innodb_log_group_home_dir and check that the directories exist. It will copy the MyISAM tables, indexes, etc. ( .frm , .MRG , .MYD , .MYI , .TRG , .TRN , .ARM , .ARZ , .CSM , .CSV , par and .opt files) first, InnoDB tables and indexes next and the log files at last. It will preserve file\u2019s attributes when copying them, you may have to change the files\u2019 ownership to mysql before starting the database server, as they will be owned by the user who created the backup. Alternatively, the innobackupex --move-back option may be used to restore a backup. This option is similar to innobackupex --copy-back with the only difference that instead of copying files it moves them to their target locations. As this option removes backup files, it must be used with caution. It is useful in cases when there is not enough free disk space to hold both data files and their backup copies.","title":"Restoring a backup"},{"location":"innobackupex/improved_ftwrl.html","text":"Improved FLUSH TABLES WITH READ LOCK handling \u00b6 When taking backups, FLUSH TABLES WITH READ LOCK is being used before the non-InnoDB files are being backed up to ensure backup is being consistent. FLUSH TABLES WITH READ LOCK can be run even though there may be a running query that has been executing for hours. In this case everything will be locked up in Waiting for table flush or Waiting for master to send event states. Killing the FLUSH TABLES WITH READ LOCK does not correct this issue either. In this case the only way to get the server operating normally again is to kill off the long running queries that blocked it to begin with. This means that if there are long running queries FLUSH TABLES WITH READ LOCK can get stuck, leaving server in read-only mode until waiting for these queries to complete. Note All described in this section has no effect when backup locks are used. Percona XtraBackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. In order to prevent this from happening two things have been implemented: innobackupex can wait for a good moment to issue the global lock. innobackupex can kill all or only SELECT queries which are preventing the global lock from being acquired Waiting for queries to finish \u00b6 Good moment to issue a global lock is the moment when there are no long queries running. But waiting for a good moment to issue the global lock for extended period of time isn\u2019t always good approach, as it can extend the time needed for backup to take place. To prevent innobackupex from waiting to issue FLUSH TABLES WITH READ LOCK for too long, new option has been implemented: innobackupex \u2013ftwrl-wait-timeout option can be used to limit the waiting time. If the good moment to issue the lock did not happen during this time, innobackupex will give up and exit with an error message and backup will not be taken. Zero value for this option turns off the feature (which is default). Another possibility is to specify the type of query to wait on. In this case innobackupex --ftwrl-wait-query-type . Possible values are all and update . When all is used innobackupex will wait for all long running queries (execution time longer than allowed by innobackupex --ftwrl-wait-threshold ) to finish before running the FLUSH TABLES WITH READ LOCK . When update is used innobackupex will wait on UPDATE/ALTER/REPLACE/INSERT queries to finish. Although time needed for specific query to complete is hard to predict, we can assume that queries that are running for a long time already will likely not be completed soon, and queries which are running for a short time will likely be completed shortly. innobackupex can use the value of innobackupex --ftwrl-wait-threshold option to specify which query is long running and will likely block global lock for a while. In order to use this option xtrabackup user should have PROCESS and SUPER privileges. Killing the blocking queries \u00b6 Second option is to kill all the queries which prevent global lock from being acquired. In this case all the queries which run longer than FLUSH TABLES WITH READ LOCK are possible blockers. Although all queries can be killed, additional time can be specified for the short running queries to complete. This can be specified by innobackupex --kill-long-queries-timeout option. This option specifies the time for queries to complete, after the value is reached, all the running queries will be killed. Default value is zero, which turns this feature off. innobackupex --kill-long-query-type option can be used to specify all or only SELECT queries that are preventing global lock from being acquired. In order to use this option xtrabackup user should have PROCESS and SUPER privileges. Options summary \u00b6 innobackupex --ftwrl-wait-timeout (seconds) - how long to wait for a good moment. Default is 0, not to wait. innobackupex --ftwrl-wait-query-type - which long queries should be finished before FLUSH TABLES WITH READ LOCK is run. Default is all. innobackupex --ftwrl-wait-threshold (seconds) - how long query should be running before we consider it long running and potential blocker of global lock. innobackupex --kill-long-queries-timeout (seconds) - how many time we give for queries to complete after FLUSH TABLES WITH READ LOCK is issued before start to kill. Default if 0 , not to kill. innobackupex --kill-long-query-type - which queries should be killed once kill-long-queries-timeout has expired. Example \u00b6 Running the innobackupex with the following options will cause innobackupex to spend no longer than 3 minutes waiting for all queries older than 40 seconds to complete. $ innobackupex --ftwrl-wait-threshold = 40 --ftwrl-wait-query-type = all --ftwrl-wait-timeout = 180 --kill-long-queries-timeout = 20 --kill-long-query-type = all /data/backups/ After FLUSH TABLES WITH READ LOCK is issued, innobackupex will wait 20 seconds for lock to be acquired. If lock is still not acquired after 20 seconds, it will kill all queries which are running longer that the FLUSH TABLES WITH READ LOCK .","title":"Improved `FLUSH TABLES WITH READ LOCK` handling"},{"location":"innobackupex/improved_ftwrl.html#improved-flush-tables-with-read-lock-handling","text":"When taking backups, FLUSH TABLES WITH READ LOCK is being used before the non-InnoDB files are being backed up to ensure backup is being consistent. FLUSH TABLES WITH READ LOCK can be run even though there may be a running query that has been executing for hours. In this case everything will be locked up in Waiting for table flush or Waiting for master to send event states. Killing the FLUSH TABLES WITH READ LOCK does not correct this issue either. In this case the only way to get the server operating normally again is to kill off the long running queries that blocked it to begin with. This means that if there are long running queries FLUSH TABLES WITH READ LOCK can get stuck, leaving server in read-only mode until waiting for these queries to complete. Note All described in this section has no effect when backup locks are used. Percona XtraBackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. In order to prevent this from happening two things have been implemented: innobackupex can wait for a good moment to issue the global lock. innobackupex can kill all or only SELECT queries which are preventing the global lock from being acquired","title":"Improved FLUSH TABLES WITH READ LOCK handling"},{"location":"innobackupex/improved_ftwrl.html#waiting-for-queries-to-finish","text":"Good moment to issue a global lock is the moment when there are no long queries running. But waiting for a good moment to issue the global lock for extended period of time isn\u2019t always good approach, as it can extend the time needed for backup to take place. To prevent innobackupex from waiting to issue FLUSH TABLES WITH READ LOCK for too long, new option has been implemented: innobackupex \u2013ftwrl-wait-timeout option can be used to limit the waiting time. If the good moment to issue the lock did not happen during this time, innobackupex will give up and exit with an error message and backup will not be taken. Zero value for this option turns off the feature (which is default). Another possibility is to specify the type of query to wait on. In this case innobackupex --ftwrl-wait-query-type . Possible values are all and update . When all is used innobackupex will wait for all long running queries (execution time longer than allowed by innobackupex --ftwrl-wait-threshold ) to finish before running the FLUSH TABLES WITH READ LOCK . When update is used innobackupex will wait on UPDATE/ALTER/REPLACE/INSERT queries to finish. Although time needed for specific query to complete is hard to predict, we can assume that queries that are running for a long time already will likely not be completed soon, and queries which are running for a short time will likely be completed shortly. innobackupex can use the value of innobackupex --ftwrl-wait-threshold option to specify which query is long running and will likely block global lock for a while. In order to use this option xtrabackup user should have PROCESS and SUPER privileges.","title":"Waiting for queries to finish"},{"location":"innobackupex/improved_ftwrl.html#killing-the-blocking-queries","text":"Second option is to kill all the queries which prevent global lock from being acquired. In this case all the queries which run longer than FLUSH TABLES WITH READ LOCK are possible blockers. Although all queries can be killed, additional time can be specified for the short running queries to complete. This can be specified by innobackupex --kill-long-queries-timeout option. This option specifies the time for queries to complete, after the value is reached, all the running queries will be killed. Default value is zero, which turns this feature off. innobackupex --kill-long-query-type option can be used to specify all or only SELECT queries that are preventing global lock from being acquired. In order to use this option xtrabackup user should have PROCESS and SUPER privileges.","title":"Killing the blocking queries"},{"location":"innobackupex/improved_ftwrl.html#options-summary","text":"innobackupex --ftwrl-wait-timeout (seconds) - how long to wait for a good moment. Default is 0, not to wait. innobackupex --ftwrl-wait-query-type - which long queries should be finished before FLUSH TABLES WITH READ LOCK is run. Default is all. innobackupex --ftwrl-wait-threshold (seconds) - how long query should be running before we consider it long running and potential blocker of global lock. innobackupex --kill-long-queries-timeout (seconds) - how many time we give for queries to complete after FLUSH TABLES WITH READ LOCK is issued before start to kill. Default if 0 , not to kill. innobackupex --kill-long-query-type - which queries should be killed once kill-long-queries-timeout has expired.","title":"Options summary"},{"location":"innobackupex/improved_ftwrl.html#example","text":"Running the innobackupex with the following options will cause innobackupex to spend no longer than 3 minutes waiting for all queries older than 40 seconds to complete. $ innobackupex --ftwrl-wait-threshold = 40 --ftwrl-wait-query-type = all --ftwrl-wait-timeout = 180 --kill-long-queries-timeout = 20 --kill-long-query-type = all /data/backups/ After FLUSH TABLES WITH READ LOCK is issued, innobackupex will wait 20 seconds for lock to be acquired. If lock is still not acquired after 20 seconds, it will kill all queries which are running longer that the FLUSH TABLES WITH READ LOCK .","title":"Example"},{"location":"innobackupex/incremental_backups_innobackupex.html","text":"Incremental Backups with innobackupex \u00b6 As not all information changes between each backup, the incremental backup strategy uses this to reduce the storage needs and the duration of making a backup. This can be done because each InnoDB page has a log sequence number, LSN , which acts as a version number of the entire database. Every time the database is modified, this number gets incremented. An incremental backup copies all pages since a specific LSN . Once the pages have been put together in their respective order, applying the logs will recreate the process that affected the database, yielding the data at the moment of the most recently created backup. Creating an Incremental Backups with innobackupex \u00b6 First, you need to make a full backup as the BASE for subsequent incremental backups: $ innobackupex /data/backups This will create a timestamped directory in /data/backups . Assuming that the backup is done last day of the month, BASEDIR would be /data/backups/2013-03-31_23-01-18 , for example. Note You can use the innobackupex --no-timestamp option to override this behavior and the backup will be created in the given directory. If you check at the xtrabackup-checkpoints file in BASE-DIR , you should see something like: backup_type = full-backuped from_lsn = 0 to_lsn = 1626007 last_lsn = 1626007 compact = 0 recover_binlog_info = 1 To create an incremental backup the next day, use innobackupex --incremental and provide the BASEDIR: $ innobackupex --incremental /data/backups --incremental-basedir = BASEDIR Another timestamped directory will be created in /data/backups , in this example, /data/backups/2013-04-01_23-01-18 containing the incremental backup. We will call this INCREMENTAL-DIR-1 . If you check at the xtrabackup-checkpoints file in INCREMENTAL-DIR-1 , you should see something like: backup_type = incremental from_lsn = 1626007 to_lsn = 4124244 last_lsn = 4124244 compact = 0 recover_binlog_info = 1 Creating another incremental backup the next day will be analogous, but this time the previous incremental one will be base: $ innobackupex --incremental /data/backups --incremental-basedir = INCREMENTAL-DIR-1 Yielding (in this example) /data/backups/2013-04-02_23-01-18 . We will use INCREMENTAL-DIR-2 instead for simplicity. At this point, the xtrabackup-checkpoints file in INCREMENTAL-DIR-2 should contain something like: backup_type = incremental from_lsn = 4124244 to_lsn = 6938371 last_lsn = 7110572 compact = 0 recover_binlog_info = 1 As it was said before, an incremental backup only copy pages with a LSN greater than a specific value. Providing the LSN would have produced directories with the same data inside: innobackupex --incremental /data/backups --incremental-lsn=4124244 innobackupex --incremental /data/backups --incremental-lsn=6938371 This is a very useful way of doing an incremental backup, since not always the base or the last incremental will be available in the system. Warning This procedure only affects XtraDB or InnoDB -based tables. Other tables with a different storage engine, e.g. MyISAM , will be copied entirely each time an incremental backup is performed. Preparing an Incremental Backup with innobackupex \u00b6 Preparing incremental backups is a bit different than full backups. This is, perhaps, the stage where more attention is needed: First, only the committed transactions must be replayed on each backup . This will merge the base full backup with the incremental ones. Then, the uncommitted transaction must be rolled back in order to have ready-to-use backup. If you replay the committed transactions and rollback the uncommitted ones on the base backup, you will not be able to add the incremental ones. If you do this on an incremental one, you won\u2019t be able to add data from that moment and the remaining increments. Having this in mind, the procedure is very straight-forward using the innobackupex --redo-only option, starting with the base backup: $ innobackupex --apply-log --redo-only BASE-DIR You should see an output similar to: 160103 22:00:12 InnoDB: Shutdown completed; log sequence number 4124244 160103 22:00:12 innobackupex: completed OK! Then, the first incremental backup can be applied to the base backup, by issuing: $ innobackupex --apply-log --redo-only BASE-DIR --incremental-dir = INCREMENTAL-DIR-1 You should see an output similar to the previous one but with corresponding LSN : 160103 22:08:43 InnoDB: Shutdown completed; log sequence number 6938371 160103 22:08:43 innobackupex: completed OK! If no innobackupex --incremental-dir is set, innobackupex will use the most recent subdirectory created in the basedir. At this moment, BASE-DIR contains the data up to the moment of the first incremental backup. Note that the full data will always be in the directory of the base backup, as we are appending the increments to it. Repeat the procedure with the second one: $ innobackupex --apply-log BASE-DIR --incremental-dir = INCREMENTAL-DIR-2 If the completed OK! message was shown, the final data will be in the base backup directory, BASE-DIR . Note innobackupex --redo-only should be used when merging all incrementals except the last one. That\u2019s why the previous line doesn\u2019t contain the innobackupex --redo-only option. Even if the innobackupex --redo-only was used on the last step, backup would still be consistent but in that case server would perform the rollback phase. You can use this procedure to add more increments to the base, as long as you do it in the chronological order that the backups were done. If you merge the incremental backups in the wrong order, the backup will be useless. If you have doubts about the order that they must be applied, you can check the file xtrabackup_checkpoints at the directory of each one, as shown in the beginning of this section. Once you merge the base with all the increments, you can prepare it to roll back the uncommitted transactions: $ innobackupex --apply-log BASE-DIR Now your backup is ready to be used immediately after restoring it. This preparation step is optional. However, if you restore without doing the prepare, the database server will begin to rollback uncommitted transactions, the same work it would do if a crash had occurred. This results in delay as the database server starts, and you can avoid the delay if you do the prepare. Note that the iblog\\* files will not be created by innobackupex , if you want them to be created, use xtrabackup --prepare on the directory. Otherwise, the files will be created by the server once started. Restoring Incremental Backups with innobackupex \u00b6 After preparing the incremental backups, the base directory contains the same data as the full backup. For restoring it, you can use the xtrabackup --copy-back parameter: $ xtrabackup --copy-back --target-dir = BASE-DIR If the incremental backup was created using the xtrabackup --compress option, then you need to run xtrabackup --decompress followed by xtrabackup --copy-back . $ xtrabackup --decompress --target-dir = BASE-DIR $ xtrabackup --copy-back --target-dir = BASE-DIR You may have to change the ownership as detailed on Restoring a Full Backup with innobackupex . Incremental Streaming Backups using xbstream and tar \u00b6 Incremental streaming backups can be performed with the xbstream streaming option. Currently backups are packed in custom xbstream format. With this feature, you need to take a BASE backup as well. Taking a base backup \u00b6 $ innobackupex /data/backups Taking a local backup \u00b6 $ innobackupex --incremental --incremental-lsn = LSN-number --stream = xbstream ./ > incremental.xbstream Unpacking the backup \u00b6 $ xbstream -x < incremental.xbstream Taking a local backup and streaming it to the remote server and unpacking it \u00b6 $ innobackupex --incremental --incremental-lsn = LSN-number --stream = xbstream ./ | \\ ssh user@hostname \" cat - | xbstream -x -C > /backup-dir/\"","title":"Incremental Backups with *innobackupex*"},{"location":"innobackupex/incremental_backups_innobackupex.html#incremental-backups-with-innobackupex","text":"As not all information changes between each backup, the incremental backup strategy uses this to reduce the storage needs and the duration of making a backup. This can be done because each InnoDB page has a log sequence number, LSN , which acts as a version number of the entire database. Every time the database is modified, this number gets incremented. An incremental backup copies all pages since a specific LSN . Once the pages have been put together in their respective order, applying the logs will recreate the process that affected the database, yielding the data at the moment of the most recently created backup.","title":"Incremental Backups with innobackupex"},{"location":"innobackupex/incremental_backups_innobackupex.html#creating-an-incremental-backups-with-innobackupex","text":"First, you need to make a full backup as the BASE for subsequent incremental backups: $ innobackupex /data/backups This will create a timestamped directory in /data/backups . Assuming that the backup is done last day of the month, BASEDIR would be /data/backups/2013-03-31_23-01-18 , for example. Note You can use the innobackupex --no-timestamp option to override this behavior and the backup will be created in the given directory. If you check at the xtrabackup-checkpoints file in BASE-DIR , you should see something like: backup_type = full-backuped from_lsn = 0 to_lsn = 1626007 last_lsn = 1626007 compact = 0 recover_binlog_info = 1 To create an incremental backup the next day, use innobackupex --incremental and provide the BASEDIR: $ innobackupex --incremental /data/backups --incremental-basedir = BASEDIR Another timestamped directory will be created in /data/backups , in this example, /data/backups/2013-04-01_23-01-18 containing the incremental backup. We will call this INCREMENTAL-DIR-1 . If you check at the xtrabackup-checkpoints file in INCREMENTAL-DIR-1 , you should see something like: backup_type = incremental from_lsn = 1626007 to_lsn = 4124244 last_lsn = 4124244 compact = 0 recover_binlog_info = 1 Creating another incremental backup the next day will be analogous, but this time the previous incremental one will be base: $ innobackupex --incremental /data/backups --incremental-basedir = INCREMENTAL-DIR-1 Yielding (in this example) /data/backups/2013-04-02_23-01-18 . We will use INCREMENTAL-DIR-2 instead for simplicity. At this point, the xtrabackup-checkpoints file in INCREMENTAL-DIR-2 should contain something like: backup_type = incremental from_lsn = 4124244 to_lsn = 6938371 last_lsn = 7110572 compact = 0 recover_binlog_info = 1 As it was said before, an incremental backup only copy pages with a LSN greater than a specific value. Providing the LSN would have produced directories with the same data inside: innobackupex --incremental /data/backups --incremental-lsn=4124244 innobackupex --incremental /data/backups --incremental-lsn=6938371 This is a very useful way of doing an incremental backup, since not always the base or the last incremental will be available in the system. Warning This procedure only affects XtraDB or InnoDB -based tables. Other tables with a different storage engine, e.g. MyISAM , will be copied entirely each time an incremental backup is performed.","title":"Creating an Incremental Backups with innobackupex"},{"location":"innobackupex/incremental_backups_innobackupex.html#preparing-an-incremental-backup-with-innobackupex","text":"Preparing incremental backups is a bit different than full backups. This is, perhaps, the stage where more attention is needed: First, only the committed transactions must be replayed on each backup . This will merge the base full backup with the incremental ones. Then, the uncommitted transaction must be rolled back in order to have ready-to-use backup. If you replay the committed transactions and rollback the uncommitted ones on the base backup, you will not be able to add the incremental ones. If you do this on an incremental one, you won\u2019t be able to add data from that moment and the remaining increments. Having this in mind, the procedure is very straight-forward using the innobackupex --redo-only option, starting with the base backup: $ innobackupex --apply-log --redo-only BASE-DIR You should see an output similar to: 160103 22:00:12 InnoDB: Shutdown completed; log sequence number 4124244 160103 22:00:12 innobackupex: completed OK! Then, the first incremental backup can be applied to the base backup, by issuing: $ innobackupex --apply-log --redo-only BASE-DIR --incremental-dir = INCREMENTAL-DIR-1 You should see an output similar to the previous one but with corresponding LSN : 160103 22:08:43 InnoDB: Shutdown completed; log sequence number 6938371 160103 22:08:43 innobackupex: completed OK! If no innobackupex --incremental-dir is set, innobackupex will use the most recent subdirectory created in the basedir. At this moment, BASE-DIR contains the data up to the moment of the first incremental backup. Note that the full data will always be in the directory of the base backup, as we are appending the increments to it. Repeat the procedure with the second one: $ innobackupex --apply-log BASE-DIR --incremental-dir = INCREMENTAL-DIR-2 If the completed OK! message was shown, the final data will be in the base backup directory, BASE-DIR . Note innobackupex --redo-only should be used when merging all incrementals except the last one. That\u2019s why the previous line doesn\u2019t contain the innobackupex --redo-only option. Even if the innobackupex --redo-only was used on the last step, backup would still be consistent but in that case server would perform the rollback phase. You can use this procedure to add more increments to the base, as long as you do it in the chronological order that the backups were done. If you merge the incremental backups in the wrong order, the backup will be useless. If you have doubts about the order that they must be applied, you can check the file xtrabackup_checkpoints at the directory of each one, as shown in the beginning of this section. Once you merge the base with all the increments, you can prepare it to roll back the uncommitted transactions: $ innobackupex --apply-log BASE-DIR Now your backup is ready to be used immediately after restoring it. This preparation step is optional. However, if you restore without doing the prepare, the database server will begin to rollback uncommitted transactions, the same work it would do if a crash had occurred. This results in delay as the database server starts, and you can avoid the delay if you do the prepare. Note that the iblog\\* files will not be created by innobackupex , if you want them to be created, use xtrabackup --prepare on the directory. Otherwise, the files will be created by the server once started.","title":"Preparing an Incremental Backup with innobackupex"},{"location":"innobackupex/incremental_backups_innobackupex.html#restoring-incremental-backups-with-innobackupex","text":"After preparing the incremental backups, the base directory contains the same data as the full backup. For restoring it, you can use the xtrabackup --copy-back parameter: $ xtrabackup --copy-back --target-dir = BASE-DIR If the incremental backup was created using the xtrabackup --compress option, then you need to run xtrabackup --decompress followed by xtrabackup --copy-back . $ xtrabackup --decompress --target-dir = BASE-DIR $ xtrabackup --copy-back --target-dir = BASE-DIR You may have to change the ownership as detailed on Restoring a Full Backup with innobackupex .","title":"Restoring Incremental Backups with innobackupex"},{"location":"innobackupex/incremental_backups_innobackupex.html#incremental-streaming-backups-using-xbstream-and-tar","text":"Incremental streaming backups can be performed with the xbstream streaming option. Currently backups are packed in custom xbstream format. With this feature, you need to take a BASE backup as well.","title":"Incremental Streaming Backups using xbstream and tar"},{"location":"innobackupex/incremental_backups_innobackupex.html#taking-a-base-backup","text":"$ innobackupex /data/backups","title":"Taking a base backup"},{"location":"innobackupex/incremental_backups_innobackupex.html#taking-a-local-backup","text":"$ innobackupex --incremental --incremental-lsn = LSN-number --stream = xbstream ./ > incremental.xbstream","title":"Taking a local backup"},{"location":"innobackupex/incremental_backups_innobackupex.html#unpacking-the-backup","text":"$ xbstream -x < incremental.xbstream","title":"Unpacking the backup"},{"location":"innobackupex/incremental_backups_innobackupex.html#taking-a-local-backup-and-streaming-it-to-the-remote-server-and-unpacking-it","text":"$ innobackupex --incremental --incremental-lsn = LSN-number --stream = xbstream ./ | \\ ssh user@hostname \" cat - | xbstream -x -C > /backup-dir/\"","title":"Taking a local backup and streaming it to the remote server and unpacking it"},{"location":"innobackupex/innobackupex_option_reference.html","text":"The innobackupex Option Reference \u00b6 This page documents all the command-line options for the innobackupex . Options \u00b6 \u2013apply-log \u00b6 Prepare a backup in BACKUP-DIR by applying the transaction log file named xtrabackup_logfile located in the same directory. Also, create new transaction logs. The InnoDB configuration is read from the file backup-my.cnf created by innobackupex when the backup was made. innobackupex --apply-log uses InnoDB configuration from backup-my.cnf by default, or from --defaults-file , if specified. InnoDB configuration in this context means server variables that affect data format, i.e. innodb_page_size , innodb_log_block_size , etc. Location-related variables, like innodb_log_group_home_dir or innodb_data_file_path are always ignored by --apply-log , so preparing a backup always works with data files from the backup directory, rather than any external ones. \u2013backup-locks \u00b6 This option controls if backup locks should be used instead of FLUSH TABLES WITH READ LOCK on the backup stage. The option has no effect when backup locks are not supported by the server. This option is enabled by default, disable with --no-backup-locks . \u2013no-backup-locks \u00b6 Explicity disables the option \u2013-backup-locks which is enabled by default. \u2013close-files \u00b6 Do not keep files opened. This option is passed directly to xtrabackup. When xtrabackup opens tablespace it normally doesn\u2019t close its file handle in order to handle the DDL operations correctly. However, if the number of tablespaces is really huge and can not fit into any limit, there is an option to close file handles once they are no longer accessed. Percona XtraBackup can produce inconsistent backups with this option enabled. Use at your own risk. \u2013compress \u00b6 This option instructs xtrabackup to compress backup copies of InnoDB data files. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for details. \u2013compress-threads= \u00b6 This option specifies the number of worker threads that will be used for parallel compression. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for details. \u2013compress-chunk-size= \u00b6 This option specifies the size of the internal working buffer for each compression thread, measured in bytes. It is passed directly to the xtrabackup child process. The default value is 64K. See the xtrabackup documentation for details. \u2013copy-back \u00b6 Copy all the files in a previously made backup from the backup directory to their original locations. Percona XtraBackup innobackupex --copy-back option will not copy over existing files unless innobackupex --force-non-empty-directories option is specified. \u2013databases=LIST \u00b6 This option specifies the list of databases that innobackupex should back up. The option accepts a string argument or path to file that contains the list of databases to back up. The list is of the form \u201cdatabasename1[.table_name1] databasename2[.table_name2] \u2026\u201d. If this option is not specified, all databases containing MyISAM and InnoDB tables will be backed up. Please make sure that \u2013databases contains all of the InnoDB databases and tables, so that all of the innodb.frm files are also backed up. In case the list is very long, this can be specified in a file, and the full path of the file can be specified instead of the list. (See option \u2013tables-file.) \u2013decompress \u00b6 Decompresses all files with the .qp extension in a backup previously made with the innobackupex --compress option. The innobackupex --parallel option will allow multiple files to be decrypted and/or decompressed simultaneously. In order to decompress, the qpress utility MUST be installed and accessible within the path. Percona XtraBackup doesn\u2019t automatically remove the compressed files. In order to clean up the backup directory users should remove the \\*.qp files manually. \u2013decrypt=ENCRYPTION-ALGORITHM \u00b6 Decrypts all files with the .xbcrypt extension in a backup previously made with \u2013encrypt option. The innobackupex --parallel option will allow multiple files to be decrypted and/or decompressed simultaneously. \u2013defaults-file=[MY.CNF] \u00b6 This option accepts a string argument that specifies what file to read the default MySQL options from. Must be given as the first option on the command-line. \u2013defaults-extra-file=[MY.CNF] \u00b6 This option specifies what extra file to read the default MySQL options from before the standard defaults-file. Must be given as the first option on the command-line. \u2013defaults-group=GROUP-NAME \u00b6 This option accepts a string argument that specifies the group which should be read from the configuration file. This is needed if you use mysqld_multi. This can also be used to indicate groups other than mysqld and xtrabackup. \u2013encrypt=ENCRYPTION_ALGORITHM \u00b6 This option instructs xtrabackup to encrypt backup copies of InnoDB data files using the algorithm specified in the ENCRYPTION_ALGORITHM. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. Currently, the following algorithms are supported: AES128 , AES192 and AES256 . \u2013encrypt-key=ENCRYPTION_KEY \u00b6 This option instructs xtrabackup to use the given proper length encryption key as the ENCRYPTION_KEY when using the \u2013encrypt option. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. It is not recommended to use this option where there is uncontrolled access to the machine as the command line and thus the key can be viewed as part of the process info. \u2013encrypt-key-file=ENCRYPTION_KEY_FILE \u00b6 This option instructs xtrabackup to use the encryption key stored in the given ENCRYPTION_KEY_FILE when using the \u2013encrypt option. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. The file must be a simple binary (or text) file that contains exactly the key to be used. \u2013encrypt-threads= \u00b6 This option specifies the number of worker threads that will be used for parallel encryption. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. \u2013encrypt-chunk-size= \u00b6 This option specifies the size of the internal working buffer for each encryption thread, measured in bytes. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. \u2013export \u00b6 This option is passed directly to xtrabackup --export option. It enables exporting individual tables for import into another server. See the xtrabackup documentation for details. \u2013extra-lsndir=DIRECTORY \u00b6 This option accepts a string argument that specifies the directory in which to save an extra copy of the xtrabackup_checkpoints file. It is passed directly to xtrabackup \u2019s innobackupex --extra-lsndir option. See the xtrabackup documentation for details. \u2013force-non-empty-directories \u00b6 When specified, it makes innobackupex --copy-back option or innobackupex --move-back option transfer files to non-empty directories. No existing files will be overwritten. If \u2013copy-back or \u2013move-back has to copy a file from the backup directory which already exists in the destination directory, it will still fail with an error. \u2013galera-info \u00b6 This options creates the xtrabackup_galera_info file which contains the local node state at the time of the backup. Option should be used when performing the backup of Percona-XtraDB-Cluster. Has no effect when backup locks are used to create the backup. \u2013help \u00b6 This option displays a help screen and exits. \u2013history=NAME \u00b6 This option enables the tracking of backup history in the PERCONA_SCHEMA.xtrabackup_history table. An optional history series name may be specified that will be placed with the history record for the current backup being taken. \u2013host=HOST \u00b6 This option accepts a string argument that specifies the host to use when connecting to the database server with TCP/IP. It is passed to the mysql child process without alteration. See mysql --help for details. \u2013ibbackup=IBBACKUP-BINARY \u00b6 This option specifies which xtrabackup binary should be used. The option accepts a string argument. IBBACKUP-BINARY should be the command used to run Percona XtraBackup . The option can be useful if the xtrabackup binary is not in your search path or working directory. If this option is not specified, innobackupex attempts to determine the binary to use automatically. \u2013include=REGEXP \u00b6 This option is a regular expression to be matched against table names in databasename.tablename format. It is passed directly to xtrabackup\u2019s xtrabackup --tables option. See the xtrabackup documentation for details. \u2013incremental \u00b6 This option tells xtrabackup to create an incremental backup, rather than a full one. It is passed to the xtrabackup child process. When this option is specified, either innobackupex --incremental-lsn or innobackupex --incremental-basedir can also be given. If neither option is given, option innobackupex --incremental-basedir is passed to xtrabackup by default, set to the first timestamped backup directory in the backup base directory. \u2013incremental-basedir=DIRECTORY \u00b6 This option accepts a string argument that specifies the directory containing the full backup that is the base dataset for the incremental backup. It is used with the innobackupex --incremental option. \u2013incremental-dir=DIRECTORY \u00b6 This option accepts a string argument that specifies the directory where the incremental backup will be combined with the full backup to make a new full backup. It is used with the innobackupex --incremental option. \u2013incremental-history-name=NAME \u00b6 This option specifies the name of the backup series stored in the PERCONA_SCHEMA.xtrabackup_history history record to base an incremental backup on. Percona Xtrabackup will search the history table looking for the most recent (highest innodb_to_lsn), successful backup in the series and take the to_lsn value to use as the starting lsn for the incremental backup. This will be mutually exclusive with innobackupex --incremental-history-uuid , innobackupex --incremental-basedir and innobackupex --incremental-lsn . If no valid lsn can be found (no series by that name, no successful backups by that name) xtrabackup will return with an error. It is used with the innobackupex --incremental option. \u2013incremental-history-uuid=UUID \u00b6 This option specifies the UUID of the specific history record stored in the PERCONA_SCHEMA.xtrabackup_history to base an incremental backup on. innobackupex --incremental-history-name , innobackupex --incremental-basedir\\ and innobackupex --incremental-lsn . If no valid lsn can be found (no success record with that uuid) xtrabackup will return with an error. It is used with the innobackupex --incremental option. \u2013incremental-lsn=LSN \u00b6 This option accepts a string argument that specifies the log sequence number ( LSN ) to use for the incremental backup. It is used with the innobackupex --incremental option. It is used instead of specifying innobackupex --incremental-basedir . For databases created by MySQL and Percona Server 5.0-series versions, specify the as two 32-bit integers in high:low format. For databases created in 5.1 and later, specify the LSN as a single 64-bit integer. \u2013kill-long-queries-timeout=SECONDS \u00b6 This option specifies the number of seconds innobackupex waits between starting FLUSH TABLES WITH READ LOCK and killing those queries that block it. Default is 0 seconds, which means innobackupex will not attempt to kill any queries. In order to use this option xtrabackup user should have PROCESS and SUPER privileges. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. \u2013kill-long-query-type=all|select \u00b6 This option specifies which types of queries should be killed to unblock the global lock. Default is \u201call\u201d. \u2013ftwrl-wait-timeout=SECONDS \u00b6 This option specifies time in seconds that innobackupex should wait for queries that would block FLUSH TABLES WITH READ LOCK before running it. If there are still such queries when the timeout expires, innobackupex terminates with an error. Default is 0, in which case innobackupex does not wait for queries to complete and starts FLUSH TABLES WITH READ LOCK immediately. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. \u2013ftwrl-wait-threshold=SECONDS \u00b6 This option specifies the query run time threshold which is used by innobackupex to detect long-running queries with a non-zero value of innobackupex \u2013ftwrl-wait-timeout . FLUSH TABLES WITH READ LOCK is not started until such long-running queries exist. This option has no effect if \u2013ftwrl-wait-timeout is 0. Default value is 60 seconds. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. \u2013ftwrl-wait-query-type=all|update \u00b6 This option specifies which types of queries are allowed to complete before innobackupex will issue the global lock. Default is all. \u2013log-copy-interval= \u00b6 This option specifies time interval between checks done by log copying thread in milliseconds. \u2013move-back \u00b6 Move all the files in a previously made backup from the backup directory to their original locations. As this option removes backup files, it must be used with caution. \u2013no-lock \u00b6 Use this option to disable table lock with FLUSH TABLES WITH READ LOCK . Use it only if ALL your tables are InnoDB and you DO NOT CARE about the binary log position of the backup. This option shouldn\u2019t be used if there are any DDL statements being executed or if any updates are happening on non-InnoDB tables (this includes the system MyISAM tables in the mysql database), otherwise it could lead to an inconsistent backup. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. If you are considering to use innobackupex --no-lock because your backups are failing to acquire the lock, this could be because of incoming replication events preventing the lock from succeeding. Please try using innobackupex --safe-slave-backup to momentarily stop the replication replica thread, this may help the backup to succeed and you then don\u2019t need to resort to using this option. xtrabackup_binlog_info is not created when \u2013no-lock option is used (because SHOW MASTER STATUS may be inconsistent), but under certain conditions xtrabackup_binlog_pos_innodb can be used instead to get consistent binlog coordinates as described in Working with Binary Logs . \u2013no-timestamp \u00b6 This option prevents creation of a time-stamped subdirectory of the BACKUP-ROOT-DIR given on the command line. When it is specified, the backup is done in BACKUP-ROOT-DIR instead. \u2013no-version-check \u00b6 This option disables the version check. If you do not pass this option, the automatic version check is enabled implicitly when xtrabackup runs in the --backup mode. To disable the version check, explicitly pass the --no-version-check option when invoking xtrabackup . When the automatic version check is enabled,|program| performs a version check against the server on the backup stage after creating a server connection. xtrabackup sends the following information to the server: MySQL flavour and version Operating system name Percona Toolkit version Perl version Each piece of information has a unique identifier which is an MD5 hash value that Percona Toolkit uses to obtain statistics about how it is used. This value is a random UUID; no client information is either collected or stored. \u2013parallel=NUMBER-OF-THREADS \u00b6 This option accepts an integer argument that specifies the number of threads the xtrabackup child process should use to back up files concurrently. Note that this option works on file level, that is, if you have several .ibd files, they will be copied in parallel. If your tables are stored together in a single tablespace file, it will have no effect. This option will allow multiple files to be decrypted and/or decompressed simultaneously. In order to decompress, the qpress utility MUST be installed and accessable within the path. This process will remove the original compressed/encrypted files and leave the results in the same location. It is passed directly to xtrabackup\u2019s xtrabackup --parallel option. See the xtrabackup documentation for details \u2013password=PASSWORD \u00b6 This option accepts a string argument specifying the password to use when connecting to the database. It is passed to the mysql child process without alteration. See mysql --help for details. \u2013port=PORT \u00b6 This option accepts a string argument that specifies the port to use when connecting to the database server with TCP/IP. It is passed to the mysql child process. It is passed to the mysql child process without alteration. See mysql --help for details. \u2013rebuild-indexes \u00b6 This option only has effect when used together with the --apply-log <innobackupex --apply-log> option and is passed directly to xtrabackup. When used, makes xtrabackup rebuild all secondary indexes after applying the log. This option is normally used to prepare compact backups. See the xtrabackup documentation for more information. \u2013rebuild-threads=NUMBER-OF-THREADS \u00b6 This option only has effect when used together with the innobackupex --apply-log and innobackupex --rebuild-indexes option and is passed directly to xtrabackup. When used, xtrabackup processes tablespaces in parallel with the specified number of threads when rebuilding indexes. See the xtrabackup documentation for more information. \u2013redo-only \u00b6 This option should be used when preparing the base full backup and when merging all incrementals except the last one. It is passed directly to xtrabackup\u2019s xtrabackup --apply-log-only option. This forces xtrabackup to skip the \u201crollback\u201d phase and do a \u201credo\u201d only. This is necessary if the backup will have incremental changes applied to it later. See the xtrabackup documentation for details. \u2013rsync \u00b6 Uses the rsync utility to optimize local file transfers. When this option is specified, innobackupex uses rsync to copy all non-InnoDB files instead of spawning a separate cp for each file, which can be much faster for servers with a large number of databases or tables. This option cannot be used together with innobackupex --stream . \u2013safe-slave-backup \u00b6 When specified, innobackupex will stop the replica SQL thread just before running FLUSH TABLES WITH READ LOCK and wait to start backup until Slave_open_temp_tables in SHOW STATUS is zero. If there are no open temporary tables, the backup will take place, otherwise the SQL thread will be started and stopped until there are no open temporary tables. The backup will fail if Slave_open_temp_tables does not become zero after innobackupex --safe-slave-backup-timeout seconds. The replica SQL thread will be restarted when the backup finishes. \u2013safe-slave-backup-timeout=SECONDS \u00b6 How many seconds innobackupex --safe-slave-backup should wait for Slave_open_temp_tables to become zero. Defaults to 300 seconds. \u2013slave-info \u00b6 This option is useful when backing up a replication replica server. It prints the binary log position and name of the source server. It also writes this information to the xtrabackup_slave_info file as a CHANGE MASTER command. A new replica for this source can be set up by starting a replica server on this backup and issuing a CHANGE MASTER command with the binary log position saved in the xtrabackup_slave_info file. \u2013socket \u00b6 This option accepts a string argument that specifies the socket to use when connecting to the local database server with a UNIX domain socket. It is passed to the mysql child process without alteration. See mysql --help for details. \u2013stream=STREAMNAME \u00b6 This option accepts a string argument that specifies the format in which to do the streamed backup. The backup will be done to STDOUT in the specified format. Currently, supported formats are tar and xbstream . Uses xbstream , which is available in Percona XtraBackup distributions. If you specify a path after this option, it will be interpreted as the value of tmpdir \u2013tables-file=FILE \u00b6 This option accepts a string argument that specifies the file in which there are a list of names of the form database.table , one per line. The option is passed directly to xtrabackup \u2018s innobackupex --tables-file option. \u2013throttle= \u00b6 This option limits the number of chunks copied per second. The chunk size is 10 MB . To limit the bandwidth to 10 MB/s , set the option to 1 : --throttle=1 . See also More information about how to throttle a backup Throttling Backups . \u2013tmpdir=DIRECTORY \u00b6 This option accepts a string argument that specifies the location where a temporary file will be stored. It may be used when innobackupex --stream is specified. For these options, the transaction log will first be stored to a temporary file, before streaming or copying to a remote host. This option specifies the location where that temporary file will be stored. If the option is not specified, the default is to use the value of tmpdir read from the server configuration. innobackupex is passing the tmpdir value specified in my.cnf as the --target-dir option to the xtrabackup binary. Both [mysqld] and [xtrabackup] groups are read from my.cnf. If there is tmpdir in both, then the value being used depends on the order of those group in my.cnf. \u2013use-memory= \u00b6 This option accepts a string argument that specifies the amount of memory in bytes for xtrabackup to use for crash recovery while preparing a backup. Multiples are supported providing the unit (e.g. 1MB, 1M, 1GB, 1G). It is used only with the option innobackupex --apply-log . It is passed directly to xtrabackup\u2019s xtrabackup --use-memory option. See the xtrabackup documentation for details. \u2013user=USER \u00b6 This option accepts a string argument that specifies the user (i.e., the MySQL username used when connecting to the server) to login as, if that\u2019s not the current user. It is passed to the mysql child process without alteration. See mysql --help for details. \u2013version \u00b6 This option displays the innobackupex version and copyright notice and then exits.","title":"The innobackupex Option Reference"},{"location":"innobackupex/innobackupex_option_reference.html#the-innobackupex-option-reference","text":"This page documents all the command-line options for the innobackupex .","title":"The innobackupex Option Reference"},{"location":"innobackupex/innobackupex_option_reference.html#options","text":"","title":"Options"},{"location":"innobackupex/innobackupex_option_reference.html#-apply-log","text":"Prepare a backup in BACKUP-DIR by applying the transaction log file named xtrabackup_logfile located in the same directory. Also, create new transaction logs. The InnoDB configuration is read from the file backup-my.cnf created by innobackupex when the backup was made. innobackupex --apply-log uses InnoDB configuration from backup-my.cnf by default, or from --defaults-file , if specified. InnoDB configuration in this context means server variables that affect data format, i.e. innodb_page_size , innodb_log_block_size , etc. Location-related variables, like innodb_log_group_home_dir or innodb_data_file_path are always ignored by --apply-log , so preparing a backup always works with data files from the backup directory, rather than any external ones.","title":"--apply-log"},{"location":"innobackupex/innobackupex_option_reference.html#-backup-locks","text":"This option controls if backup locks should be used instead of FLUSH TABLES WITH READ LOCK on the backup stage. The option has no effect when backup locks are not supported by the server. This option is enabled by default, disable with --no-backup-locks .","title":"--backup-locks"},{"location":"innobackupex/innobackupex_option_reference.html#-no-backup-locks","text":"Explicity disables the option \u2013-backup-locks which is enabled by default.","title":"--no-backup-locks"},{"location":"innobackupex/innobackupex_option_reference.html#-close-files","text":"Do not keep files opened. This option is passed directly to xtrabackup. When xtrabackup opens tablespace it normally doesn\u2019t close its file handle in order to handle the DDL operations correctly. However, if the number of tablespaces is really huge and can not fit into any limit, there is an option to close file handles once they are no longer accessed. Percona XtraBackup can produce inconsistent backups with this option enabled. Use at your own risk.","title":"--close-files"},{"location":"innobackupex/innobackupex_option_reference.html#-compress","text":"This option instructs xtrabackup to compress backup copies of InnoDB data files. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for details.","title":"--compress"},{"location":"innobackupex/innobackupex_option_reference.html#-compress-threads","text":"This option specifies the number of worker threads that will be used for parallel compression. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for details.","title":"--compress-threads="},{"location":"innobackupex/innobackupex_option_reference.html#-compress-chunk-size","text":"This option specifies the size of the internal working buffer for each compression thread, measured in bytes. It is passed directly to the xtrabackup child process. The default value is 64K. See the xtrabackup documentation for details.","title":"--compress-chunk-size="},{"location":"innobackupex/innobackupex_option_reference.html#-copy-back","text":"Copy all the files in a previously made backup from the backup directory to their original locations. Percona XtraBackup innobackupex --copy-back option will not copy over existing files unless innobackupex --force-non-empty-directories option is specified.","title":"--copy-back"},{"location":"innobackupex/innobackupex_option_reference.html#-databaseslist","text":"This option specifies the list of databases that innobackupex should back up. The option accepts a string argument or path to file that contains the list of databases to back up. The list is of the form \u201cdatabasename1[.table_name1] databasename2[.table_name2] \u2026\u201d. If this option is not specified, all databases containing MyISAM and InnoDB tables will be backed up. Please make sure that \u2013databases contains all of the InnoDB databases and tables, so that all of the innodb.frm files are also backed up. In case the list is very long, this can be specified in a file, and the full path of the file can be specified instead of the list. (See option \u2013tables-file.)","title":"--databases=LIST"},{"location":"innobackupex/innobackupex_option_reference.html#-decompress","text":"Decompresses all files with the .qp extension in a backup previously made with the innobackupex --compress option. The innobackupex --parallel option will allow multiple files to be decrypted and/or decompressed simultaneously. In order to decompress, the qpress utility MUST be installed and accessible within the path. Percona XtraBackup doesn\u2019t automatically remove the compressed files. In order to clean up the backup directory users should remove the \\*.qp files manually.","title":"--decompress"},{"location":"innobackupex/innobackupex_option_reference.html#-decryptencryption-algorithm","text":"Decrypts all files with the .xbcrypt extension in a backup previously made with \u2013encrypt option. The innobackupex --parallel option will allow multiple files to be decrypted and/or decompressed simultaneously.","title":"--decrypt=ENCRYPTION-ALGORITHM"},{"location":"innobackupex/innobackupex_option_reference.html#-defaults-filemycnf","text":"This option accepts a string argument that specifies what file to read the default MySQL options from. Must be given as the first option on the command-line.","title":"--defaults-file=[MY.CNF]"},{"location":"innobackupex/innobackupex_option_reference.html#-defaults-extra-filemycnf","text":"This option specifies what extra file to read the default MySQL options from before the standard defaults-file. Must be given as the first option on the command-line.","title":"--defaults-extra-file=[MY.CNF]"},{"location":"innobackupex/innobackupex_option_reference.html#-defaults-groupgroup-name","text":"This option accepts a string argument that specifies the group which should be read from the configuration file. This is needed if you use mysqld_multi. This can also be used to indicate groups other than mysqld and xtrabackup.","title":"--defaults-group=GROUP-NAME"},{"location":"innobackupex/innobackupex_option_reference.html#-encryptencryption_algorithm","text":"This option instructs xtrabackup to encrypt backup copies of InnoDB data files using the algorithm specified in the ENCRYPTION_ALGORITHM. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. Currently, the following algorithms are supported: AES128 , AES192 and AES256 .","title":"--encrypt=ENCRYPTION_ALGORITHM"},{"location":"innobackupex/innobackupex_option_reference.html#-encrypt-keyencryption_key","text":"This option instructs xtrabackup to use the given proper length encryption key as the ENCRYPTION_KEY when using the \u2013encrypt option. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. It is not recommended to use this option where there is uncontrolled access to the machine as the command line and thus the key can be viewed as part of the process info.","title":"--encrypt-key=ENCRYPTION_KEY"},{"location":"innobackupex/innobackupex_option_reference.html#-encrypt-key-fileencryption_key_file","text":"This option instructs xtrabackup to use the encryption key stored in the given ENCRYPTION_KEY_FILE when using the \u2013encrypt option. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. The file must be a simple binary (or text) file that contains exactly the key to be used.","title":"--encrypt-key-file=ENCRYPTION_KEY_FILE"},{"location":"innobackupex/innobackupex_option_reference.html#-encrypt-threads","text":"This option specifies the number of worker threads that will be used for parallel encryption. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details.","title":"--encrypt-threads="},{"location":"innobackupex/innobackupex_option_reference.html#-encrypt-chunk-size","text":"This option specifies the size of the internal working buffer for each encryption thread, measured in bytes. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details.","title":"--encrypt-chunk-size="},{"location":"innobackupex/innobackupex_option_reference.html#-export","text":"This option is passed directly to xtrabackup --export option. It enables exporting individual tables for import into another server. See the xtrabackup documentation for details.","title":"--export"},{"location":"innobackupex/innobackupex_option_reference.html#-extra-lsndirdirectory","text":"This option accepts a string argument that specifies the directory in which to save an extra copy of the xtrabackup_checkpoints file. It is passed directly to xtrabackup \u2019s innobackupex --extra-lsndir option. See the xtrabackup documentation for details.","title":"--extra-lsndir=DIRECTORY"},{"location":"innobackupex/innobackupex_option_reference.html#-force-non-empty-directories","text":"When specified, it makes innobackupex --copy-back option or innobackupex --move-back option transfer files to non-empty directories. No existing files will be overwritten. If \u2013copy-back or \u2013move-back has to copy a file from the backup directory which already exists in the destination directory, it will still fail with an error.","title":"--force-non-empty-directories"},{"location":"innobackupex/innobackupex_option_reference.html#-galera-info","text":"This options creates the xtrabackup_galera_info file which contains the local node state at the time of the backup. Option should be used when performing the backup of Percona-XtraDB-Cluster. Has no effect when backup locks are used to create the backup.","title":"--galera-info"},{"location":"innobackupex/innobackupex_option_reference.html#-help","text":"This option displays a help screen and exits.","title":"--help"},{"location":"innobackupex/innobackupex_option_reference.html#-historyname","text":"This option enables the tracking of backup history in the PERCONA_SCHEMA.xtrabackup_history table. An optional history series name may be specified that will be placed with the history record for the current backup being taken.","title":"--history=NAME"},{"location":"innobackupex/innobackupex_option_reference.html#-hosthost","text":"This option accepts a string argument that specifies the host to use when connecting to the database server with TCP/IP. It is passed to the mysql child process without alteration. See mysql --help for details.","title":"--host=HOST"},{"location":"innobackupex/innobackupex_option_reference.html#-ibbackupibbackup-binary","text":"This option specifies which xtrabackup binary should be used. The option accepts a string argument. IBBACKUP-BINARY should be the command used to run Percona XtraBackup . The option can be useful if the xtrabackup binary is not in your search path or working directory. If this option is not specified, innobackupex attempts to determine the binary to use automatically.","title":"--ibbackup=IBBACKUP-BINARY"},{"location":"innobackupex/innobackupex_option_reference.html#-includeregexp","text":"This option is a regular expression to be matched against table names in databasename.tablename format. It is passed directly to xtrabackup\u2019s xtrabackup --tables option. See the xtrabackup documentation for details.","title":"--include=REGEXP"},{"location":"innobackupex/innobackupex_option_reference.html#-incremental","text":"This option tells xtrabackup to create an incremental backup, rather than a full one. It is passed to the xtrabackup child process. When this option is specified, either innobackupex --incremental-lsn or innobackupex --incremental-basedir can also be given. If neither option is given, option innobackupex --incremental-basedir is passed to xtrabackup by default, set to the first timestamped backup directory in the backup base directory.","title":"--incremental"},{"location":"innobackupex/innobackupex_option_reference.html#-incremental-basedirdirectory","text":"This option accepts a string argument that specifies the directory containing the full backup that is the base dataset for the incremental backup. It is used with the innobackupex --incremental option.","title":"--incremental-basedir=DIRECTORY"},{"location":"innobackupex/innobackupex_option_reference.html#-incremental-dirdirectory","text":"This option accepts a string argument that specifies the directory where the incremental backup will be combined with the full backup to make a new full backup. It is used with the innobackupex --incremental option.","title":"--incremental-dir=DIRECTORY"},{"location":"innobackupex/innobackupex_option_reference.html#-incremental-history-namename","text":"This option specifies the name of the backup series stored in the PERCONA_SCHEMA.xtrabackup_history history record to base an incremental backup on. Percona Xtrabackup will search the history table looking for the most recent (highest innodb_to_lsn), successful backup in the series and take the to_lsn value to use as the starting lsn for the incremental backup. This will be mutually exclusive with innobackupex --incremental-history-uuid , innobackupex --incremental-basedir and innobackupex --incremental-lsn . If no valid lsn can be found (no series by that name, no successful backups by that name) xtrabackup will return with an error. It is used with the innobackupex --incremental option.","title":"--incremental-history-name=NAME"},{"location":"innobackupex/innobackupex_option_reference.html#-incremental-history-uuiduuid","text":"This option specifies the UUID of the specific history record stored in the PERCONA_SCHEMA.xtrabackup_history to base an incremental backup on. innobackupex --incremental-history-name , innobackupex --incremental-basedir\\ and innobackupex --incremental-lsn . If no valid lsn can be found (no success record with that uuid) xtrabackup will return with an error. It is used with the innobackupex --incremental option.","title":"--incremental-history-uuid=UUID"},{"location":"innobackupex/innobackupex_option_reference.html#-incremental-lsnlsn","text":"This option accepts a string argument that specifies the log sequence number ( LSN ) to use for the incremental backup. It is used with the innobackupex --incremental option. It is used instead of specifying innobackupex --incremental-basedir . For databases created by MySQL and Percona Server 5.0-series versions, specify the as two 32-bit integers in high:low format. For databases created in 5.1 and later, specify the LSN as a single 64-bit integer.","title":"--incremental-lsn=LSN"},{"location":"innobackupex/innobackupex_option_reference.html#-kill-long-queries-timeoutseconds","text":"This option specifies the number of seconds innobackupex waits between starting FLUSH TABLES WITH READ LOCK and killing those queries that block it. Default is 0 seconds, which means innobackupex will not attempt to kill any queries. In order to use this option xtrabackup user should have PROCESS and SUPER privileges. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables.","title":"--kill-long-queries-timeout=SECONDS"},{"location":"innobackupex/innobackupex_option_reference.html#-kill-long-query-typeallselect","text":"This option specifies which types of queries should be killed to unblock the global lock. Default is \u201call\u201d.","title":"--kill-long-query-type=all|select"},{"location":"innobackupex/innobackupex_option_reference.html#-ftwrl-wait-timeoutseconds","text":"This option specifies time in seconds that innobackupex should wait for queries that would block FLUSH TABLES WITH READ LOCK before running it. If there are still such queries when the timeout expires, innobackupex terminates with an error. Default is 0, in which case innobackupex does not wait for queries to complete and starts FLUSH TABLES WITH READ LOCK immediately. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables.","title":"--ftwrl-wait-timeout=SECONDS"},{"location":"innobackupex/innobackupex_option_reference.html#-ftwrl-wait-thresholdseconds","text":"This option specifies the query run time threshold which is used by innobackupex to detect long-running queries with a non-zero value of innobackupex \u2013ftwrl-wait-timeout . FLUSH TABLES WITH READ LOCK is not started until such long-running queries exist. This option has no effect if \u2013ftwrl-wait-timeout is 0. Default value is 60 seconds. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables.","title":"--ftwrl-wait-threshold=SECONDS"},{"location":"innobackupex/innobackupex_option_reference.html#-ftwrl-wait-query-typeallupdate","text":"This option specifies which types of queries are allowed to complete before innobackupex will issue the global lock. Default is all.","title":"--ftwrl-wait-query-type=all|update"},{"location":"innobackupex/innobackupex_option_reference.html#-log-copy-interval","text":"This option specifies time interval between checks done by log copying thread in milliseconds.","title":"--log-copy-interval="},{"location":"innobackupex/innobackupex_option_reference.html#-move-back","text":"Move all the files in a previously made backup from the backup directory to their original locations. As this option removes backup files, it must be used with caution.","title":"--move-back"},{"location":"innobackupex/innobackupex_option_reference.html#-no-lock","text":"Use this option to disable table lock with FLUSH TABLES WITH READ LOCK . Use it only if ALL your tables are InnoDB and you DO NOT CARE about the binary log position of the backup. This option shouldn\u2019t be used if there are any DDL statements being executed or if any updates are happening on non-InnoDB tables (this includes the system MyISAM tables in the mysql database), otherwise it could lead to an inconsistent backup. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. If you are considering to use innobackupex --no-lock because your backups are failing to acquire the lock, this could be because of incoming replication events preventing the lock from succeeding. Please try using innobackupex --safe-slave-backup to momentarily stop the replication replica thread, this may help the backup to succeed and you then don\u2019t need to resort to using this option. xtrabackup_binlog_info is not created when \u2013no-lock option is used (because SHOW MASTER STATUS may be inconsistent), but under certain conditions xtrabackup_binlog_pos_innodb can be used instead to get consistent binlog coordinates as described in Working with Binary Logs .","title":"--no-lock"},{"location":"innobackupex/innobackupex_option_reference.html#-no-timestamp","text":"This option prevents creation of a time-stamped subdirectory of the BACKUP-ROOT-DIR given on the command line. When it is specified, the backup is done in BACKUP-ROOT-DIR instead.","title":"--no-timestamp"},{"location":"innobackupex/innobackupex_option_reference.html#-no-version-check","text":"This option disables the version check. If you do not pass this option, the automatic version check is enabled implicitly when xtrabackup runs in the --backup mode. To disable the version check, explicitly pass the --no-version-check option when invoking xtrabackup . When the automatic version check is enabled,|program| performs a version check against the server on the backup stage after creating a server connection. xtrabackup sends the following information to the server: MySQL flavour and version Operating system name Percona Toolkit version Perl version Each piece of information has a unique identifier which is an MD5 hash value that Percona Toolkit uses to obtain statistics about how it is used. This value is a random UUID; no client information is either collected or stored.","title":"--no-version-check"},{"location":"innobackupex/innobackupex_option_reference.html#-parallelnumber-of-threads","text":"This option accepts an integer argument that specifies the number of threads the xtrabackup child process should use to back up files concurrently. Note that this option works on file level, that is, if you have several .ibd files, they will be copied in parallel. If your tables are stored together in a single tablespace file, it will have no effect. This option will allow multiple files to be decrypted and/or decompressed simultaneously. In order to decompress, the qpress utility MUST be installed and accessable within the path. This process will remove the original compressed/encrypted files and leave the results in the same location. It is passed directly to xtrabackup\u2019s xtrabackup --parallel option. See the xtrabackup documentation for details","title":"--parallel=NUMBER-OF-THREADS"},{"location":"innobackupex/innobackupex_option_reference.html#-passwordpassword","text":"This option accepts a string argument specifying the password to use when connecting to the database. It is passed to the mysql child process without alteration. See mysql --help for details.","title":"--password=PASSWORD"},{"location":"innobackupex/innobackupex_option_reference.html#-portport","text":"This option accepts a string argument that specifies the port to use when connecting to the database server with TCP/IP. It is passed to the mysql child process. It is passed to the mysql child process without alteration. See mysql --help for details.","title":"--port=PORT"},{"location":"innobackupex/innobackupex_option_reference.html#-rebuild-indexes","text":"This option only has effect when used together with the --apply-log <innobackupex --apply-log> option and is passed directly to xtrabackup. When used, makes xtrabackup rebuild all secondary indexes after applying the log. This option is normally used to prepare compact backups. See the xtrabackup documentation for more information.","title":"--rebuild-indexes"},{"location":"innobackupex/innobackupex_option_reference.html#-rebuild-threadsnumber-of-threads","text":"This option only has effect when used together with the innobackupex --apply-log and innobackupex --rebuild-indexes option and is passed directly to xtrabackup. When used, xtrabackup processes tablespaces in parallel with the specified number of threads when rebuilding indexes. See the xtrabackup documentation for more information.","title":"--rebuild-threads=NUMBER-OF-THREADS"},{"location":"innobackupex/innobackupex_option_reference.html#-redo-only","text":"This option should be used when preparing the base full backup and when merging all incrementals except the last one. It is passed directly to xtrabackup\u2019s xtrabackup --apply-log-only option. This forces xtrabackup to skip the \u201crollback\u201d phase and do a \u201credo\u201d only. This is necessary if the backup will have incremental changes applied to it later. See the xtrabackup documentation for details.","title":"--redo-only"},{"location":"innobackupex/innobackupex_option_reference.html#-rsync","text":"Uses the rsync utility to optimize local file transfers. When this option is specified, innobackupex uses rsync to copy all non-InnoDB files instead of spawning a separate cp for each file, which can be much faster for servers with a large number of databases or tables. This option cannot be used together with innobackupex --stream .","title":"--rsync"},{"location":"innobackupex/innobackupex_option_reference.html#-safe-slave-backup","text":"When specified, innobackupex will stop the replica SQL thread just before running FLUSH TABLES WITH READ LOCK and wait to start backup until Slave_open_temp_tables in SHOW STATUS is zero. If there are no open temporary tables, the backup will take place, otherwise the SQL thread will be started and stopped until there are no open temporary tables. The backup will fail if Slave_open_temp_tables does not become zero after innobackupex --safe-slave-backup-timeout seconds. The replica SQL thread will be restarted when the backup finishes.","title":"--safe-slave-backup"},{"location":"innobackupex/innobackupex_option_reference.html#-safe-slave-backup-timeoutseconds","text":"How many seconds innobackupex --safe-slave-backup should wait for Slave_open_temp_tables to become zero. Defaults to 300 seconds.","title":"--safe-slave-backup-timeout=SECONDS"},{"location":"innobackupex/innobackupex_option_reference.html#-slave-info","text":"This option is useful when backing up a replication replica server. It prints the binary log position and name of the source server. It also writes this information to the xtrabackup_slave_info file as a CHANGE MASTER command. A new replica for this source can be set up by starting a replica server on this backup and issuing a CHANGE MASTER command with the binary log position saved in the xtrabackup_slave_info file.","title":"--slave-info"},{"location":"innobackupex/innobackupex_option_reference.html#-socket","text":"This option accepts a string argument that specifies the socket to use when connecting to the local database server with a UNIX domain socket. It is passed to the mysql child process without alteration. See mysql --help for details.","title":"--socket"},{"location":"innobackupex/innobackupex_option_reference.html#-streamstreamname","text":"This option accepts a string argument that specifies the format in which to do the streamed backup. The backup will be done to STDOUT in the specified format. Currently, supported formats are tar and xbstream . Uses xbstream , which is available in Percona XtraBackup distributions. If you specify a path after this option, it will be interpreted as the value of tmpdir","title":"--stream=STREAMNAME"},{"location":"innobackupex/innobackupex_option_reference.html#-tables-filefile","text":"This option accepts a string argument that specifies the file in which there are a list of names of the form database.table , one per line. The option is passed directly to xtrabackup \u2018s innobackupex --tables-file option.","title":"--tables-file=FILE"},{"location":"innobackupex/innobackupex_option_reference.html#-throttle","text":"This option limits the number of chunks copied per second. The chunk size is 10 MB . To limit the bandwidth to 10 MB/s , set the option to 1 : --throttle=1 . See also More information about how to throttle a backup Throttling Backups .","title":"--throttle="},{"location":"innobackupex/innobackupex_option_reference.html#-tmpdirdirectory","text":"This option accepts a string argument that specifies the location where a temporary file will be stored. It may be used when innobackupex --stream is specified. For these options, the transaction log will first be stored to a temporary file, before streaming or copying to a remote host. This option specifies the location where that temporary file will be stored. If the option is not specified, the default is to use the value of tmpdir read from the server configuration. innobackupex is passing the tmpdir value specified in my.cnf as the --target-dir option to the xtrabackup binary. Both [mysqld] and [xtrabackup] groups are read from my.cnf. If there is tmpdir in both, then the value being used depends on the order of those group in my.cnf.","title":"--tmpdir=DIRECTORY"},{"location":"innobackupex/innobackupex_option_reference.html#-use-memory","text":"This option accepts a string argument that specifies the amount of memory in bytes for xtrabackup to use for crash recovery while preparing a backup. Multiples are supported providing the unit (e.g. 1MB, 1M, 1GB, 1G). It is used only with the option innobackupex --apply-log . It is passed directly to xtrabackup\u2019s xtrabackup --use-memory option. See the xtrabackup documentation for details.","title":"--use-memory="},{"location":"innobackupex/innobackupex_option_reference.html#-useruser","text":"This option accepts a string argument that specifies the user (i.e., the MySQL username used when connecting to the server) to login as, if that\u2019s not the current user. It is passed to the mysql child process without alteration. See mysql --help for details.","title":"--user=USER"},{"location":"innobackupex/innobackupex_option_reference.html#-version","text":"This option displays the innobackupex version and copyright notice and then exits.","title":"--version"},{"location":"innobackupex/innobackupex_script.html","text":"The innobackupex Program \u00b6 The innobackupex program is a symlink to the xtrabackup C program. It lets you perform point-in-time backups of InnoDB / XtraDB tables together with the schema definitions, MyISAM tables, and other portions of the server. In previous versions innobackupex was implemented as a Perl script. This manual section explains how to use innobackupex in detail. Warning The innobackupex program is deprecated. Please switch to xtrabackup . The Backup Cycle - Full Backups \u00b6 Creating a Backup with innobackupex Preparing a Full Backup with innobackupex Restoring a Full Backup with innobackupex Other Types of Backup \u00b6 Incremental Backups with innobackupex Partial Backups Encrypted Backups Advanced Features \u00b6 Streaming and Compressing Backups Taking Backups in Replication Environments Accelerating the backup process Throttling backups with innobackupex Restoring Individual Tables Point-In-Time recovery Improved FLUSH TABLES WITH READ LOCK handling Store backup history on the server Implementation \u00b6 How innobackupex Works References \u00b6 The innobackupex Option Reference","title":"The innobackupex Program"},{"location":"innobackupex/innobackupex_script.html#the-innobackupex-program","text":"The innobackupex program is a symlink to the xtrabackup C program. It lets you perform point-in-time backups of InnoDB / XtraDB tables together with the schema definitions, MyISAM tables, and other portions of the server. In previous versions innobackupex was implemented as a Perl script. This manual section explains how to use innobackupex in detail. Warning The innobackupex program is deprecated. Please switch to xtrabackup .","title":"The innobackupex Program"},{"location":"innobackupex/innobackupex_script.html#the-backup-cycle-full-backups","text":"Creating a Backup with innobackupex Preparing a Full Backup with innobackupex Restoring a Full Backup with innobackupex","title":"The Backup Cycle - Full Backups"},{"location":"innobackupex/innobackupex_script.html#other-types-of-backup","text":"Incremental Backups with innobackupex Partial Backups Encrypted Backups","title":"Other Types of Backup"},{"location":"innobackupex/innobackupex_script.html#advanced-features","text":"Streaming and Compressing Backups Taking Backups in Replication Environments Accelerating the backup process Throttling backups with innobackupex Restoring Individual Tables Point-In-Time recovery Improved FLUSH TABLES WITH READ LOCK handling Store backup history on the server","title":"Advanced Features"},{"location":"innobackupex/innobackupex_script.html#implementation","text":"How innobackupex Works","title":"Implementation"},{"location":"innobackupex/innobackupex_script.html#references","text":"The innobackupex Option Reference","title":"References"},{"location":"innobackupex/parallel_copy_ibk.html","text":"Accelerating the backup process \u00b6 Accelerating with innobackupex --parallel copy and --compress-threads \u00b6 When performing a local backup or the streaming backup with xbstream option, multiple files can be copied concurrently by using the innobackupex --parallel option. This option specifies the number of threads created by xtrabackup to copy data files. To take advantage of this option either the multiple tablespaces option must be enabled ( innodb_file_per_table ) or the shared tablespace must be stored in multiple ibdata files with the innodb_data_file_path option. Having multiple files for the database (or splitting one into many) doesn\u2019t have a measurable impact on performance. As this feature is implemented at a file level , concurrent file transfer can sometimes increase I/O throughput when doing a backup on highly fragmented data files, due to the overlap of a greater number of random read requests. You should consider tuning the filesystem also to obtain the maximum performance (e.g. checking fragmentation). If the data is stored on a single file, this option will have no effect. To use this feature, simply add the option to a local backup, for example: $ innobackupex --parallel = 4 /path/to/backup By using the xbstream in streaming backups you can additionally speed up the compression process by using the innobackupex --compress-threads option. This option specifies the number of threads created by xtrabackup for for parallel data compression. The default value for this option is 1. To use this feature, simply add the option to a local backup, for example $ innobackupex --stream = xbstream --compress --compress-threads = 4 ./ > backup.xbstream Before applying logs, compressed files will need to be uncompressed. Accelerating with innobackupex --rsync \u00b6 In order to speed up the backup process and to minimize the time FLUSH TABLES WITH READ LOCK is blocking the writes, option innobackupex --rsync should be used. When this option is specified, innobackupex uses rsync to copy all non-InnoDB files instead of spawning a separate cp for each file, which can be much faster for servers with a large number of databases or tables. innobackupex will call the rsync twice, once before the FLUSH TABLES WITH READ LOCK and once during to minimize the time the read lock is being held. During the second rsync call, it will only synchronize the changes to non-transactional data (if any) since the first call performed before the FLUSH TABLES WITH READ LOCK . Note that Percona XtraBackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Note This option cannot be used together with the innobackupex --stream option.","title":"Accelerating the backup process"},{"location":"innobackupex/parallel_copy_ibk.html#accelerating-the-backup-process","text":"","title":"Accelerating the backup process"},{"location":"innobackupex/parallel_copy_ibk.html#accelerating-with-innobackupex-parallel-copy-and-compress-threads","text":"When performing a local backup or the streaming backup with xbstream option, multiple files can be copied concurrently by using the innobackupex --parallel option. This option specifies the number of threads created by xtrabackup to copy data files. To take advantage of this option either the multiple tablespaces option must be enabled ( innodb_file_per_table ) or the shared tablespace must be stored in multiple ibdata files with the innodb_data_file_path option. Having multiple files for the database (or splitting one into many) doesn\u2019t have a measurable impact on performance. As this feature is implemented at a file level , concurrent file transfer can sometimes increase I/O throughput when doing a backup on highly fragmented data files, due to the overlap of a greater number of random read requests. You should consider tuning the filesystem also to obtain the maximum performance (e.g. checking fragmentation). If the data is stored on a single file, this option will have no effect. To use this feature, simply add the option to a local backup, for example: $ innobackupex --parallel = 4 /path/to/backup By using the xbstream in streaming backups you can additionally speed up the compression process by using the innobackupex --compress-threads option. This option specifies the number of threads created by xtrabackup for for parallel data compression. The default value for this option is 1. To use this feature, simply add the option to a local backup, for example $ innobackupex --stream = xbstream --compress --compress-threads = 4 ./ > backup.xbstream Before applying logs, compressed files will need to be uncompressed.","title":"Accelerating with innobackupex --parallel copy and --compress-threads"},{"location":"innobackupex/parallel_copy_ibk.html#accelerating-with-innobackupex-rsync","text":"In order to speed up the backup process and to minimize the time FLUSH TABLES WITH READ LOCK is blocking the writes, option innobackupex --rsync should be used. When this option is specified, innobackupex uses rsync to copy all non-InnoDB files instead of spawning a separate cp for each file, which can be much faster for servers with a large number of databases or tables. innobackupex will call the rsync twice, once before the FLUSH TABLES WITH READ LOCK and once during to minimize the time the read lock is being held. During the second rsync call, it will only synchronize the changes to non-transactional data (if any) since the first call performed before the FLUSH TABLES WITH READ LOCK . Note that Percona XtraBackup will use Backup locks where available as a lightweight alternative to FLUSH TABLES WITH READ LOCK . This feature is available in Percona Server for MySQL 5.6+. Percona XtraBackup uses this automatically to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Note This option cannot be used together with the innobackupex --stream option.","title":"Accelerating with innobackupex --rsync"},{"location":"innobackupex/partial_backups_innobackupex.html","text":"Partial Backups \u00b6 Percona XtraBackup features partial backups, which means that you may backup only some specific tables or databases. The tables you back up must be in separate tablespaces, as a result of being created or altered after you enabled the innodb_file_per_table option on the server. There is only one caveat about partial backups: do not copy back the prepared backup. Restoring partial backups should be done by importing the tables, not by using the traditional innobackupex --copy-back option. Although there are some scenarios where restoring can be done by copying back the files, this may be lead to database inconsistencies in many cases and it is not the recommended way to do it. Creating Partial Backups \u00b6 There are three ways of specifying which part of the whole data will be backed up: regular expressions ( innobackupex --include ), enumerating the tables in a file ( innobackupex --tables-file ) or providing a list of databases ( innobackupex --databases ). Using innobackupex --include \u00b6 The regular expression provided to this will be matched against the fully qualified table name, including the database name, in the form databasename.tablename . For example, $ innobackupex --include = '^mydatabase[.]mytable' /path/to/backup The command above will create a timestamped directory with the usual files that innobackupex creates, but only the data files related to the tables matched. Note that this option is passed to xtrabackup --tables and is matched against each table of each database, the directories of each database will be created even if they are empty. Using innobackupex --tables-file \u00b6 The text file provided (the path) to this option can contain multiple table names, one per line, in the databasename.tablename format. For example, $ echo \"mydatabase.mytable\" > /tmp/tables.txt $ innobackupex --tables-file = /tmp/tables.txt /path/to/backup The command above will create a timestamped directory with the usual files that innobackupex creates, but only containing the data-files related to the tables specified in the file. This option is passed to xtrabackup --tables-file and, unlike the --tables <xtrabackup \u2013tables> option, only directories of databases of the selected tables will be created. Using innobackupex --databases \u00b6 This option accepts either a space-separated list of the databases and tables to backup - in the databasename[.tablename] form. For example, $ innobackupex --databases = \"mydatabase.mytable mysql\" /path/to/backup The command above will create a timestamped directory with the usual files that innobackupex creates, but only containing the datafiles related to mytable in the mydatabase directory and the mysql directory with the entire mysql database. The --databases-file option specifies the path to a text file which contains a case-sensitive list of databases and tables to be backed up. The file can contain the names of multiple databases and tables in a databasename[.tablename] format with one element for each line. Only the named databases and tables are backed up. The names must match exactly. There is no pattern matching or regular expression matching. Preparing Partial Backups \u00b6 For preparing partial backups, the procedure is analogous to restoring individual tables : apply the logs and use the innobackupex --export option: $ innobackupex --apply-log --export /path/to/partial/backup You may see warnings in the output about tables that don\u2019t exist. This is because InnoDB -based engines stores its data dictionary inside the tablespace files besides the .frm files. innobackupex will use xtrabackup to remove the missing tables (those who weren\u2019t selected in the partial backup) from the data dictionary in order to avoid future warnings or errors: 111225 0:54:06 InnoDB: Error: table 'mydatabase/mytablenotincludedinpartialb' InnoDB: in InnoDB data dictionary has tablespace id 6, InnoDB: but tablespace with that id or name does not exist. It will be removed from data dictionary. You should also see the notification of the creation of a file needed for importing (.exp file) for each table included in the partial backup: xtrabackup: export option is specified. xtrabackup: export metadata of table 'employees/departments' to file `.//departments.exp` (2 indexes) xtrabackup: name=PRIMARY, id.low=80, page=3 xtrabackup: name=dept_name, id.low=81, page=4 Note that you can use the innobackupex --export option with innobackupex --apply-log to an already-prepared backup in order to create the .exp files. Finally, check for the confirmation message in the output: 111225 00:54:18 innobackupex: completed OK! Restoring Partial Backups \u00b6 Restoring should be done by restoring individual tables in the partial backup to the server. It can also be done by copying back the prepared backup to a \u201cclean\u201d datadir (in that case, make sure to include the mysql database). System database can be created with: $ sudo mysql_install_db --user = mysql","title":"Partial Backups"},{"location":"innobackupex/partial_backups_innobackupex.html#partial-backups","text":"Percona XtraBackup features partial backups, which means that you may backup only some specific tables or databases. The tables you back up must be in separate tablespaces, as a result of being created or altered after you enabled the innodb_file_per_table option on the server. There is only one caveat about partial backups: do not copy back the prepared backup. Restoring partial backups should be done by importing the tables, not by using the traditional innobackupex --copy-back option. Although there are some scenarios where restoring can be done by copying back the files, this may be lead to database inconsistencies in many cases and it is not the recommended way to do it.","title":"Partial Backups"},{"location":"innobackupex/partial_backups_innobackupex.html#creating-partial-backups","text":"There are three ways of specifying which part of the whole data will be backed up: regular expressions ( innobackupex --include ), enumerating the tables in a file ( innobackupex --tables-file ) or providing a list of databases ( innobackupex --databases ).","title":"Creating Partial Backups"},{"location":"innobackupex/partial_backups_innobackupex.html#using-innobackupex-include","text":"The regular expression provided to this will be matched against the fully qualified table name, including the database name, in the form databasename.tablename . For example, $ innobackupex --include = '^mydatabase[.]mytable' /path/to/backup The command above will create a timestamped directory with the usual files that innobackupex creates, but only the data files related to the tables matched. Note that this option is passed to xtrabackup --tables and is matched against each table of each database, the directories of each database will be created even if they are empty.","title":"Using innobackupex --include"},{"location":"innobackupex/partial_backups_innobackupex.html#using-innobackupex-tables-file","text":"The text file provided (the path) to this option can contain multiple table names, one per line, in the databasename.tablename format. For example, $ echo \"mydatabase.mytable\" > /tmp/tables.txt $ innobackupex --tables-file = /tmp/tables.txt /path/to/backup The command above will create a timestamped directory with the usual files that innobackupex creates, but only containing the data-files related to the tables specified in the file. This option is passed to xtrabackup --tables-file and, unlike the --tables <xtrabackup \u2013tables> option, only directories of databases of the selected tables will be created.","title":"Using innobackupex --tables-file"},{"location":"innobackupex/partial_backups_innobackupex.html#using-innobackupex-databases","text":"This option accepts either a space-separated list of the databases and tables to backup - in the databasename[.tablename] form. For example, $ innobackupex --databases = \"mydatabase.mytable mysql\" /path/to/backup The command above will create a timestamped directory with the usual files that innobackupex creates, but only containing the datafiles related to mytable in the mydatabase directory and the mysql directory with the entire mysql database. The --databases-file option specifies the path to a text file which contains a case-sensitive list of databases and tables to be backed up. The file can contain the names of multiple databases and tables in a databasename[.tablename] format with one element for each line. Only the named databases and tables are backed up. The names must match exactly. There is no pattern matching or regular expression matching.","title":"Using innobackupex --databases"},{"location":"innobackupex/partial_backups_innobackupex.html#preparing-partial-backups","text":"For preparing partial backups, the procedure is analogous to restoring individual tables : apply the logs and use the innobackupex --export option: $ innobackupex --apply-log --export /path/to/partial/backup You may see warnings in the output about tables that don\u2019t exist. This is because InnoDB -based engines stores its data dictionary inside the tablespace files besides the .frm files. innobackupex will use xtrabackup to remove the missing tables (those who weren\u2019t selected in the partial backup) from the data dictionary in order to avoid future warnings or errors: 111225 0:54:06 InnoDB: Error: table 'mydatabase/mytablenotincludedinpartialb' InnoDB: in InnoDB data dictionary has tablespace id 6, InnoDB: but tablespace with that id or name does not exist. It will be removed from data dictionary. You should also see the notification of the creation of a file needed for importing (.exp file) for each table included in the partial backup: xtrabackup: export option is specified. xtrabackup: export metadata of table 'employees/departments' to file `.//departments.exp` (2 indexes) xtrabackup: name=PRIMARY, id.low=80, page=3 xtrabackup: name=dept_name, id.low=81, page=4 Note that you can use the innobackupex --export option with innobackupex --apply-log to an already-prepared backup in order to create the .exp files. Finally, check for the confirmation message in the output: 111225 00:54:18 innobackupex: completed OK!","title":"Preparing Partial Backups"},{"location":"innobackupex/partial_backups_innobackupex.html#restoring-partial-backups","text":"Restoring should be done by restoring individual tables in the partial backup to the server. It can also be done by copying back the prepared backup to a \u201cclean\u201d datadir (in that case, make sure to include the mysql database). System database can be created with: $ sudo mysql_install_db --user = mysql","title":"Restoring Partial Backups"},{"location":"innobackupex/pit_recovery_ibk.html","text":"Point-In-Time recovery \u00b6 Recovering up to particular moment in database\u2019s history can be done with innobackupex and the binary logs of the server. Note that the binary log contains the operations that modified the database from a point in the past. You need a full datadir as a base, and then you can apply a series of operations from the binary log to make the data match what it was at the point in time you want. For taking the snapshot, we will use innobackupex for a full backup: $ innobackupex /path/to/backup --no-timestamp (the innobackupex --no-timestamp option is for convenience in this example) and we will prepare it to be ready for restoration: $ innobackupex --apply-log /path/to/backup For more details on these procedures, see Creating a Backup with innobackupex and Preparing a Full Backup with innobackupex . Now, suppose that time has passed, and you want to restore the database to a certain point in the past, having in mind that there is the constraint of the point where the snapshot was taken. To find out what is the situation of binary logging in the server, execute the following queries: mysql > SHOW BINARY LOGS ; The result is similar to the following: +------------------+-----------+ | Log_name | File_size | +------------------+-----------+ | mysql-bin.000001 | 126 | | mysql-bin.000002 | 1306 | | mysql-bin.000003 | 126 | | mysql-bin.000004 | 497 | +------------------+-----------+ and mysql > SHOW MASTER STATUS ; The result is similar to the following: +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000004 | 497 | | | +------------------+----------+--------------+------------------+ The first query will tell you which files contain the binary log and the second one which file is currently being used to record changes, and the current position within it. Those files are stored usually in the datadir (unless other location is specified when the server is started with the --log-bin= option). To find out the position of the snapshot taken, see the xtrabackup_binlog_info at the backup\u2019s directory: $ cat /path/to/backup/xtrabackup_binlog_info The result is similar to the following: mysql-bin.000003 57 This will tell you which file was used at moment of the backup for the binary log and its position. That position will be the effective one when you restore the backup: $ innobackupex --copy-back /path/to/backup As the restoration will not affect the binary log files (you may need to adjust file permissions, see Restoring a Full Backup with innobackupex ), the next step is extracting the queries from the binary log with mysqlbinlog starting from the position of the snapshot and redirecting it to a file $ mysqlbinlog /path/to/datadir/mysql-bin.000003 /path/to/datadir/mysql-bin.000004 \\ --start-position = 57 > mybinlog.sql Note that if you have multiple files for the binary log, as in the example, you have to extract the queries with one process, as shown above. Inspect the file with the queries to determine which position or date corresponds to the point-in-time wanted. Once determined, pipe it to the server. Assuming the point is 11-12-25 01:00:00 : $ mysqlbinlog /path/to/datadir/mysql-bin.000003 /path/to/datadir/mysql-bin.000004 \\ --start-position = 57 --stop-datetime = \"11-12-25 01:00:00\" * mysql -u root -p and the database will be rolled forward up to that Point-In-Time.","title":"Point-In-Time recovery"},{"location":"innobackupex/pit_recovery_ibk.html#point-in-time-recovery","text":"Recovering up to particular moment in database\u2019s history can be done with innobackupex and the binary logs of the server. Note that the binary log contains the operations that modified the database from a point in the past. You need a full datadir as a base, and then you can apply a series of operations from the binary log to make the data match what it was at the point in time you want. For taking the snapshot, we will use innobackupex for a full backup: $ innobackupex /path/to/backup --no-timestamp (the innobackupex --no-timestamp option is for convenience in this example) and we will prepare it to be ready for restoration: $ innobackupex --apply-log /path/to/backup For more details on these procedures, see Creating a Backup with innobackupex and Preparing a Full Backup with innobackupex . Now, suppose that time has passed, and you want to restore the database to a certain point in the past, having in mind that there is the constraint of the point where the snapshot was taken. To find out what is the situation of binary logging in the server, execute the following queries: mysql > SHOW BINARY LOGS ; The result is similar to the following: +------------------+-----------+ | Log_name | File_size | +------------------+-----------+ | mysql-bin.000001 | 126 | | mysql-bin.000002 | 1306 | | mysql-bin.000003 | 126 | | mysql-bin.000004 | 497 | +------------------+-----------+ and mysql > SHOW MASTER STATUS ; The result is similar to the following: +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000004 | 497 | | | +------------------+----------+--------------+------------------+ The first query will tell you which files contain the binary log and the second one which file is currently being used to record changes, and the current position within it. Those files are stored usually in the datadir (unless other location is specified when the server is started with the --log-bin= option). To find out the position of the snapshot taken, see the xtrabackup_binlog_info at the backup\u2019s directory: $ cat /path/to/backup/xtrabackup_binlog_info The result is similar to the following: mysql-bin.000003 57 This will tell you which file was used at moment of the backup for the binary log and its position. That position will be the effective one when you restore the backup: $ innobackupex --copy-back /path/to/backup As the restoration will not affect the binary log files (you may need to adjust file permissions, see Restoring a Full Backup with innobackupex ), the next step is extracting the queries from the binary log with mysqlbinlog starting from the position of the snapshot and redirecting it to a file $ mysqlbinlog /path/to/datadir/mysql-bin.000003 /path/to/datadir/mysql-bin.000004 \\ --start-position = 57 > mybinlog.sql Note that if you have multiple files for the binary log, as in the example, you have to extract the queries with one process, as shown above. Inspect the file with the queries to determine which position or date corresponds to the point-in-time wanted. Once determined, pipe it to the server. Assuming the point is 11-12-25 01:00:00 : $ mysqlbinlog /path/to/datadir/mysql-bin.000003 /path/to/datadir/mysql-bin.000004 \\ --start-position = 57 --stop-datetime = \"11-12-25 01:00:00\" * mysql -u root -p and the database will be rolled forward up to that Point-In-Time.","title":"Point-In-Time recovery"},{"location":"innobackupex/preparing_a_backup_ibk.html","text":"Preparing a Full Backup with innobackupex \u00b6 The purpose of the prepare stage is to perform any pending operations and make the data consistent. After creating a backup, for example, uncommitted transactions must be undone or log transactions must be replayed. After this stage has finished, the data is ready. To prepare a backup with innobackupex you have to use the innobackupex --apply-log option and full path to the backup directory as an argument: $ innobackupex --apply-log /path/to/BACKUP-DIR and check the last line of the output for a confirmation on the process: 150806 01:01:57 InnoDB: Shutdown completed; log sequence number 1609228 150806 01:01:57 innobackupex: completed OK! If it succeeded, innobackupex performed all operations needed, leaving the data ready to use immediately. Under the hood \u00b6 innobackupex started the prepare process by reading the configuration from the backup-my.cnf file in the backup directory. After that, innobackupex replayed the committed transactions in the log files (some transactions could have been done while the backup was being done) and rolled back the uncommitted ones. Once this is done, all the information lay in the tablespace (the InnoDB files), and the log files are re-created. This implies calling innobackupex --apply-log twice. More details of this process are shown in the xtrabackup section . Note that this preparation is not suited for incremental backups. If you perform it on the base of an incremental backup, you will not be able to \u201cadd\u201d the increments. See Incremental Backups with innobackupex . Other options to consider \u00b6 innobackupex --use-memory \u00b6 The preparing process can be sped up by using more memory in it. It depends on the free or available RAM on your system, it defaults to 100MB . In general, the more memory available to the process, the better. The amount of memory used in the process can be specified by multiples of bytes: $ innobackupex --apply-log --use-memory = 4G /path/to/BACKUP-DIR","title":"Preparing a Full Backup with *innobackupex*"},{"location":"innobackupex/preparing_a_backup_ibk.html#preparing-a-full-backup-with-innobackupex","text":"The purpose of the prepare stage is to perform any pending operations and make the data consistent. After creating a backup, for example, uncommitted transactions must be undone or log transactions must be replayed. After this stage has finished, the data is ready. To prepare a backup with innobackupex you have to use the innobackupex --apply-log option and full path to the backup directory as an argument: $ innobackupex --apply-log /path/to/BACKUP-DIR and check the last line of the output for a confirmation on the process: 150806 01:01:57 InnoDB: Shutdown completed; log sequence number 1609228 150806 01:01:57 innobackupex: completed OK! If it succeeded, innobackupex performed all operations needed, leaving the data ready to use immediately.","title":"Preparing a Full Backup with innobackupex"},{"location":"innobackupex/preparing_a_backup_ibk.html#under-the-hood","text":"innobackupex started the prepare process by reading the configuration from the backup-my.cnf file in the backup directory. After that, innobackupex replayed the committed transactions in the log files (some transactions could have been done while the backup was being done) and rolled back the uncommitted ones. Once this is done, all the information lay in the tablespace (the InnoDB files), and the log files are re-created. This implies calling innobackupex --apply-log twice. More details of this process are shown in the xtrabackup section . Note that this preparation is not suited for incremental backups. If you perform it on the base of an incremental backup, you will not be able to \u201cadd\u201d the increments. See Incremental Backups with innobackupex .","title":"Under the hood"},{"location":"innobackupex/preparing_a_backup_ibk.html#other-options-to-consider","text":"","title":"Other options to consider"},{"location":"innobackupex/preparing_a_backup_ibk.html#innobackupex-use-memory","text":"The preparing process can be sped up by using more memory in it. It depends on the free or available RAM on your system, it defaults to 100MB . In general, the more memory available to the process, the better. The amount of memory used in the process can be specified by multiples of bytes: $ innobackupex --apply-log --use-memory = 4G /path/to/BACKUP-DIR","title":"innobackupex --use-memory"},{"location":"innobackupex/replication_ibk.html","text":"Taking Backups in Replication Environments \u00b6 There are options specific to back up from a replication replica. innobackupex --slave-info \u00b6 This option is useful when backing up a replication replica server. It prints the binary log position and name of the source server. It also writes this information to the xtrabackup_slave_info file as a CHANGE MASTER statement. This is useful for setting up a new replica for this source can be set up by starting a replica server on this backup and issuing the statement saved in the xtrabackup_slave_info file. More details of this procedure can be found in How to setup a replica for replication in 6 simple steps with Percona XtraBackup . innobackupex \u2013safe-slave-backup \u00b6 In order to assure a consistent replication state, this option stops the replica SQL thread and waits to start backing up until Slave_open_temp_tables in SHOW STATUS is zero. If there are no open temporary tables, the backup will take place, otherwise the SQL thread will be started and stopped until there are no open temporary tables. The backup will fail if Slave_open_temp_tables does not become zero after innobackupex \u2013safe-slave-backup-timeout seconds (defaults to 300 seconds). The replica SQL thread will be restarted when the backup finishes. Using this option is always recommended when taking backups from a replica server. Note Make sure your replica is a true replica of the source before using it as a source for backup. A good tool to validate a replica is pt-table-checksum .","title":"Taking Backups in Replication Environments"},{"location":"innobackupex/replication_ibk.html#taking-backups-in-replication-environments","text":"There are options specific to back up from a replication replica.","title":"Taking Backups in Replication Environments"},{"location":"innobackupex/replication_ibk.html#innobackupex-slave-info","text":"This option is useful when backing up a replication replica server. It prints the binary log position and name of the source server. It also writes this information to the xtrabackup_slave_info file as a CHANGE MASTER statement. This is useful for setting up a new replica for this source can be set up by starting a replica server on this backup and issuing the statement saved in the xtrabackup_slave_info file. More details of this procedure can be found in How to setup a replica for replication in 6 simple steps with Percona XtraBackup .","title":"innobackupex --slave-info"},{"location":"innobackupex/replication_ibk.html#innobackupex-safe-slave-backup","text":"In order to assure a consistent replication state, this option stops the replica SQL thread and waits to start backing up until Slave_open_temp_tables in SHOW STATUS is zero. If there are no open temporary tables, the backup will take place, otherwise the SQL thread will be started and stopped until there are no open temporary tables. The backup will fail if Slave_open_temp_tables does not become zero after innobackupex \u2013safe-slave-backup-timeout seconds (defaults to 300 seconds). The replica SQL thread will be restarted when the backup finishes. Using this option is always recommended when taking backups from a replica server. Note Make sure your replica is a true replica of the source before using it as a source for backup. A good tool to validate a replica is pt-table-checksum .","title":"innobackupex \u2013safe-slave-backup"},{"location":"innobackupex/restoring_a_backup_ibk.html","text":"Restoring a Full Backup with innobackupex \u00b6 For convenience, innobackupex has a innobackupex --copy-back option, which performs the restoration of a backup to the server\u2019s datadir : $ innobackupex --copy-back /path/to/BACKUP-DIR It will copy all the data-related files back to the server\u2019s datadir , determined by the server\u2019s my.cnf configuration file. You should check the last line of the output for a success message: innobackupex: Finished copying back files. 111225 01:08:13 innobackupex: completed OK! Note The datadir must be empty; Percona XtraBackup innobackupex --copy-back option will not copy over existing files unless innobackupex --force-non-empty-directories option is specified. Also it is important to note that MySQL server needs to be shut down before restore is performed. You can\u2019t restore to a datadir of a running mysqld instance (except when importing a partial backup). As files\u2019 attributes will be preserved, in most cases you will need to change the files\u2019 ownership to mysql before starting the database server, as they will be owned by the user who created the backup: $ chown -R mysql:mysql /var/lib/mysql Also note that all of these operations will be done as the user calling innobackupex , you will need write permissions on the server\u2019s datadir .","title":"Restoring a Full Backup with *innobackupex*"},{"location":"innobackupex/restoring_a_backup_ibk.html#restoring-a-full-backup-with-innobackupex","text":"For convenience, innobackupex has a innobackupex --copy-back option, which performs the restoration of a backup to the server\u2019s datadir : $ innobackupex --copy-back /path/to/BACKUP-DIR It will copy all the data-related files back to the server\u2019s datadir , determined by the server\u2019s my.cnf configuration file. You should check the last line of the output for a success message: innobackupex: Finished copying back files. 111225 01:08:13 innobackupex: completed OK! Note The datadir must be empty; Percona XtraBackup innobackupex --copy-back option will not copy over existing files unless innobackupex --force-non-empty-directories option is specified. Also it is important to note that MySQL server needs to be shut down before restore is performed. You can\u2019t restore to a datadir of a running mysqld instance (except when importing a partial backup). As files\u2019 attributes will be preserved, in most cases you will need to change the files\u2019 ownership to mysql before starting the database server, as they will be owned by the user who created the backup: $ chown -R mysql:mysql /var/lib/mysql Also note that all of these operations will be done as the user calling innobackupex , you will need write permissions on the server\u2019s datadir .","title":"Restoring a Full Backup with innobackupex"},{"location":"innobackupex/restoring_individual_tables_ibk.html","text":"Restoring Individual Tables \u00b6 In server versions prior to 5.6, it is not possible to copy tables between servers by copying the files, even with innodb_file_per_table . However, with the Percona XtraBackup , you can export individual tables from any InnoDB database, and import them into Percona Server with XtraDB or MySQL 5.6 (The source doesn\u2019t have to be XtraDB or or MySQL 5.6, but the destination does). This only works on individual .ibd files, and cannot export a table that is not contained in its own .ibd file. Note If you\u2019re running Percona Server version older than 5.5.10-20.1, variable innodb_expand_import should be used instead of innodb_import_table_from_xtrabackup . Exporting tables \u00b6 Exporting is done in the preparation stage, not at the moment of creating the backup. Once a full backup is created, prepare it with the innobackupex --export option: $ innobackupex --apply-log --export /path/to/backup This will create for each InnoDB with its own tablespace a file with .exp extension. An output of this procedure would contain: .. xtrabackup: export option is specified. xtrabackup: export metadata of table 'mydatabase/mytable' to file `./mydatabase/mytable.exp` (1 indexes) .. Now you should see a .exp file in the target directory: $ find /data/backups/mysql/ -name export_test.* The result should be similar to the following: /data/backups/mysql/test/export_test.exp /data/backups/mysql/test/export_test.ibd /data/backups/mysql/test/export_test.cfg These three files are all you need to import the table into a server running Percona Server for MySQL with XtraDB or MySQL 5.6. Note MySQL uses .cfg file which contains InnoDB dictionary dump in special format. This format is different from the .exp one which is used in XtraDB for the same purpose. Strictly speaking, a .cfg file is not required to import a tablespace to MySQL 5.6 or Percona Server for MySQL 5.6. A tablespace will be imported successfully even if it is from another server, but InnoDB will do schema validation if the corresponding .cfg file is present in the same directory. Each .exp (or .cfg ) file will be used for importing that table. Note InnoDB does a slow shutdown (i.e. full purge + change buffer merge) on \u2013export, otherwise the tablespaces wouldn\u2019t be consistent and thus couldn\u2019t be imported. All the usual performance considerations apply: sufficient buffer pool (i.e. --use-memory , 100MB by default) and fast enough storage, otherwise it can take a prohibitive amount of time for export to complete. Importing tables \u00b6 To import a table to other server, first create a new table with the same structure as the one that will be imported at that server: OTHERSERVER | mysql > CREATE TABLE mytable (...) ENGINE = InnoDB ; then discard its tablespace: OTHERSERVER | mysql > ALTER TABLE mydatabase . mytable DISCARD TABLESPACE ; Next, copy mytable.ibd and mytable.exp ( or mytable.cfg if importing to MySQL 5.6) files to database\u2019s home, and import its tablespace: OTHERSERVER | mysql > ALTER TABLE mydatabase . mytable IMPORT TABLESPACE ; Set the owner and group of the files: $ chown -R mysql:mysql /datadir/db_name/table_name.* After running this command, data in the imported table will be available.","title":"Restoring Individual Tables"},{"location":"innobackupex/restoring_individual_tables_ibk.html#restoring-individual-tables","text":"In server versions prior to 5.6, it is not possible to copy tables between servers by copying the files, even with innodb_file_per_table . However, with the Percona XtraBackup , you can export individual tables from any InnoDB database, and import them into Percona Server with XtraDB or MySQL 5.6 (The source doesn\u2019t have to be XtraDB or or MySQL 5.6, but the destination does). This only works on individual .ibd files, and cannot export a table that is not contained in its own .ibd file. Note If you\u2019re running Percona Server version older than 5.5.10-20.1, variable innodb_expand_import should be used instead of innodb_import_table_from_xtrabackup .","title":"Restoring Individual Tables"},{"location":"innobackupex/restoring_individual_tables_ibk.html#exporting-tables","text":"Exporting is done in the preparation stage, not at the moment of creating the backup. Once a full backup is created, prepare it with the innobackupex --export option: $ innobackupex --apply-log --export /path/to/backup This will create for each InnoDB with its own tablespace a file with .exp extension. An output of this procedure would contain: .. xtrabackup: export option is specified. xtrabackup: export metadata of table 'mydatabase/mytable' to file `./mydatabase/mytable.exp` (1 indexes) .. Now you should see a .exp file in the target directory: $ find /data/backups/mysql/ -name export_test.* The result should be similar to the following: /data/backups/mysql/test/export_test.exp /data/backups/mysql/test/export_test.ibd /data/backups/mysql/test/export_test.cfg These three files are all you need to import the table into a server running Percona Server for MySQL with XtraDB or MySQL 5.6. Note MySQL uses .cfg file which contains InnoDB dictionary dump in special format. This format is different from the .exp one which is used in XtraDB for the same purpose. Strictly speaking, a .cfg file is not required to import a tablespace to MySQL 5.6 or Percona Server for MySQL 5.6. A tablespace will be imported successfully even if it is from another server, but InnoDB will do schema validation if the corresponding .cfg file is present in the same directory. Each .exp (or .cfg ) file will be used for importing that table. Note InnoDB does a slow shutdown (i.e. full purge + change buffer merge) on \u2013export, otherwise the tablespaces wouldn\u2019t be consistent and thus couldn\u2019t be imported. All the usual performance considerations apply: sufficient buffer pool (i.e. --use-memory , 100MB by default) and fast enough storage, otherwise it can take a prohibitive amount of time for export to complete.","title":"Exporting tables"},{"location":"innobackupex/restoring_individual_tables_ibk.html#importing-tables","text":"To import a table to other server, first create a new table with the same structure as the one that will be imported at that server: OTHERSERVER | mysql > CREATE TABLE mytable (...) ENGINE = InnoDB ; then discard its tablespace: OTHERSERVER | mysql > ALTER TABLE mydatabase . mytable DISCARD TABLESPACE ; Next, copy mytable.ibd and mytable.exp ( or mytable.cfg if importing to MySQL 5.6) files to database\u2019s home, and import its tablespace: OTHERSERVER | mysql > ALTER TABLE mydatabase . mytable IMPORT TABLESPACE ; Set the owner and group of the files: $ chown -R mysql:mysql /datadir/db_name/table_name.* After running this command, data in the imported table will be available.","title":"Importing tables"},{"location":"innobackupex/storing_history.html","text":"Store backup history on the server \u00b6 Percona XtraBackup supports storing the backups history on the server. This feature was implemented in Percona XtraBackup 2.2. Storing backup history on the server was implemented to provide users with additional information about backups that are being taken. Backup history information will be stored in the PERCONA_SCHEMA.XTRABACKUP_HISTORY table. To use this feature three new innobackupex options have been implemented: innobackupex --history = : This option enables the history feature and allows the user to specify a backup series name that will be placed within the history record. innobackupex --incremental-history-name = : This option allows an incremental backup to be made based on a specific history series by name. innobackupex will search the history table looking for the most recent (highest to_lsn ) backup in the series and take the to_lsn value to use as it\u2019s starting lsn. This is mutually exclusive with innobackupex --incremental-history-uuid , innobackupex --incremental-basedir and innobackupex --incremental-lsn options. If no valid LSN can be found (no series by that name) innobackupex will return with an error. innobackupex --incremental-history-uuid = : Allows an incremental backup to be made based on a specific history record identified by UUID. innobackupex will search the history table looking for the record matching UUID and take the to_lsn value to use as it\u2019s starting LSN. This options is mutually exclusive with innobackupex --incremental-basedir , innobackupex --incremental-lsn and innobackupex --incremental-history-name options. If no valid LSN can be found (no record by that UUID or missing to_lsn ), innobackupex will return with an error. Note Backup that\u2019s currently being performed will NOT exist in the xtrabackup_history table within the resulting backup set as the record will not be added to that table until after the backup has been taken. If you want access to backup history outside of your backup set in the case of some catastrophic event, you will need to either perform a mysqldump , partial backup or SELECT * on the history table after innobackupex completes and store the results with you backup set. Privileges \u00b6 User performing the backup will need following privileges: CREATE privilege in order to create the PERCONA_SCHEMA.xtrabackup_history database and table. INSERT privilege in order to add history records to the PERCONA_SCHEMA.xtrabackup_history table. SELECT privilege in order to use innobackupex --incremental-history-name or innobackupex --incremental-history-uuid in order for the feature to look up the innodb_to_lsn values in the PERCONA_SCHEMA.xtrabackup_history table. PERCONA_SCHEMA.XTRABACKUP_HISTORY table \u00b6 This table contains the information about the previous server backups. Information about the backups will only be written if the backup was taken with innobackupex \u2013history option. Column Name Description uuid Unique backup id name User provided name of backup series. There may be multiple entries with the same name used to identify related backups in a series. tool_name Name of tool used to take backup tool_command Exact command line given to the tool with \u2013password and \u2013encryption_key obfuscated tool_version Version of tool used to take backup ibbackup_version Version of the xtrabackup binary used to take backup server_version Server version on which backup was taken start_time Time at the start of the backup end_time Time at the end of the backup lock_time Amount of time, in seconds, spent calling and holding locks for FLUSH TABLES WITH READ LOCK binlog_pos Binlog file and position at end of FLUSH TABLES WITH READ LOCK innodb_from_lsn LSN at beginning of backup which can be used to determine prior backups innodb_to_lsn LSN at end of backup which can be used as the starting lsn for the next incremental partial Is this a partial backup, if N that means that it\u2019s the full backup incremental Is this an incremental backup format Description of result format ( file , tar , xbstream ) compressed Is this a compressed backup encrypted Is this an encrypted backup Limitations \u00b6 innobackupex --history option must be specified only on the innobackupex command line and not within a configuration file in order to be effective. innobackupex --incremental-history-name and innobackupex --incremental-history-uuid options must be specified only on the innobackupex command line and not within a configuration file in order to be effective.","title":"Store backup history on the server"},{"location":"innobackupex/storing_history.html#store-backup-history-on-the-server","text":"Percona XtraBackup supports storing the backups history on the server. This feature was implemented in Percona XtraBackup 2.2. Storing backup history on the server was implemented to provide users with additional information about backups that are being taken. Backup history information will be stored in the PERCONA_SCHEMA.XTRABACKUP_HISTORY table. To use this feature three new innobackupex options have been implemented: innobackupex --history = : This option enables the history feature and allows the user to specify a backup series name that will be placed within the history record. innobackupex --incremental-history-name = : This option allows an incremental backup to be made based on a specific history series by name. innobackupex will search the history table looking for the most recent (highest to_lsn ) backup in the series and take the to_lsn value to use as it\u2019s starting lsn. This is mutually exclusive with innobackupex --incremental-history-uuid , innobackupex --incremental-basedir and innobackupex --incremental-lsn options. If no valid LSN can be found (no series by that name) innobackupex will return with an error. innobackupex --incremental-history-uuid = : Allows an incremental backup to be made based on a specific history record identified by UUID. innobackupex will search the history table looking for the record matching UUID and take the to_lsn value to use as it\u2019s starting LSN. This options is mutually exclusive with innobackupex --incremental-basedir , innobackupex --incremental-lsn and innobackupex --incremental-history-name options. If no valid LSN can be found (no record by that UUID or missing to_lsn ), innobackupex will return with an error. Note Backup that\u2019s currently being performed will NOT exist in the xtrabackup_history table within the resulting backup set as the record will not be added to that table until after the backup has been taken. If you want access to backup history outside of your backup set in the case of some catastrophic event, you will need to either perform a mysqldump , partial backup or SELECT * on the history table after innobackupex completes and store the results with you backup set.","title":"Store backup history on the server"},{"location":"innobackupex/storing_history.html#privileges","text":"User performing the backup will need following privileges: CREATE privilege in order to create the PERCONA_SCHEMA.xtrabackup_history database and table. INSERT privilege in order to add history records to the PERCONA_SCHEMA.xtrabackup_history table. SELECT privilege in order to use innobackupex --incremental-history-name or innobackupex --incremental-history-uuid in order for the feature to look up the innodb_to_lsn values in the PERCONA_SCHEMA.xtrabackup_history table.","title":"Privileges"},{"location":"innobackupex/storing_history.html#percona_schemaxtrabackup_history-table","text":"This table contains the information about the previous server backups. Information about the backups will only be written if the backup was taken with innobackupex \u2013history option. Column Name Description uuid Unique backup id name User provided name of backup series. There may be multiple entries with the same name used to identify related backups in a series. tool_name Name of tool used to take backup tool_command Exact command line given to the tool with \u2013password and \u2013encryption_key obfuscated tool_version Version of tool used to take backup ibbackup_version Version of the xtrabackup binary used to take backup server_version Server version on which backup was taken start_time Time at the start of the backup end_time Time at the end of the backup lock_time Amount of time, in seconds, spent calling and holding locks for FLUSH TABLES WITH READ LOCK binlog_pos Binlog file and position at end of FLUSH TABLES WITH READ LOCK innodb_from_lsn LSN at beginning of backup which can be used to determine prior backups innodb_to_lsn LSN at end of backup which can be used as the starting lsn for the next incremental partial Is this a partial backup, if N that means that it\u2019s the full backup incremental Is this an incremental backup format Description of result format ( file , tar , xbstream ) compressed Is this a compressed backup encrypted Is this an encrypted backup","title":"PERCONA_SCHEMA.XTRABACKUP_HISTORY table"},{"location":"innobackupex/storing_history.html#limitations","text":"innobackupex --history option must be specified only on the innobackupex command line and not within a configuration file in order to be effective. innobackupex --incremental-history-name and innobackupex --incremental-history-uuid options must be specified only on the innobackupex command line and not within a configuration file in order to be effective.","title":"Limitations"},{"location":"innobackupex/streaming_backups_innobackupex.html","text":"Streaming and Compressing Backups \u00b6 Streaming mode, supported by Percona XtraBackup , sends backup to STDOUT in special tar or xbstream format instead of copying files to the backup directory. This allows you to use other programs to filter the output of the backup, providing greater flexibility for storage of the backup. For example, compression is achieved by piping the output to a compression utility. One of the benefits of streaming backups and using Unix pipes is that the backups can be automatically encrypted. To use the streaming feature, you must use the innobackupex --stream , providing the format of the stream ( tar or xbstream ) and where to store the temporary files: $ innobackupex --stream = tar /tmp innobackupex uses xbstream to stream all of the data files to STDOUT , in a special xbstream format. See The xbstream binary for details. After it finishes streaming all of the data files to STDOUT , it stops xtrabackup and streams the saved log file too. When compression is enabled, xtrabackup compresses all output data, except the meta and non-InnoDB files which are not compressed, using the specified compression algorithm. The only currently supported algorithm is quicklz . The resulting files have the qpress archive format, i.e. every *.qp file produced by xtrabackup is essentially a one-file qpress archive and can be extracted and uncompressed by the qpress file archiver which is available from Percona Software repositories. Using xbstream as a stream option, backups can be copied and compressed in parallel which can significantly speed up the backup process. In case backups were both compressed and encrypted, they\u2019ll need to decrypted first in order to be uncompressed. Examples using xbstream \u00b6 Store the complete backup directly to a single file: $ innobackupex --stream = xbstream /root/backup/ > /root/backup/backup.xbstream To stream and compress the backup: $ innobackupex --stream = xbstream --compress /root/backup/ > /root/backup/backup.xbstream To unpack the backup to the /root/backup/ directory: $ xbstream -x < backup.xbstream -C /root/backup/ To send the compressed backup to another host and unpack it: $ innobackupex --compress --stream = xbstream /root/backup/ | ssh user@otherhost \"xbstream -x -C /root/backup/\" Examples using tar \u00b6 Store the complete backup directly to a tar archive: $ innobackupex --stream = tar /root/backup/ > /root/backup/out.tar To send the tar archive to another host: $ innobackupex --stream = tar ./ | ssh user@destination \\ \"cat - > /data/backups/backup.tar\" Note To extract Percona XtraBackup \u2019s archive you must use tar with -i option: $ tar -xizf backup.tar.gz Compress with your preferred compression tool: $ innobackupex --stream = tar ./ | gzip - > backup.tar.gz $ innobackupex --stream = tar ./ | bzip2 - > backup.tar.bz2 Note that the streamed backup will need to be prepared before restoration. Streaming mode does not prepare the backup.","title":"Streaming and Compressing Backups"},{"location":"innobackupex/streaming_backups_innobackupex.html#streaming-and-compressing-backups","text":"Streaming mode, supported by Percona XtraBackup , sends backup to STDOUT in special tar or xbstream format instead of copying files to the backup directory. This allows you to use other programs to filter the output of the backup, providing greater flexibility for storage of the backup. For example, compression is achieved by piping the output to a compression utility. One of the benefits of streaming backups and using Unix pipes is that the backups can be automatically encrypted. To use the streaming feature, you must use the innobackupex --stream , providing the format of the stream ( tar or xbstream ) and where to store the temporary files: $ innobackupex --stream = tar /tmp innobackupex uses xbstream to stream all of the data files to STDOUT , in a special xbstream format. See The xbstream binary for details. After it finishes streaming all of the data files to STDOUT , it stops xtrabackup and streams the saved log file too. When compression is enabled, xtrabackup compresses all output data, except the meta and non-InnoDB files which are not compressed, using the specified compression algorithm. The only currently supported algorithm is quicklz . The resulting files have the qpress archive format, i.e. every *.qp file produced by xtrabackup is essentially a one-file qpress archive and can be extracted and uncompressed by the qpress file archiver which is available from Percona Software repositories. Using xbstream as a stream option, backups can be copied and compressed in parallel which can significantly speed up the backup process. In case backups were both compressed and encrypted, they\u2019ll need to decrypted first in order to be uncompressed.","title":"Streaming and Compressing Backups"},{"location":"innobackupex/streaming_backups_innobackupex.html#examples-using-xbstream","text":"Store the complete backup directly to a single file: $ innobackupex --stream = xbstream /root/backup/ > /root/backup/backup.xbstream To stream and compress the backup: $ innobackupex --stream = xbstream --compress /root/backup/ > /root/backup/backup.xbstream To unpack the backup to the /root/backup/ directory: $ xbstream -x < backup.xbstream -C /root/backup/ To send the compressed backup to another host and unpack it: $ innobackupex --compress --stream = xbstream /root/backup/ | ssh user@otherhost \"xbstream -x -C /root/backup/\"","title":"Examples using xbstream"},{"location":"innobackupex/streaming_backups_innobackupex.html#examples-using-tar","text":"Store the complete backup directly to a tar archive: $ innobackupex --stream = tar /root/backup/ > /root/backup/out.tar To send the tar archive to another host: $ innobackupex --stream = tar ./ | ssh user@destination \\ \"cat - > /data/backups/backup.tar\" Note To extract Percona XtraBackup \u2019s archive you must use tar with -i option: $ tar -xizf backup.tar.gz Compress with your preferred compression tool: $ innobackupex --stream = tar ./ | gzip - > backup.tar.gz $ innobackupex --stream = tar ./ | bzip2 - > backup.tar.bz2 Note that the streamed backup will need to be prepared before restoration. Streaming mode does not prepare the backup.","title":"Examples using tar"},{"location":"innobackupex/throttling_ibk.html","text":"Throttling backups with innobackupex \u00b6 Although innobackupex does not block your database\u2019s operation, any backup can add load to the system being backed up. On systems that do not have much spare I/O capacity, it might be helpful to throttle the rate at which innobackupex reads and writes InnoDB data. You can do this with the innobackupex --throttle option. This option is passed directly to xtrabackup binary and only limits the operations on the logs and files of InnoDB tables. It doesn\u2019t have an effect on reading or writing files from tables with other storage engine. One way of checking the current I/O operations at a system is with iostat command. See Throttling Backups for details of how throttling works. Note innobackupex --throttle option works only during the backup phase, i.e. it will not work with innobackupex --apply-log and innobackupex --copy-back options.","title":"Throttling backups with *innobackupex*"},{"location":"innobackupex/throttling_ibk.html#throttling-backups-with-innobackupex","text":"Although innobackupex does not block your database\u2019s operation, any backup can add load to the system being backed up. On systems that do not have much spare I/O capacity, it might be helpful to throttle the rate at which innobackupex reads and writes InnoDB data. You can do this with the innobackupex --throttle option. This option is passed directly to xtrabackup binary and only limits the operations on the logs and files of InnoDB tables. It doesn\u2019t have an effect on reading or writing files from tables with other storage engine. One way of checking the current I/O operations at a system is with iostat command. See Throttling Backups for details of how throttling works. Note innobackupex --throttle option works only during the backup phase, i.e. it will not work with innobackupex --apply-log and innobackupex --copy-back options.","title":"Throttling backups with innobackupex"},{"location":"installation/apt_repo.html","text":"Installing Percona XtraBackup on Debian and Ubuntu \u00b6 Ready-to-use packages are available from the Percona XtraBackup software repositories and the Percona download page . Specific information on the supported platforms, products, and versions is described in Percona Software and Platform Lifecycle . What\u2019s in each DEB package? \u00b6 The percona-xtrabackup-24 package contains the latest Percona XtraBackup GA binaries and associated files. The percona-xtrabackup-dbg-24 package contains the debug symbols for binaries in percona-xtrabackup-24 . The percona-xtrabackup-test-24 package contains the test suite for Percona XtraBackup . The percona-xtrabackup package contains the older version of the Percona XtraBackup . Installing Percona XtraBackup via percona-release \u00b6 Percona XtraBackup , like many other Percona products, is installed with the percona-release package configuration tool. Download a deb package for percona-release the repository packages from Percona web: $ wget https://repo.percona.com/apt/percona-release_latest. $( lsb_release -sc ) _all.deb Install the downloaded package with dpkg . To do that, run the following commands as root or with sudo : $ sudo dpkg -i percona-release_latest. $( lsb_release -sc ) _all.deb Once you install this package the Percona repositories should be added. You can check the repository setup in the /etc/apt/sources.list.d/percona-release.list file. Enable the repository: percona-release enable-only tools release If Percona XtraBackup is intended to be used in combination with the upstream MySQL Server, you enable only the tools repository: percona-release enable-only tools . After that you can install the percona-xtrabackup-24 package: $ sudo apt install percona-xtrabackup-24 In order to make compressed backups, install the qpress package: $ sudo apt install qpress Apt-Pinning the packages \u00b6 In some cases you might need to \u201cpin\u201d the selected packages to avoid the upgrades from the distribution repositories. You\u2019ll need to make a new file /etc/apt/preferences.d/00percona.pref and add the following lines in it: Package: * Pin: release o=Percona Development Team Pin-Priority: 1001 For more information about the pinning you can check the official debian wiki . Installing Percona XtraBackup using downloaded deb packages \u00b6 Download the packages of the desired series for your architecture from the download page . Following example downloads the Percona XtraBackup 2.4.20 release package for Debian 9.0: $ wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.20/ \\ binary/debian/stretch/x86_64/percona-xtrabackup-24_2.4.20-1.stretch_amd64.deb Now you can install Percona XtraBackup by running: $ sudo dpkg -i percona-xtrabackup-24_2.4.20-1.stretch_amd64.deb Note Installing the packages manually like this, you must resolve all dependencies and install the missing packages yourself. Update the Curl utility in Debian 10 \u00b6 The default curl version, 7.64.0, in Debian 10 has known issues when attempting to reuse an already closed connection. This issue directly affects xbcloud and users may see intermittent backup failures. For more details, see curl #3750 or curl #3763 . Follow these steps to upgrade curl to version 7.74.0: Edit the /etc/apt/sources.list to add the following: deb http://ftp.de.debian.org/debian buster-backports main Refresh the apt sources: sudo apt update Install the version from buster-backports : $ sudo apt install curl/buster-backports Verify the version number: $ curl --version The result is similar to the following; curl 7.74.0 (x86_64-pc-linux-gnu) libcurl/7.74.0 Uninstalling Percona XtraBackup \u00b6 To uninstall Percona XtraBackup you\u2019ll need to remove all the installed packages. Remove the packages $ sudo apt remove percona-xtrabackup-24","title":"Installing Percona XtraBackup on Debian and Ubuntu"},{"location":"installation/apt_repo.html#installing-percona-xtrabackup-on-debian-and-ubuntu","text":"Ready-to-use packages are available from the Percona XtraBackup software repositories and the Percona download page . Specific information on the supported platforms, products, and versions is described in Percona Software and Platform Lifecycle .","title":"Installing Percona XtraBackup on Debian and Ubuntu"},{"location":"installation/apt_repo.html#whats-in-each-deb-package","text":"The percona-xtrabackup-24 package contains the latest Percona XtraBackup GA binaries and associated files. The percona-xtrabackup-dbg-24 package contains the debug symbols for binaries in percona-xtrabackup-24 . The percona-xtrabackup-test-24 package contains the test suite for Percona XtraBackup . The percona-xtrabackup package contains the older version of the Percona XtraBackup .","title":"What\u2019s in each DEB package?"},{"location":"installation/apt_repo.html#installing-percona-xtrabackup-via-percona-release","text":"Percona XtraBackup , like many other Percona products, is installed with the percona-release package configuration tool. Download a deb package for percona-release the repository packages from Percona web: $ wget https://repo.percona.com/apt/percona-release_latest. $( lsb_release -sc ) _all.deb Install the downloaded package with dpkg . To do that, run the following commands as root or with sudo : $ sudo dpkg -i percona-release_latest. $( lsb_release -sc ) _all.deb Once you install this package the Percona repositories should be added. You can check the repository setup in the /etc/apt/sources.list.d/percona-release.list file. Enable the repository: percona-release enable-only tools release If Percona XtraBackup is intended to be used in combination with the upstream MySQL Server, you enable only the tools repository: percona-release enable-only tools . After that you can install the percona-xtrabackup-24 package: $ sudo apt install percona-xtrabackup-24 In order to make compressed backups, install the qpress package: $ sudo apt install qpress","title":"Installing Percona XtraBackup via percona-release"},{"location":"installation/apt_repo.html#apt-pinning-the-packages","text":"In some cases you might need to \u201cpin\u201d the selected packages to avoid the upgrades from the distribution repositories. You\u2019ll need to make a new file /etc/apt/preferences.d/00percona.pref and add the following lines in it: Package: * Pin: release o=Percona Development Team Pin-Priority: 1001 For more information about the pinning you can check the official debian wiki .","title":"Apt-Pinning the packages"},{"location":"installation/apt_repo.html#installing-percona-xtrabackup-using-downloaded-deb-packages","text":"Download the packages of the desired series for your architecture from the download page . Following example downloads the Percona XtraBackup 2.4.20 release package for Debian 9.0: $ wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.20/ \\ binary/debian/stretch/x86_64/percona-xtrabackup-24_2.4.20-1.stretch_amd64.deb Now you can install Percona XtraBackup by running: $ sudo dpkg -i percona-xtrabackup-24_2.4.20-1.stretch_amd64.deb Note Installing the packages manually like this, you must resolve all dependencies and install the missing packages yourself.","title":"Installing Percona XtraBackup using downloaded deb packages"},{"location":"installation/apt_repo.html#update-the-curl-utility-in-debian-10","text":"The default curl version, 7.64.0, in Debian 10 has known issues when attempting to reuse an already closed connection. This issue directly affects xbcloud and users may see intermittent backup failures. For more details, see curl #3750 or curl #3763 . Follow these steps to upgrade curl to version 7.74.0: Edit the /etc/apt/sources.list to add the following: deb http://ftp.de.debian.org/debian buster-backports main Refresh the apt sources: sudo apt update Install the version from buster-backports : $ sudo apt install curl/buster-backports Verify the version number: $ curl --version The result is similar to the following; curl 7.74.0 (x86_64-pc-linux-gnu) libcurl/7.74.0","title":"Update the Curl utility in Debian 10"},{"location":"installation/apt_repo.html#uninstalling-percona-xtrabackup","text":"To uninstall Percona XtraBackup you\u2019ll need to remove all the installed packages. Remove the packages $ sudo apt remove percona-xtrabackup-24","title":"Uninstalling Percona XtraBackup"},{"location":"installation/binary-tarball.html","text":"Installing Percona XtraBackup from a Binary Tarball \u00b6 Percona provides binary tarballs of Percona XtraBackup . Binary tarballs contain pre-compiled executables, libraries, and other dependencies and are compressed tar archives. Extract the binary tarballs to any path. Binary tarballs are available for download and installation. The following table lists the tarballs available in Linux - Generic . Select the Percona XtraBackup 2.4 version number and the type of tarball for your installation. Binary tarballs support all distributions. After you have downloaded the binary tarballs, extract the tarball in the file location of your choice. Type Name Description Full percona-xtrabackup\u2013Linux.x86_64.glibc2.12.tar.gz Contains binaries, libraries, test files, and debug symbols Minimal percona-xtrabackup\u2013Linux.x86_64.glibc2.12-minimal.tar.gz Contains binaries, and libraries but does not include test files, or debug symbols Fetch and extract the correct binary tarball. For example, the following downloads the full tarball for version 2.4.24: $ wget https://downloads.percona.com/downloads/Percona-XtraBackup-2.4/Percona-XtraBackup-2.4.24/binary/tarball/percona-xtrabackup-2.4.24-Linux-x86_64.glibc2.12.tar.gz The result may be similar to the following: --2022-04-13 14:34:45-- https://downloads.percona.com/downloads/Percona-XtraBackup-2.4/Percona-XtraBackup-2.4.24/binary/tarball/percona-xtrabackup-2.4.24-Linux-x86_64.glibc2.12.tar.gz Resolving downloads.percona.com (downloads.percona.com)... 162.220.4.221, 162.220.4.222, 74.121.199.231 Connecting to downloads.percona.com (downloads.percona.com)|162.220.4.221|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 76263220 (73M) [application/x-gzip] Saving to: \u2018percona-xtrabackup-2.4.24-Linux-x86_64.glibc2.12.tar.gz\u2019 percona-xtrabackup-2.4.24-Linux-x86_64 100%[==========================================================================>] 72.73M 5.29MB/s in 20s 2022-04-13 14:35:05 (3.61 MB/s) - \u2018percona-xtrabackup-2.4.24-Linux-x86_64.glibc2.12.tar.gz\u2019 saved [76263220/76263220] Uncompress the file: $ tar xvf percona-xtrabackup-2.4.21-Linux-x86_64.glibc2.12.tar.gz An installation from a binary tarball requires doing certain tasks manually, such as configuring settings, and policies, that a repository package installation performs automatically.","title":"Installing Percona XtraBackup from a Binary Tarball"},{"location":"installation/binary-tarball.html#installing-percona-xtrabackup-from-a-binary-tarball","text":"Percona provides binary tarballs of Percona XtraBackup . Binary tarballs contain pre-compiled executables, libraries, and other dependencies and are compressed tar archives. Extract the binary tarballs to any path. Binary tarballs are available for download and installation. The following table lists the tarballs available in Linux - Generic . Select the Percona XtraBackup 2.4 version number and the type of tarball for your installation. Binary tarballs support all distributions. After you have downloaded the binary tarballs, extract the tarball in the file location of your choice. Type Name Description Full percona-xtrabackup\u2013Linux.x86_64.glibc2.12.tar.gz Contains binaries, libraries, test files, and debug symbols Minimal percona-xtrabackup\u2013Linux.x86_64.glibc2.12-minimal.tar.gz Contains binaries, and libraries but does not include test files, or debug symbols Fetch and extract the correct binary tarball. For example, the following downloads the full tarball for version 2.4.24: $ wget https://downloads.percona.com/downloads/Percona-XtraBackup-2.4/Percona-XtraBackup-2.4.24/binary/tarball/percona-xtrabackup-2.4.24-Linux-x86_64.glibc2.12.tar.gz The result may be similar to the following: --2022-04-13 14:34:45-- https://downloads.percona.com/downloads/Percona-XtraBackup-2.4/Percona-XtraBackup-2.4.24/binary/tarball/percona-xtrabackup-2.4.24-Linux-x86_64.glibc2.12.tar.gz Resolving downloads.percona.com (downloads.percona.com)... 162.220.4.221, 162.220.4.222, 74.121.199.231 Connecting to downloads.percona.com (downloads.percona.com)|162.220.4.221|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 76263220 (73M) [application/x-gzip] Saving to: \u2018percona-xtrabackup-2.4.24-Linux-x86_64.glibc2.12.tar.gz\u2019 percona-xtrabackup-2.4.24-Linux-x86_64 100%[==========================================================================>] 72.73M 5.29MB/s in 20s 2022-04-13 14:35:05 (3.61 MB/s) - \u2018percona-xtrabackup-2.4.24-Linux-x86_64.glibc2.12.tar.gz\u2019 saved [76263220/76263220] Uncompress the file: $ tar xvf percona-xtrabackup-2.4.21-Linux-x86_64.glibc2.12.tar.gz An installation from a binary tarball requires doing certain tasks manually, such as configuring settings, and policies, that a repository package installation performs automatically.","title":"Installing Percona XtraBackup from a Binary Tarball"},{"location":"installation/compiling_xtrabackup.html","text":"Compiling and Installing from Source Code \u00b6 The source code is available from the Percona XtraBackup Github project . The easiest way to get the code is by using the git clone command. Then, switch to the release branch that you want to install, such as 2.4 . $ git clone https://github.com/percona/percona-xtrabackup.git $ cd percona-xtrabackup $ git checkout 2 .4 Step 1: Installing prerequisites \u00b6 The following packages and tools must be installed to compile Percona XtraBackup from source. These might vary from system to system. Important In order to build Percona XtraBackup v8.0 from source, you need to use cmake version 3. In your distribution, it may be available either as a separate package cmake3 or as cmake . To check which version is installed, run cmake --version and if it does report a version 3, install cmake3 for your system. See also https://cmake.org/ Debian or Ubuntu using apt \u00b6 $ sudo apt install build-essential flex bison automake autoconf \\ libtool cmake libaio-dev mysql-client libncurses-dev zlib1g-dev \\ libgcrypt11-dev libev-dev libcurl4-gnutls-dev vim-common To install the man pages, install the python3-sphinx package: $ sudo apt install python3-sphinx CentOS or Red Hat using yum \u00b6 Percona Xtrabackup requires GCC version 5.3 or higher. If the version of GCC installed on your system is lower then you may need to install and enable the Developer Toolset on RPM -based distributions to make sure that you use the latest GCC compiler and development tools. Then, install cmake and other dependencies: $ sudo yum install cmake openssl-devel libaio libaio-devel automake autoconf \\ bison libtool ncurses-devel libgcrypt-devel libev-devel libcurl-devel zlib-devel \\ vim-common To install the man pages, install the python3-sphinx package: $ sudo yum install python3-sphinx Step 2: Generating the build pipeline \u00b6 At this step, you have cmake run the commands in the CMakeList.txt file to generate the build pipeline, i.e. a native build environment that will be used to compile the source code). Change to the directory where you cloned the Percona XtraBackup repository $ cd percona-xtrabackup Create a directory to store the compiled files and then change to that directory: $ mkdir build $ cd build Run cmake or cmake3 . In either case, the options you need to use are the same. Note You can build Percona XtraBackup with man pages but this requires python-sphinx package which isn\u2019t available from that main repositories for every distribution. If you installed the python-sphinx package you need to remove the -DWITH_MAN_PAGES=OFF from previous command. $ cmake -DWITH_BOOST = PATH-TO-BOOST-LIBRARY -DDOWNLOAD_BOOST = ON \\ -DBUILD_CONFIG = xtrabackup_release -DWITH_MAN_PAGES = OFF -B .. More information about parameters \u00b6 -DWITH_BOOST For the -DWITH_BOOST parameter, specify the name of a directory to download the boost library to. This directory will be created automatically in your current directory. -B (--build) Percona XtraBackup is configured to forbid generating the build pipeline for make in the same directory where you store your sources. The -B parameter refers to the directory that contains the source code. In this example we use the relative path to the parent directory (..). Important CMake Error at CMakeLists.txt:367 (MESSAGE): Please do not build in-source. Out-of source builds are highly recommended: you can have multiple builds for the same source, and there is an easy way to do cleanup, simply remove the build directory (note that \u2018make clean\u2019 or \u2018make distclean\u2019 does not work) You can force in-source build by invoking cmake with -DFORCE_INSOURCE_BUILD=1 -DWITH_MAN_PAGES To build Percona XtraBackup man pages, use ON or remove this parameter from the command line (it is ON by default). To install the man pages, install the python3-sphinx package: Step 2: Compiling the source code \u00b6 To compile the source code in your build directory, use the make command. Important The computer where you intend to compile Percona XtraBackup 8.0 must have at least 2G of RAM available. Change to the build directory (created at Step 2: Generating the build pipeline ). Run the make command. This command may take a long time to complete. $ make Step 3: Installing on the target system \u00b6 The following command installs all Percona XtraBackup binaries xtrabackup and tests to default location on the target system: /usr/local/xtrabackup . Run make install to install Percona XtraBackup to the default location. $ sudo make install Installing to a non-default location \u00b6 You may use the DESTDIR parameter with make install to install Percona XtraBackup to another location. Make sure that the effective user is able to write to the destination you choose. $ sudo make DESTDIR = <DIR_NAME> install In fact, the destination directory is determined by the installation layout ( -DINSTALL_LAYOUT ) that cmake applies (see Step 2: Generating the build pipeline). In addition to the installation directory, this parameter controls a number of other destinations that you can adjust for your system. By default, this parameter is set to STANDALONE , which implies the installation directory to be /usr/local/xtrabackup. See also MySQL Documentation: -DINSTALL_LAYOUT Step 4: Running \u00b6 After Percona XtraBackup is installed on your system, you may run it by using the full path to the xtrabackup command: $ /usr/local/xtrabackup/bin/xtrabackup Update your PATH environment variable if you would like to use the command on the command line directly. $# Setting $PATH on the command line $ PATH = $PATH :/usr/local/xtrabackup/bin/xtrabackup $# Run xtrabackup directly $ xtrabackup Alternatively, you may consider placing a soft link (using ln -s ) to one of the locations listed in your PATH environment variable. See also man ln To view the documentation with man , update the MANPATH variable.","title":"Compiling and Installing from Source Code"},{"location":"installation/compiling_xtrabackup.html#compiling-and-installing-from-source-code","text":"The source code is available from the Percona XtraBackup Github project . The easiest way to get the code is by using the git clone command. Then, switch to the release branch that you want to install, such as 2.4 . $ git clone https://github.com/percona/percona-xtrabackup.git $ cd percona-xtrabackup $ git checkout 2 .4","title":"Compiling and Installing from Source Code"},{"location":"installation/compiling_xtrabackup.html#step-1-installing-prerequisites","text":"The following packages and tools must be installed to compile Percona XtraBackup from source. These might vary from system to system. Important In order to build Percona XtraBackup v8.0 from source, you need to use cmake version 3. In your distribution, it may be available either as a separate package cmake3 or as cmake . To check which version is installed, run cmake --version and if it does report a version 3, install cmake3 for your system. See also https://cmake.org/","title":"Step 1: Installing prerequisites"},{"location":"installation/compiling_xtrabackup.html#debian-or-ubuntu-using-apt","text":"$ sudo apt install build-essential flex bison automake autoconf \\ libtool cmake libaio-dev mysql-client libncurses-dev zlib1g-dev \\ libgcrypt11-dev libev-dev libcurl4-gnutls-dev vim-common To install the man pages, install the python3-sphinx package: $ sudo apt install python3-sphinx","title":"Debian or Ubuntu using apt"},{"location":"installation/compiling_xtrabackup.html#centos-or-red-hat-using-yum","text":"Percona Xtrabackup requires GCC version 5.3 or higher. If the version of GCC installed on your system is lower then you may need to install and enable the Developer Toolset on RPM -based distributions to make sure that you use the latest GCC compiler and development tools. Then, install cmake and other dependencies: $ sudo yum install cmake openssl-devel libaio libaio-devel automake autoconf \\ bison libtool ncurses-devel libgcrypt-devel libev-devel libcurl-devel zlib-devel \\ vim-common To install the man pages, install the python3-sphinx package: $ sudo yum install python3-sphinx","title":"CentOS or Red Hat using yum"},{"location":"installation/compiling_xtrabackup.html#step-2-generating-the-build-pipeline","text":"At this step, you have cmake run the commands in the CMakeList.txt file to generate the build pipeline, i.e. a native build environment that will be used to compile the source code). Change to the directory where you cloned the Percona XtraBackup repository $ cd percona-xtrabackup Create a directory to store the compiled files and then change to that directory: $ mkdir build $ cd build Run cmake or cmake3 . In either case, the options you need to use are the same. Note You can build Percona XtraBackup with man pages but this requires python-sphinx package which isn\u2019t available from that main repositories for every distribution. If you installed the python-sphinx package you need to remove the -DWITH_MAN_PAGES=OFF from previous command. $ cmake -DWITH_BOOST = PATH-TO-BOOST-LIBRARY -DDOWNLOAD_BOOST = ON \\ -DBUILD_CONFIG = xtrabackup_release -DWITH_MAN_PAGES = OFF -B ..","title":"Step 2: Generating the build pipeline"},{"location":"installation/compiling_xtrabackup.html#more-information-about-parameters","text":"-DWITH_BOOST For the -DWITH_BOOST parameter, specify the name of a directory to download the boost library to. This directory will be created automatically in your current directory. -B (--build) Percona XtraBackup is configured to forbid generating the build pipeline for make in the same directory where you store your sources. The -B parameter refers to the directory that contains the source code. In this example we use the relative path to the parent directory (..). Important CMake Error at CMakeLists.txt:367 (MESSAGE): Please do not build in-source. Out-of source builds are highly recommended: you can have multiple builds for the same source, and there is an easy way to do cleanup, simply remove the build directory (note that \u2018make clean\u2019 or \u2018make distclean\u2019 does not work) You can force in-source build by invoking cmake with -DFORCE_INSOURCE_BUILD=1 -DWITH_MAN_PAGES To build Percona XtraBackup man pages, use ON or remove this parameter from the command line (it is ON by default). To install the man pages, install the python3-sphinx package:","title":"More information about parameters"},{"location":"installation/compiling_xtrabackup.html#step-2-compiling-the-source-code","text":"To compile the source code in your build directory, use the make command. Important The computer where you intend to compile Percona XtraBackup 8.0 must have at least 2G of RAM available. Change to the build directory (created at Step 2: Generating the build pipeline ). Run the make command. This command may take a long time to complete. $ make","title":"Step 2: Compiling the source code"},{"location":"installation/compiling_xtrabackup.html#step-3-installing-on-the-target-system","text":"The following command installs all Percona XtraBackup binaries xtrabackup and tests to default location on the target system: /usr/local/xtrabackup . Run make install to install Percona XtraBackup to the default location. $ sudo make install","title":"Step 3: Installing on the target system"},{"location":"installation/compiling_xtrabackup.html#installing-to-a-non-default-location","text":"You may use the DESTDIR parameter with make install to install Percona XtraBackup to another location. Make sure that the effective user is able to write to the destination you choose. $ sudo make DESTDIR = <DIR_NAME> install In fact, the destination directory is determined by the installation layout ( -DINSTALL_LAYOUT ) that cmake applies (see Step 2: Generating the build pipeline). In addition to the installation directory, this parameter controls a number of other destinations that you can adjust for your system. By default, this parameter is set to STANDALONE , which implies the installation directory to be /usr/local/xtrabackup. See also MySQL Documentation: -DINSTALL_LAYOUT","title":"Installing to a non-default location"},{"location":"installation/compiling_xtrabackup.html#step-4-running","text":"After Percona XtraBackup is installed on your system, you may run it by using the full path to the xtrabackup command: $ /usr/local/xtrabackup/bin/xtrabackup Update your PATH environment variable if you would like to use the command on the command line directly. $# Setting $PATH on the command line $ PATH = $PATH :/usr/local/xtrabackup/bin/xtrabackup $# Run xtrabackup directly $ xtrabackup Alternatively, you may consider placing a soft link (using ln -s ) to one of the locations listed in your PATH environment variable. See also man ln To view the documentation with man , update the MANPATH variable.","title":"Step 4: Running"},{"location":"installation/docker.html","text":"Running Percona XtraBackup in a Docker container \u00b6 Docker allows you to run applications in a lightweight unit called a container. You can run Percona XtraBackup in a Docker container without installing the product. All required libraries are available in the container. Being a lightweight execution environment, Docker containers enable creating configurations where each program runs in a separate container. You may run Percona Server for MySQL in one container and Percona XtraBackup in another. Docker images offer a range of options. Create a Docker container based on a Docker image. Docker images for Percona XtraBackup are hosted publicly on Docker Hub at percona/percona-xtrabackup . $ sudo docker create ... percona/percona-xtrabackup --name xtrabackup ... Scope of this section \u00b6 This section demonstrates how to backup data on a Percona Server for MySQL running in another Docker container. Installing Docker \u00b6 Your operating system may already provide a package for docker . However, the versions of Docker provided by your operating system are likely to be outdated. Use the installation instructions for your operating system available from the Docker site to set up the latest version of docker . Note Docker Documentation: * How to use Docker * Installing * Getting started Connecting to a Percona Server for MySQL container \u00b6 Percona XtraBackup works in combination with a database server. When running a Docker container for Percona XtraBackup, you can make backups for a database server either installed on the host machine or running in a separate Docker container. To set up a database server on a host machine or in Docker container, follow the documentation of the supported product that you intend to use with Percona XtraBackup. See also Percona Server for MySQL Documentation: * Installing on a host machine * Running in a Docker container $ sudo docker run -d --name percona-server-mysql-5.7 \\ -e MYSQL_ROOT_PASSWORD = root percona/percona-server:5.7 As soon as Percona Server for MySQL runs, add some data to it. Now, you are ready to make backups with Percona XtraBackup. Creating a Docker container from Percona XtraBackup image \u00b6 You can create a Docker container based on Percona XtraBackup image with either docker create or docker run command. docker create creates a Docker container and makes it available for starting later. Docker downloads the Percona XtraBackup image from the Docker Hub. If it is not the first time you use the selected image, Docker uses the image available locally. $ sudo docker create --name percona-xtrabackup-2.4 --volumes-from percona-server-mysql-5.7 \\ percona/percona-xtrabackup:2.4 \\ xtrabackup --backup --datadir = /var/lib/mysql/ --target-dir = /backup \\ --user = root --password = mysql With --name you give a meaningful name to your new Docker container so that you could easily locate it among your other containers. The --volumes-from referring to percona-server-mysql indicates that you indend to use the same data as the percona-server-mysql container. Run the container with exactly the same parameters that were used when the container was created: $ sudo docker start -ai percona-xtrabackup-2.4 This command starts the percona-xtrabackup container, attaches to its input/output streams, and opens an interactive shell. The docker run is a shortcut command that creates a Docker container and then immediately runs it. $ sudo docker run --name percona-xtrabackup-2.4 --volumes-from percona-server-mysql-5.7 \\ percona/percona-xtrabackup:2.4 xtrabackup --backup --data-dir = /var/lib/mysql --target-dir = /backup --user = root --password = mysql See also More in Docker documentation * Docker volumes as persistent data storage for containers * More information about containers","title":"Running Percona XtraBackup in a Docker container"},{"location":"installation/docker.html#running-percona-xtrabackup-in-a-docker-container","text":"Docker allows you to run applications in a lightweight unit called a container. You can run Percona XtraBackup in a Docker container without installing the product. All required libraries are available in the container. Being a lightweight execution environment, Docker containers enable creating configurations where each program runs in a separate container. You may run Percona Server for MySQL in one container and Percona XtraBackup in another. Docker images offer a range of options. Create a Docker container based on a Docker image. Docker images for Percona XtraBackup are hosted publicly on Docker Hub at percona/percona-xtrabackup . $ sudo docker create ... percona/percona-xtrabackup --name xtrabackup ...","title":"Running Percona XtraBackup in a Docker container"},{"location":"installation/docker.html#scope-of-this-section","text":"This section demonstrates how to backup data on a Percona Server for MySQL running in another Docker container.","title":"Scope of this section"},{"location":"installation/docker.html#installing-docker","text":"Your operating system may already provide a package for docker . However, the versions of Docker provided by your operating system are likely to be outdated. Use the installation instructions for your operating system available from the Docker site to set up the latest version of docker . Note Docker Documentation: * How to use Docker * Installing * Getting started","title":"Installing Docker"},{"location":"installation/docker.html#connecting-to-a-percona-server-for-mysql-container","text":"Percona XtraBackup works in combination with a database server. When running a Docker container for Percona XtraBackup, you can make backups for a database server either installed on the host machine or running in a separate Docker container. To set up a database server on a host machine or in Docker container, follow the documentation of the supported product that you intend to use with Percona XtraBackup. See also Percona Server for MySQL Documentation: * Installing on a host machine * Running in a Docker container $ sudo docker run -d --name percona-server-mysql-5.7 \\ -e MYSQL_ROOT_PASSWORD = root percona/percona-server:5.7 As soon as Percona Server for MySQL runs, add some data to it. Now, you are ready to make backups with Percona XtraBackup.","title":"Connecting to a Percona Server for MySQL container"},{"location":"installation/docker.html#creating-a-docker-container-from-percona-xtrabackup-image","text":"You can create a Docker container based on Percona XtraBackup image with either docker create or docker run command. docker create creates a Docker container and makes it available for starting later. Docker downloads the Percona XtraBackup image from the Docker Hub. If it is not the first time you use the selected image, Docker uses the image available locally. $ sudo docker create --name percona-xtrabackup-2.4 --volumes-from percona-server-mysql-5.7 \\ percona/percona-xtrabackup:2.4 \\ xtrabackup --backup --datadir = /var/lib/mysql/ --target-dir = /backup \\ --user = root --password = mysql With --name you give a meaningful name to your new Docker container so that you could easily locate it among your other containers. The --volumes-from referring to percona-server-mysql indicates that you indend to use the same data as the percona-server-mysql container. Run the container with exactly the same parameters that were used when the container was created: $ sudo docker start -ai percona-xtrabackup-2.4 This command starts the percona-xtrabackup container, attaches to its input/output streams, and opens an interactive shell. The docker run is a shortcut command that creates a Docker container and then immediately runs it. $ sudo docker run --name percona-xtrabackup-2.4 --volumes-from percona-server-mysql-5.7 \\ percona/percona-xtrabackup:2.4 xtrabackup --backup --data-dir = /var/lib/mysql --target-dir = /backup --user = root --password = mysql See also More in Docker documentation * Docker volumes as persistent data storage for containers * More information about containers","title":"Creating a Docker container from Percona XtraBackup image"},{"location":"installation/yum_repo.html","text":"Installing Percona XtraBackup on Red Hat Enterprise Linux and CentOS \u00b6 Ready-to-use packages are available from the Percona XtraBackup software repositories and the download page . The Percona yum repository supports popular RPM -based operating systems, including the Amazon Linux AMI . The easiest way to install the Percona Yum repository is to install an RPM that configures yum and installs the Percona GPG key . Specific information on the supported platforms, products, and versions is described in Percona Software and Platform Lifecycle . What\u2019s in each RPM package? \u00b6 The percona-xtrabackup-24 package contains the latest Percona XtraBackup GA binaries and associated files. The percona-xtrabackup-24-debuginfo package contains the debug symbols for binaries in percona-xtrabackup-24 . The percona-xtrabackup-test-24 package contains the test suite for Percona XtraBackup . The percona-xtrabackup package contains the older version of the Percona XtraBackup . Installing Percona XtraBackup from Percona yum repository \u00b6 Install the percona-release configuration tool You can install the yum repository for percona-release by running the following command as a root user or with sudo : $ yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm You should see some output such as the following: Retrieving https://repo.percona.com/yum/percona-release-latest.noarch.rpm Preparing... ########################################### [100%] 1:percona-release ########################################### [100%] Note RHEL / Centos 5 doesn\u2019t support installing the packages directly from the remote location so you\u2019ll need to download the package first and install it manually with rpm: $ wget https://repo.percona.com/yum/percona-release-latest.noarch.rpm $ rpm -ivH percona-release-latest.noarch.rpm Testing the repository Make sure packages are now available from the repository, by executing the following command: $ yum list | grep percona You should see output similar to the following: ... percona-xtrabackup-20.x86_64 2.0.8-587.rhel5 percona-release-x86_64 percona-xtrabackup-20-debuginfo.x86_64 2.0.8-587.rhel5 percona-release-x86_64 percona-xtrabackup-20-test.x86_64 2.0.8-587.rhel5 percona-release-x86_64 percona-xtrabackup-21.x86_64 2.1.9-746.rhel5 percona-release-x86_64 percona-xtrabackup-21-debuginfo.x86_64 2.1.9-746.rhel5 percona-release-x86_64 percona-xtrabackup-22.x86_64 2.2.13-1.el5 percona-release-x86_64 percona-xtrabackup-22-debuginfo.x86_64 2.2.13-1.el5 percona-release-x86_64 percona-xtrabackup-debuginfo.x86_64 2.3.5-1.el5 percona-release-x86_64 percona-xtrabackup-test.x86_64 2.3.5-1.el5 percona-release-x86_64 percona-xtrabackup-test-21.x86_64 2.1.9-746.rhel5 percona-release-x86_64 percona-xtrabackup-test-22.x86_64 2.2.13-1.el5 percona-release-x86_64 ... Enable the repository: percona-release enable-only tools release If Percona XtraBackup is intented to be used in combination with the upstream MySQL Server, you only need to enable the tools repository: percona-release enable-only tools . Install Percona XtraBackup by running: yum install percona-xtrabackup-24 Warning In order to sucessfully install Percona XtraBackup on CentOS prior to version 7, the libev package needs to be installed first. This package libev package can be installed from the EPEL repositories. Percona yum Testing Repository \u00b6 Percona offers pre-release builds from our testing repository. To subscribe to the testing repository, you\u2019ll need to enable the testing repository in /etc/yum.repos.d/percona-release.repo. To do so, set both percona-testing-$basearch and percona-testing-noarch to enabled = 1 (Note that there are 3 sections in this file: release, testing and experimental - in this case it is the second section that requires updating). NOTE: You\u2019ll need to install the Percona repository first (ref above) if this hasn\u2019t been done already. To be able to make compressed backups, install the qpress package: $ yum install qpress See also Compressed Backup Installing Percona XtraBackup using downloaded rpm packages \u00b6 Download the packages of the desired series for your architecture from the download page . Following example will download Percona XtraBackup 2.4.4 release package for CentOS 7: $ wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.4/ \\ binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm Now you can install Percona XtraBackup by running: $ yum localinstall percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm Note When installing packages manually like this, you\u2019ll need to make sure to resolve all the dependencies and install missing packages yourself. Uninstalling Percona XtraBackup \u00b6 To completely uninstall Percona XtraBackup you\u2019ll need to remove all the installed packages. Remove the packages yum remove percona-xtrabackup","title":"Installing Percona XtraBackup on Red Hat Enterprise Linux and CentOS"},{"location":"installation/yum_repo.html#installing-percona-xtrabackup-on-red-hat-enterprise-linux-and-centos","text":"Ready-to-use packages are available from the Percona XtraBackup software repositories and the download page . The Percona yum repository supports popular RPM -based operating systems, including the Amazon Linux AMI . The easiest way to install the Percona Yum repository is to install an RPM that configures yum and installs the Percona GPG key . Specific information on the supported platforms, products, and versions is described in Percona Software and Platform Lifecycle .","title":"Installing Percona XtraBackup on Red Hat Enterprise Linux and CentOS"},{"location":"installation/yum_repo.html#whats-in-each-rpm-package","text":"The percona-xtrabackup-24 package contains the latest Percona XtraBackup GA binaries and associated files. The percona-xtrabackup-24-debuginfo package contains the debug symbols for binaries in percona-xtrabackup-24 . The percona-xtrabackup-test-24 package contains the test suite for Percona XtraBackup . The percona-xtrabackup package contains the older version of the Percona XtraBackup .","title":"What\u2019s in each RPM package?"},{"location":"installation/yum_repo.html#installing-percona-xtrabackup-from-percona-yum-repository","text":"Install the percona-release configuration tool You can install the yum repository for percona-release by running the following command as a root user or with sudo : $ yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm You should see some output such as the following: Retrieving https://repo.percona.com/yum/percona-release-latest.noarch.rpm Preparing... ########################################### [100%] 1:percona-release ########################################### [100%] Note RHEL / Centos 5 doesn\u2019t support installing the packages directly from the remote location so you\u2019ll need to download the package first and install it manually with rpm: $ wget https://repo.percona.com/yum/percona-release-latest.noarch.rpm $ rpm -ivH percona-release-latest.noarch.rpm Testing the repository Make sure packages are now available from the repository, by executing the following command: $ yum list | grep percona You should see output similar to the following: ... percona-xtrabackup-20.x86_64 2.0.8-587.rhel5 percona-release-x86_64 percona-xtrabackup-20-debuginfo.x86_64 2.0.8-587.rhel5 percona-release-x86_64 percona-xtrabackup-20-test.x86_64 2.0.8-587.rhel5 percona-release-x86_64 percona-xtrabackup-21.x86_64 2.1.9-746.rhel5 percona-release-x86_64 percona-xtrabackup-21-debuginfo.x86_64 2.1.9-746.rhel5 percona-release-x86_64 percona-xtrabackup-22.x86_64 2.2.13-1.el5 percona-release-x86_64 percona-xtrabackup-22-debuginfo.x86_64 2.2.13-1.el5 percona-release-x86_64 percona-xtrabackup-debuginfo.x86_64 2.3.5-1.el5 percona-release-x86_64 percona-xtrabackup-test.x86_64 2.3.5-1.el5 percona-release-x86_64 percona-xtrabackup-test-21.x86_64 2.1.9-746.rhel5 percona-release-x86_64 percona-xtrabackup-test-22.x86_64 2.2.13-1.el5 percona-release-x86_64 ... Enable the repository: percona-release enable-only tools release If Percona XtraBackup is intented to be used in combination with the upstream MySQL Server, you only need to enable the tools repository: percona-release enable-only tools . Install Percona XtraBackup by running: yum install percona-xtrabackup-24 Warning In order to sucessfully install Percona XtraBackup on CentOS prior to version 7, the libev package needs to be installed first. This package libev package can be installed from the EPEL repositories.","title":"Installing Percona XtraBackup from Percona yum repository"},{"location":"installation/yum_repo.html#percona-yum-testing-repository","text":"Percona offers pre-release builds from our testing repository. To subscribe to the testing repository, you\u2019ll need to enable the testing repository in /etc/yum.repos.d/percona-release.repo. To do so, set both percona-testing-$basearch and percona-testing-noarch to enabled = 1 (Note that there are 3 sections in this file: release, testing and experimental - in this case it is the second section that requires updating). NOTE: You\u2019ll need to install the Percona repository first (ref above) if this hasn\u2019t been done already. To be able to make compressed backups, install the qpress package: $ yum install qpress See also Compressed Backup","title":"Percona yum Testing Repository"},{"location":"installation/yum_repo.html#installing-percona-xtrabackup-using-downloaded-rpm-packages","text":"Download the packages of the desired series for your architecture from the download page . Following example will download Percona XtraBackup 2.4.4 release package for CentOS 7: $ wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.4/ \\ binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm Now you can install Percona XtraBackup by running: $ yum localinstall percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm Note When installing packages manually like this, you\u2019ll need to make sure to resolve all the dependencies and install missing packages yourself.","title":"Installing Percona XtraBackup using downloaded rpm packages"},{"location":"installation/yum_repo.html#uninstalling-percona-xtrabackup","text":"To completely uninstall Percona XtraBackup you\u2019ll need to remove all the installed packages. Remove the packages yum remove percona-xtrabackup","title":"Uninstalling Percona XtraBackup"},{"location":"release-notes/2.4/2.4.0-rc1.html","text":"Percona XtraBackup 2.4.0-rc1 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.0-rc1 on February 8 th 2016. Downloads are available from our download site and from apt and yum repositories. This is a Release Candidate quality release and it is not intended for production. If you want a high quality, Generally Available release, the current Stable version should be used (currently 2.3.3 in the 2.3 series at the time of writing). New features \u00b6 Percona XtraBackup has implemented basic support for MySQL 5.7 and Percona Server for MySQL 5.7. Known Issues \u00b6 Backed-up table data could not be recovered if backup was taken while running OPTIMIZE TABLE (bug PXB-1360 ) or ALTER TABLE ... TABLESPACE (bug PXB-1360 ) on that table. Compact Backups currently don\u2019t work due to bug PXB-372 .","title":"Percona XtraBackup 2.4.0-rc1"},{"location":"release-notes/2.4/2.4.0-rc1.html#percona-xtrabackup-240-rc1","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.0-rc1 on February 8 th 2016. Downloads are available from our download site and from apt and yum repositories. This is a Release Candidate quality release and it is not intended for production. If you want a high quality, Generally Available release, the current Stable version should be used (currently 2.3.3 in the 2.3 series at the time of writing).","title":"Percona XtraBackup 2.4.0-rc1"},{"location":"release-notes/2.4/2.4.0-rc1.html#new-features","text":"Percona XtraBackup has implemented basic support for MySQL 5.7 and Percona Server for MySQL 5.7.","title":"New features"},{"location":"release-notes/2.4/2.4.0-rc1.html#known-issues","text":"Backed-up table data could not be recovered if backup was taken while running OPTIMIZE TABLE (bug PXB-1360 ) or ALTER TABLE ... TABLESPACE (bug PXB-1360 ) on that table. Compact Backups currently don\u2019t work due to bug PXB-372 .","title":"Known Issues"},{"location":"release-notes/2.4/2.4.1.html","text":"Percona XtraBackup 2.4.1 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.1 on February 16 th 2016. Downloads are available from our download site and from apt and yum repositories. This release is the first GA (Generally Available) stable release in the 2.4 series. This release contains all the features and bug fixes in Percona XtraBackup 2.3.3, plus the following: New features \u00b6 Percona XtraBackup has implemented basic support for MySQL 5.7 and Percona Server for MySQL 5.7. Bugs Fixed \u00b6 Percona XtraBackup didn\u2019t respect innodb_log_file_size variable stored in backup-my.cnf . Bug fixed PXB-450 . If server would run out of space while backups were taken with innobackupex \u2013rsync option backup process would fail but innobackupex would still complete with completed OK! message. Bug fixed PXB-459 . Percona XtraBackup was silently skipping extra arguments. Bug fixed PXB-747 ( Fungo Wang ). Other bugs fixed: PXB-1368 and 1363 .","title":"Percona XtraBackup 2.4.1"},{"location":"release-notes/2.4/2.4.1.html#percona-xtrabackup-241","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.1 on February 16 th 2016. Downloads are available from our download site and from apt and yum repositories. This release is the first GA (Generally Available) stable release in the 2.4 series. This release contains all the features and bug fixes in Percona XtraBackup 2.3.3, plus the following:","title":"Percona XtraBackup 2.4.1"},{"location":"release-notes/2.4/2.4.1.html#new-features","text":"Percona XtraBackup has implemented basic support for MySQL 5.7 and Percona Server for MySQL 5.7.","title":"New features"},{"location":"release-notes/2.4/2.4.1.html#bugs-fixed","text":"Percona XtraBackup didn\u2019t respect innodb_log_file_size variable stored in backup-my.cnf . Bug fixed PXB-450 . If server would run out of space while backups were taken with innobackupex \u2013rsync option backup process would fail but innobackupex would still complete with completed OK! message. Bug fixed PXB-459 . Percona XtraBackup was silently skipping extra arguments. Bug fixed PXB-747 ( Fungo Wang ). Other bugs fixed: PXB-1368 and 1363 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.10.html","text":"Percona XtraBackup 2.4.10 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.10 on March 30, 2018. Downloads are available from our download site and from apt and yum repositories. This release is based on MySQL 5.7.19 and is the current GA (Generally Available) stable release in the 2.4 series. Starting from now, Percona XtraBackup issue tracking system was moved from launchpad to JIRA . All Percona software is open-source and free. Bugs fixed \u00b6 xbcrypt with --encrypt-key-file option was failing due to regression in Percona XtraBackup 2.4.9. Bug fixed PXB-518 . Simultaneous usage of both --lock-ddl and --lock-ddl-per-table options caused Percona XtraBackup lock with the backup process never completed. Bug fixed PXB-792 . Compilation under Mac OS X was broken. Bug fixed PXB-796 . A regression of the maximum number of pending reads and the unnoticed earlier possibility of a pending reads related deadlock caused Percona XtraBackup to stuck in prepare stage. Bug fixed: PXB-1467 . Percona XtraBackup skipped tablespaces with corrupted first page instead of aborting the backup. Bug fixed PXB-1497 . Other bugs fixed: PXB-513 .","title":"Percona XtraBackup 2.4.10"},{"location":"release-notes/2.4/2.4.10.html#percona-xtrabackup-2410","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.10 on March 30, 2018. Downloads are available from our download site and from apt and yum repositories. This release is based on MySQL 5.7.19 and is the current GA (Generally Available) stable release in the 2.4 series. Starting from now, Percona XtraBackup issue tracking system was moved from launchpad to JIRA . All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.10"},{"location":"release-notes/2.4/2.4.10.html#bugs-fixed","text":"xbcrypt with --encrypt-key-file option was failing due to regression in Percona XtraBackup 2.4.9. Bug fixed PXB-518 . Simultaneous usage of both --lock-ddl and --lock-ddl-per-table options caused Percona XtraBackup lock with the backup process never completed. Bug fixed PXB-792 . Compilation under Mac OS X was broken. Bug fixed PXB-796 . A regression of the maximum number of pending reads and the unnoticed earlier possibility of a pending reads related deadlock caused Percona XtraBackup to stuck in prepare stage. Bug fixed: PXB-1467 . Percona XtraBackup skipped tablespaces with corrupted first page instead of aborting the backup. Bug fixed PXB-1497 . Other bugs fixed: PXB-513 .","title":"Bugs fixed"},{"location":"release-notes/2.4/2.4.11.html","text":"Percona XtraBackup 2.4.11 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.11 on April 23, 2018. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free. New features and improvements \u00b6 The support of the Percona Server for MySQL encrypted general tablespaces was implemented in this version of Percona XtraBackup . Issue fixed PXB-1513 . Percona XtraBackup is now able to backup encrypted Percona Server for MySQL instances which are using keyring_vault plugin . Issue fixed PXB-1514 .","title":"Percona XtraBackup 2.4.11"},{"location":"release-notes/2.4/2.4.11.html#percona-xtrabackup-2411","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.11 on April 23, 2018. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.11"},{"location":"release-notes/2.4/2.4.11.html#new-features-and-improvements","text":"The support of the Percona Server for MySQL encrypted general tablespaces was implemented in this version of Percona XtraBackup . Issue fixed PXB-1513 . Percona XtraBackup is now able to backup encrypted Percona Server for MySQL instances which are using keyring_vault plugin . Issue fixed PXB-1514 .","title":"New features and improvements"},{"location":"release-notes/2.4/2.4.12.html","text":"Percona XtraBackup 2.4.12 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.12 on June 22, 2018. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free. New features and improvements \u00b6 Percona XtraBackup now prints used arguments to standard output. Bug fixed PXB-1494 . Bugs fixed \u00b6 xtrabackup --copy-back didn\u2019t read which encryption plugin to use from plugin-load setting of the my.cnf configuration file. Bug fixed PXB-1544 . xbstream was exiting with zero return code when it failed to create one or more target files instead of returning error code 1. Bug fixed PXB-1542 . Meeting a zero sized keyring file, Percona XtraBackup was removing and immediately recreating it, which could affect external software noticing this file had undergo manipulations. Bug fixed PXB-1540 . xtrabackup_checkpoints files were encrypted during a backup, which caused additional difficulties to take incremental backups. Bug fixed PXB-202 . Other bugs fixed: PXB-1526 \u201cTest kill_long_selects.sh failing with MySQL 5.7.21\u201d.","title":"Percona XtraBackup 2.4.12"},{"location":"release-notes/2.4/2.4.12.html#percona-xtrabackup-2412","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.12 on June 22, 2018. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.12"},{"location":"release-notes/2.4/2.4.12.html#new-features-and-improvements","text":"Percona XtraBackup now prints used arguments to standard output. Bug fixed PXB-1494 .","title":"New features and improvements"},{"location":"release-notes/2.4/2.4.12.html#bugs-fixed","text":"xtrabackup --copy-back didn\u2019t read which encryption plugin to use from plugin-load setting of the my.cnf configuration file. Bug fixed PXB-1544 . xbstream was exiting with zero return code when it failed to create one or more target files instead of returning error code 1. Bug fixed PXB-1542 . Meeting a zero sized keyring file, Percona XtraBackup was removing and immediately recreating it, which could affect external software noticing this file had undergo manipulations. Bug fixed PXB-1540 . xtrabackup_checkpoints files were encrypted during a backup, which caused additional difficulties to take incremental backups. Bug fixed PXB-202 . Other bugs fixed: PXB-1526 \u201cTest kill_long_selects.sh failing with MySQL 5.7.21\u201d.","title":"Bugs fixed"},{"location":"release-notes/2.4/2.4.13.html","text":"Percona XtraBackup 2.4.13 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.13 on January 18, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free. Improvements and New Features \u00b6 PXB-1548 : Percona Xtrabackup enables updating the ib_buffer_pool file with the latest pages present in the buffer pool by setting the xtrabackup --dump-innodb-buffer-pool option to ON. Thanks to Marcelo Altmann for contribution. Bus Fixed \u00b6 xtrabackup did not delete missing tables from the partial backup which led to error messages logged by the server on startup. Bug fixed PXB-1536 . The --history option did not work when autocommit was disabled. Bug fixed PXB-1569 . xtrabackup could fail to backup encrypted tablespace when it was recently created or altered. Bug fixed PXB-1648 . When the --throttle option was used, the applied value was different from the one specified by the user (off by one error). Bug fixed PXB-1668 . It was not allowed for MTS (multi-threaded slaves) without GTID to be backed up with --safe-slave-backup . Bug fixed PXB-1672 . Percona Xtrabackup could crash when the ALTER TABLE \u2026 TRUNCATE PARTITION command was run during a backup without locking DDL. Bug fixed PXB-1679 . xbcrypt could display an assertion failure and generated core if the required parameters are missing. Bug fixed PXB-1683 . Using --lock-ddl-per-table caused the server to scan all records of partitioned tables which could lead to the \u201cout of memory\u201d error. Bugs fixed PXB-1691 and PXB-1698 . xtrabackup \u2013prepare could hang while performing insert buffer merge. Bug fixed PXB-1704 . Incremental backups did not update xtrabackup_binlog_info with --binlog-info=lockless . Bug fixed PXB-1711 . Other bugs fixed: PXB-1570 , PXB-1609 , PXB-1632","title":"Percona XtraBackup 2.4.13"},{"location":"release-notes/2.4/2.4.13.html#percona-xtrabackup-2413","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.13 on January 18, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.13"},{"location":"release-notes/2.4/2.4.13.html#improvements-and-new-features","text":"PXB-1548 : Percona Xtrabackup enables updating the ib_buffer_pool file with the latest pages present in the buffer pool by setting the xtrabackup --dump-innodb-buffer-pool option to ON. Thanks to Marcelo Altmann for contribution.","title":"Improvements and New Features"},{"location":"release-notes/2.4/2.4.13.html#bus-fixed","text":"xtrabackup did not delete missing tables from the partial backup which led to error messages logged by the server on startup. Bug fixed PXB-1536 . The --history option did not work when autocommit was disabled. Bug fixed PXB-1569 . xtrabackup could fail to backup encrypted tablespace when it was recently created or altered. Bug fixed PXB-1648 . When the --throttle option was used, the applied value was different from the one specified by the user (off by one error). Bug fixed PXB-1668 . It was not allowed for MTS (multi-threaded slaves) without GTID to be backed up with --safe-slave-backup . Bug fixed PXB-1672 . Percona Xtrabackup could crash when the ALTER TABLE \u2026 TRUNCATE PARTITION command was run during a backup without locking DDL. Bug fixed PXB-1679 . xbcrypt could display an assertion failure and generated core if the required parameters are missing. Bug fixed PXB-1683 . Using --lock-ddl-per-table caused the server to scan all records of partitioned tables which could lead to the \u201cout of memory\u201d error. Bugs fixed PXB-1691 and PXB-1698 . xtrabackup \u2013prepare could hang while performing insert buffer merge. Bug fixed PXB-1704 . Incremental backups did not update xtrabackup_binlog_info with --binlog-info=lockless . Bug fixed PXB-1711 . Other bugs fixed: PXB-1570 , PXB-1609 , PXB-1632","title":"Bus Fixed"},{"location":"release-notes/2.4/2.4.14.html","text":"Percona XtraBackup 2.4.14 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.14 on May 1, 2019. Downloads are available from our download site and from apt and yum repositories. Percona XtraBackup 2.4.14 enables saving backups to an Amazon S3 storage when using xbcloud . The following example demonstrates how to use an Amazon S3 storage to make a full backup. $ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = s3 \\ --s3-endpoint = 's3.amazonaws.com' \\ --s3-access-key = 'YOUR-ACCESSKEYID' \\ --s3-secret-key = 'YOUR-SECRETACCESSKEY' \\ --s3-bucket = 'mysql_backups' --parallel = 10 \\ ${ date -I } -full_backup All Percona software is open-source and free. New Features \u00b6 Amazon S3 is now supported in xbcloud . More information in PXB-1813 . Bugs Fixed \u00b6 When the row format was changed during the backup, xtrabackup could crash during the incremental prepare stage. Bug fixed PXB-1824 . If compressed InnoDB undo tablespaces were not removed beforehand, the incremental backup could crash at the prepare stage. Bug fixed PXB-1552 . Other bugs fixed : PXB-1771 , PXB-1809 , PXB-1837 .","title":"Percona XtraBackup 2.4.14"},{"location":"release-notes/2.4/2.4.14.html#percona-xtrabackup-2414","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.14 on May 1, 2019. Downloads are available from our download site and from apt and yum repositories. Percona XtraBackup 2.4.14 enables saving backups to an Amazon S3 storage when using xbcloud . The following example demonstrates how to use an Amazon S3 storage to make a full backup. $ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = s3 \\ --s3-endpoint = 's3.amazonaws.com' \\ --s3-access-key = 'YOUR-ACCESSKEYID' \\ --s3-secret-key = 'YOUR-SECRETACCESSKEY' \\ --s3-bucket = 'mysql_backups' --parallel = 10 \\ ${ date -I } -full_backup All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.14"},{"location":"release-notes/2.4/2.4.14.html#new-features","text":"Amazon S3 is now supported in xbcloud . More information in PXB-1813 .","title":"New Features"},{"location":"release-notes/2.4/2.4.14.html#bugs-fixed","text":"When the row format was changed during the backup, xtrabackup could crash during the incremental prepare stage. Bug fixed PXB-1824 . If compressed InnoDB undo tablespaces were not removed beforehand, the incremental backup could crash at the prepare stage. Bug fixed PXB-1552 . Other bugs fixed : PXB-1771 , PXB-1809 , PXB-1837 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.15.html","text":"Percona XtraBackup 2.4.15 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.15 on July 10, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free. Bugs Fixed \u00b6 When the encrypted tablespaces feature was enabled, encrypted and compressed tables were not usable on the joiner node (Percona XtraDB Cluster) via SST (State Snapshot Transfer) with the xtrabackup-v2 method. Bug fixed PXB-1867 . xbcloud did not update date related fields of the HTTP header when retrying a request. Bug fixed PXB-1874 . xbcloud did not retry to send the request after receiving the HTTP 408 error (request timeout). Bug fixed PXB-1875 . If the user tried to merge an already prepared incremental backup, a misleading error was produced without informing that incremental backups may not be used twice. Bug fixed PXB-1862 . xbcloud could crash with the Swift storage when project options were not included. Bug fixed PXB-1844 . xtrabackup did not accept decimal fractions as values of the innodb_max_dirty_pages_pct option. Bug fixed PXB-1807 . Other bugs fixed : PXB-1850 , PXB-1879 , PXB-1887 , PXB-1888 , PXB-1890 .","title":"Percona XtraBackup 2.4.15"},{"location":"release-notes/2.4/2.4.15.html#percona-xtrabackup-2415","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.15 on July 10, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.15"},{"location":"release-notes/2.4/2.4.15.html#bugs-fixed","text":"When the encrypted tablespaces feature was enabled, encrypted and compressed tables were not usable on the joiner node (Percona XtraDB Cluster) via SST (State Snapshot Transfer) with the xtrabackup-v2 method. Bug fixed PXB-1867 . xbcloud did not update date related fields of the HTTP header when retrying a request. Bug fixed PXB-1874 . xbcloud did not retry to send the request after receiving the HTTP 408 error (request timeout). Bug fixed PXB-1875 . If the user tried to merge an already prepared incremental backup, a misleading error was produced without informing that incremental backups may not be used twice. Bug fixed PXB-1862 . xbcloud could crash with the Swift storage when project options were not included. Bug fixed PXB-1844 . xtrabackup did not accept decimal fractions as values of the innodb_max_dirty_pages_pct option. Bug fixed PXB-1807 . Other bugs fixed : PXB-1850 , PXB-1879 , PXB-1887 , PXB-1888 , PXB-1890 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.16.html","text":"Percona XtraBackup 2.4.16 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.16 on November 4, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free. Improvements \u00b6 Two options ( --backup-lock-timeout and --backup-lock-retry-count ) were added to enable the configuring of the timeout for acquiring metadata locks in FLUSH TABLES WITH READ LOCK , LOCK TABLE FOR BACKUP , and LOCK BINLOG FOR BACKUP statements. More information in PXB-1914 Bugs Fixed \u00b6 Percona Xtrabackup was not able to connect to the database when the password was specified along with the transition-key parameter. Bug fixed PXB-1902 . In some cases, Percona Xtrabackup stuck with redo log corruption when master key is rotated. Bug fixed PXB-1903 . In rare cases, when both full and incremental backups were made before MySQL flushed the first page of the encrypted tablespace, Percona Xtrabackup could crash during the incremental backup prepare for the tablespace encryption. Bug fixed PXB-1894 . An encrypted table could not be restored when ADD/DROP INDEX was run on the table. Bug fixed PXB-1905 . In some cases xtrabackup --prepare could fail to decrypt a table but reported that the operation completed ok . Bug fixed PXB-1936 .","title":"Percona XtraBackup 2.4.16"},{"location":"release-notes/2.4/2.4.16.html#percona-xtrabackup-2416","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.16 on November 4, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.16"},{"location":"release-notes/2.4/2.4.16.html#improvements","text":"Two options ( --backup-lock-timeout and --backup-lock-retry-count ) were added to enable the configuring of the timeout for acquiring metadata locks in FLUSH TABLES WITH READ LOCK , LOCK TABLE FOR BACKUP , and LOCK BINLOG FOR BACKUP statements. More information in PXB-1914","title":"Improvements"},{"location":"release-notes/2.4/2.4.16.html#bugs-fixed","text":"Percona Xtrabackup was not able to connect to the database when the password was specified along with the transition-key parameter. Bug fixed PXB-1902 . In some cases, Percona Xtrabackup stuck with redo log corruption when master key is rotated. Bug fixed PXB-1903 . In rare cases, when both full and incremental backups were made before MySQL flushed the first page of the encrypted tablespace, Percona Xtrabackup could crash during the incremental backup prepare for the tablespace encryption. Bug fixed PXB-1894 . An encrypted table could not be restored when ADD/DROP INDEX was run on the table. Bug fixed PXB-1905 . In some cases xtrabackup --prepare could fail to decrypt a table but reported that the operation completed ok . Bug fixed PXB-1936 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.17.html","text":"Percona XtraBackup 2.4.17 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.17 on December 9, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free. Bugs Fixed \u00b6 Percona XtraBackup could crash when making a backup for Percona Server 5.7.28-31 where the tablespace encryption was used. Bug fixed PXB-1968 .","title":"Percona XtraBackup 2.4.17"},{"location":"release-notes/2.4/2.4.17.html#percona-xtrabackup-2417","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.17 on December 9, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.17"},{"location":"release-notes/2.4/2.4.17.html#bugs-fixed","text":"Percona XtraBackup could crash when making a backup for Percona Server 5.7.28-31 where the tablespace encryption was used. Bug fixed PXB-1968 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.18.html","text":"Percona XtraBackup 2.4.18 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.18 on December 16, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free. Bugs Fixed \u00b6 Sometime between December 3 rd and December 10 th , a change was introduced in AWS (Amazon Web Services) that caused an incompatibility with our Percona XtraBackup xbcloud utility. Bug fixed PXB-1978 .","title":"Percona XtraBackup 2.4.18"},{"location":"release-notes/2.4/2.4.18.html#percona-xtrabackup-2418","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.18 on December 16, 2019. Downloads are available from our download site and from apt and yum repositories. All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.18"},{"location":"release-notes/2.4/2.4.18.html#bugs-fixed","text":"Sometime between December 3 rd and December 10 th , a change was introduced in AWS (Amazon Web Services) that caused an incompatibility with our Percona XtraBackup xbcloud utility. Bug fixed PXB-1978 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.19.html","text":"Percona XtraBackup 2.4.19 \u00b6 Date March 25, 2020 Installation Installing Percona XtraBackup Downloads are available from our download site and from apt and yum repositories. Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups. All Percona software is open-source and free. Bugs Fixed \u00b6 PXB-1982 : The history table showed a wrong value for lock_time .","title":"Percona XtraBackup 2.4.19"},{"location":"release-notes/2.4/2.4.19.html#percona-xtrabackup-2419","text":"Date March 25, 2020 Installation Installing Percona XtraBackup Downloads are available from our download site and from apt and yum repositories. Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups. All Percona software is open-source and free.","title":"Percona XtraBackup 2.4.19"},{"location":"release-notes/2.4/2.4.19.html#bugs-fixed","text":"PXB-1982 : The history table showed a wrong value for lock_time .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.2.html","text":"Percona XtraBackup 2.4.2 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.2 on April 1 st 2016. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series. New features \u00b6 Percona XtraBackup has implemented support for InnoDB tablespace encryption . Percona XtraBackup has been rebased on MySQL 5.7.11. Bugs Fixed \u00b6 When backup was taken on MariaDB 10 with GTID enabled, Percona XtraBackup didn\u2019t store gtid_slave_pos in xtrabackup_slave_info but logged it only to STDERR . Bug fixed PXB-715 . Backup process would fail if xtrabackup --throttle option was used. Bug fixed PXB-465 .","title":"Percona XtraBackup 2.4.2"},{"location":"release-notes/2.4/2.4.2.html#percona-xtrabackup-242","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.2 on April 1 st 2016. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series.","title":"Percona XtraBackup 2.4.2"},{"location":"release-notes/2.4/2.4.2.html#new-features","text":"Percona XtraBackup has implemented support for InnoDB tablespace encryption . Percona XtraBackup has been rebased on MySQL 5.7.11.","title":"New features"},{"location":"release-notes/2.4/2.4.2.html#bugs-fixed","text":"When backup was taken on MariaDB 10 with GTID enabled, Percona XtraBackup didn\u2019t store gtid_slave_pos in xtrabackup_slave_info but logged it only to STDERR . Bug fixed PXB-715 . Backup process would fail if xtrabackup --throttle option was used. Bug fixed PXB-465 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.20.html","text":"Percona XtraBackup 2.4.20 \u00b6 Date April 14, 2020 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups. This release fixes security vulnerability CVE-2020-10997 Seealso Percona Database Performance Blog CVE-2020-10997 Bugs Fixed \u00b6 PXB-1783 : Xtrabackup GTID is incorrect after prepare PXB-2154 : Xbstream displayed the encrypt-key in process during backup decryption PXB-2152 : PXB wrote a new master key to standard error output PXB-2145 : encrypt-key could appear in the process-list PXB-2142 : Transition key was written to backup/stream","title":"Percona XtraBackup 2.4.20"},{"location":"release-notes/2.4/2.4.20.html#percona-xtrabackup-2420","text":"Date April 14, 2020 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups. This release fixes security vulnerability CVE-2020-10997 Seealso Percona Database Performance Blog CVE-2020-10997","title":"Percona XtraBackup 2.4.20"},{"location":"release-notes/2.4/2.4.20.html#bugs-fixed","text":"PXB-1783 : Xtrabackup GTID is incorrect after prepare PXB-2154 : Xbstream displayed the encrypt-key in process during backup decryption PXB-2152 : PXB wrote a new master key to standard error output PXB-2145 : encrypt-key could appear in the process-list PXB-2142 : Transition key was written to backup/stream","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.21.html","text":"Percona XtraBackup 2.4.21 \u00b6 Date November 12, 2020 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups. New Features \u00b6 PXB-2112 : xbcloud: support storage_class option with --storage=s3 (Thanks to user rluisr for reporting this issue) Improvements \u00b6 PXB-2254 : Redesign --lock-ddl-per-table PXB-2252 : Introduce debug option to print the redo log records scanned and applied Bugs Fixed \u00b6 PXB-793 : Fix syntax error when executing --lock-ddl-per-table queries PXB-2165 : Modify xbcloud to store backups using s3 access key parameters if AWS access key env variables are set PXB-2164 : Modify xbcloud to return the error when the backup doesn\u2019t exist in s3 bucket PXB-953 : Improve stdout for the end of usage of --lock-ddl-per-table PXB-2279 : Xbcloud: Upload failed: backup is incomplete (Thanks to user mrmainnet for reporting this issue) PXB-2127 : Modify xbcloud to upload backups with empty database to min.io storage (Thanks to user hartland for reporting this issue) PXB-2275 : Modify backup processing to add validations if an encrypted table is created PXB-2272 : Fixed Regexp from is_tmp_table doesn\u2019t account for all temporary tables PXB-2257 : fixed --lock-ddl-per-table to properly close database connection PXB-2249 : Verify perl binary exists before completing version check PXB-2239 : Partitioned table is not restored correctly when partitions are changed during backup PXB-2238 : Provide binary tarball with shared libs and glibc suffix & minimal tarballs PXB-2216 : Verify encryption version when opening tables to avoid changing encryption version PXB-2202 : Modify Xbcloud to display an error when xtrabackup fails to create a backup PXB-2198 : Modify xbcloud delete to return the error when the backup doesn\u2019t exist in s3 bucket","title":"Percona XtraBackup 2.4.21"},{"location":"release-notes/2.4/2.4.21.html#percona-xtrabackup-2421","text":"Date November 12, 2020 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups.","title":"Percona XtraBackup 2.4.21"},{"location":"release-notes/2.4/2.4.21.html#new-features","text":"PXB-2112 : xbcloud: support storage_class option with --storage=s3 (Thanks to user rluisr for reporting this issue)","title":"New Features"},{"location":"release-notes/2.4/2.4.21.html#improvements","text":"PXB-2254 : Redesign --lock-ddl-per-table PXB-2252 : Introduce debug option to print the redo log records scanned and applied","title":"Improvements"},{"location":"release-notes/2.4/2.4.21.html#bugs-fixed","text":"PXB-793 : Fix syntax error when executing --lock-ddl-per-table queries PXB-2165 : Modify xbcloud to store backups using s3 access key parameters if AWS access key env variables are set PXB-2164 : Modify xbcloud to return the error when the backup doesn\u2019t exist in s3 bucket PXB-953 : Improve stdout for the end of usage of --lock-ddl-per-table PXB-2279 : Xbcloud: Upload failed: backup is incomplete (Thanks to user mrmainnet for reporting this issue) PXB-2127 : Modify xbcloud to upload backups with empty database to min.io storage (Thanks to user hartland for reporting this issue) PXB-2275 : Modify backup processing to add validations if an encrypted table is created PXB-2272 : Fixed Regexp from is_tmp_table doesn\u2019t account for all temporary tables PXB-2257 : fixed --lock-ddl-per-table to properly close database connection PXB-2249 : Verify perl binary exists before completing version check PXB-2239 : Partitioned table is not restored correctly when partitions are changed during backup PXB-2238 : Provide binary tarball with shared libs and glibc suffix & minimal tarballs PXB-2216 : Verify encryption version when opening tables to avoid changing encryption version PXB-2202 : Modify Xbcloud to display an error when xtrabackup fails to create a backup PXB-2198 : Modify xbcloud delete to return the error when the backup doesn\u2019t exist in s3 bucket","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.22.html","text":"Percona XtraBackup 2.4.22 \u00b6 Date March 22, 2021 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups. This release fixes the security vulnerability CVE-2020-29488 Bugs Fixed \u00b6 PXB-2171 : Add missing PXB help options to the xtrabackup options reference PXB-2395 : Update versions for xbstream and xbcrypt PXB-2394 : Correct spellings in xbcloud help","title":"Percona XtraBackup 2.4.22"},{"location":"release-notes/2.4/2.4.22.html#percona-xtrabackup-2422","text":"Date March 22, 2021 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups. This release fixes the security vulnerability CVE-2020-29488","title":"Percona XtraBackup 2.4.22"},{"location":"release-notes/2.4/2.4.22.html#bugs-fixed","text":"PXB-2171 : Add missing PXB help options to the xtrabackup options reference PXB-2395 : Update versions for xbstream and xbcrypt PXB-2394 : Correct spellings in xbcloud help","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.23.html","text":"Percona XtraBackup 2.4.23 \u00b6 Date June 22, 2021 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups. Improvements \u00b6 PXB-2487 : Problems with the typesetting in the Partial Backups document (Thanks to user qing for reporting this issue). Bugs Fixed \u00b6 PXB-1462 : Long gtid_executed breaks --history functionality. PXB-2486 : When the --encrypt and --parallel parameters are used, XtraBackup does not handle a broken pipe correctly. PXB-1855 : Format correction for the `xtrabackup \u2013databasesv options. PXB-2427 : Update the XtraBackup Help description for the parameter --stream .","title":"Percona XtraBackup 2.4.23"},{"location":"release-notes/2.4/2.4.23.html#percona-xtrabackup-2423","text":"Date June 22, 2021 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups.","title":"Percona XtraBackup 2.4.23"},{"location":"release-notes/2.4/2.4.23.html#improvements","text":"PXB-2487 : Problems with the typesetting in the Partial Backups document (Thanks to user qing for reporting this issue).","title":"Improvements"},{"location":"release-notes/2.4/2.4.23.html#bugs-fixed","text":"PXB-1462 : Long gtid_executed breaks --history functionality. PXB-2486 : When the --encrypt and --parallel parameters are used, XtraBackup does not handle a broken pipe correctly. PXB-1855 : Format correction for the `xtrabackup \u2013databasesv options. PXB-2427 : Update the XtraBackup Help description for the parameter --stream .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.24.html","text":"Percona XtraBackup 2.4.24 \u00b6 Date September 14, 2021 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups. Improvements \u00b6 PXB-2477 : The xbcloud Binary should retry on error and utilize incremental backoff (Thanks to Baptiste Mille-Mathias for reporting this issue) PXB-2580 : With the xbcloud binary, a chunk-upload on SSL connect error to Amazon S3 was not retried. (Thanks to Tim Vaillancourt for providing the patch) Bugs Fixed \u00b6 PXB-1504 : The FIND_GCRYPT macro is broken. (Thanks to Maxim Bublis for reporting this issue)","title":"Percona XtraBackup 2.4.24"},{"location":"release-notes/2.4/2.4.24.html#percona-xtrabackup-2424","text":"Date September 14, 2021 Installation Installing Percona XtraBackup Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups.","title":"Percona XtraBackup 2.4.24"},{"location":"release-notes/2.4/2.4.24.html#improvements","text":"PXB-2477 : The xbcloud Binary should retry on error and utilize incremental backoff (Thanks to Baptiste Mille-Mathias for reporting this issue) PXB-2580 : With the xbcloud binary, a chunk-upload on SSL connect error to Amazon S3 was not retried. (Thanks to Tim Vaillancourt for providing the patch)","title":"Improvements"},{"location":"release-notes/2.4/2.4.24.html#bugs-fixed","text":"PXB-1504 : The FIND_GCRYPT macro is broken. (Thanks to Maxim Bublis for reporting this issue)","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.25.html","text":"Percona XtraBackup 2.4.25 \u00b6 Date April 26, 2022 Percona XtraBackup for MySQL Databases enables MySQL backups without blocking user queries. Percona XtraBackup is ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, Percona XtraBackup drives down backup costs while providing unique features for MySQL backups. Percona XtraBackup 2.4 does not support making backups of databases created in MySQL 8.0 , Percona Server for MySQL 8.0 , or Percona XtraDB Cluster 8.0 . Use Percona XtraBackup 8.0 to make backups for these versions. Release Highlights \u00b6 The xbcloud binary adds support for the Microsoft Azure Cloud Storage using the REST API. New Features \u00b6 PXB-1883 : Implements support for Microsoft Azure Cloud Storage in the xbcloud binary. (Thanks to Ivan Groenewold for reporting this issue) Bugs Fixed \u00b6 PXB-2608 : Upgraded the Vault API to V2 (Thanks to Benedito Marques Magalhaes for reporting this issue) PXB-2649 : Fix for compilation issues on GCC-10. PXB-2648 : CURL prior to 7.38.0 version doesn\u2019t use CURLE_HTTP2 and throws an error 'CURLE_HTTP2' is not a member of 'CURLcode' . Added CURLE_OBSOLETE16 as a connectivity error code. In CURL versions after 7.38.0, CURLE_OBSOLETE16 is translated to CURLE_HTTP2. PXB-2711 : Fix for libgcrypt initialization warnings in xtrabackup. PXB-2722 : Fix for when via command line, a password, passed using the -p option, was written into the backup tool_command in xtrabackup_info. Useful Links \u00b6 The Percona XtraBackup installation instructions The Percona XtraBackup downloads The Percona XtraBackup GitHub location To contribute to the documentation, review the Documentation Contribution Guide","title":"Percona XtraBackup 2.4.25"},{"location":"release-notes/2.4/2.4.25.html#percona-xtrabackup-2425","text":"Date April 26, 2022 Percona XtraBackup for MySQL Databases enables MySQL backups without blocking user queries. Percona XtraBackup is ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, Percona XtraBackup drives down backup costs while providing unique features for MySQL backups. Percona XtraBackup 2.4 does not support making backups of databases created in MySQL 8.0 , Percona Server for MySQL 8.0 , or Percona XtraDB Cluster 8.0 . Use Percona XtraBackup 8.0 to make backups for these versions.","title":"Percona XtraBackup 2.4.25"},{"location":"release-notes/2.4/2.4.25.html#release-highlights","text":"The xbcloud binary adds support for the Microsoft Azure Cloud Storage using the REST API.","title":"Release Highlights"},{"location":"release-notes/2.4/2.4.25.html#new-features","text":"PXB-1883 : Implements support for Microsoft Azure Cloud Storage in the xbcloud binary. (Thanks to Ivan Groenewold for reporting this issue)","title":"New Features"},{"location":"release-notes/2.4/2.4.25.html#bugs-fixed","text":"PXB-2608 : Upgraded the Vault API to V2 (Thanks to Benedito Marques Magalhaes for reporting this issue) PXB-2649 : Fix for compilation issues on GCC-10. PXB-2648 : CURL prior to 7.38.0 version doesn\u2019t use CURLE_HTTP2 and throws an error 'CURLE_HTTP2' is not a member of 'CURLcode' . Added CURLE_OBSOLETE16 as a connectivity error code. In CURL versions after 7.38.0, CURLE_OBSOLETE16 is translated to CURLE_HTTP2. PXB-2711 : Fix for libgcrypt initialization warnings in xtrabackup. PXB-2722 : Fix for when via command line, a password, passed using the -p option, was written into the backup tool_command in xtrabackup_info.","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.25.html#useful-links","text":"The Percona XtraBackup installation instructions The Percona XtraBackup downloads The Percona XtraBackup GitHub location To contribute to the documentation, review the Documentation Contribution Guide","title":"Useful Links"},{"location":"release-notes/2.4/2.4.26.html","text":"Percona XtraBackup 2.4.26 \u00b6 Date May 9, 2022 Percona XtraBackup for MySQL Databases enables MySQL backups without blocking user queries. Percona XtraBackup is ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, Percona XtraBackup drives down backup costs while providing unique features for MySQL backups. Percona XtraBackup 2.4 does not support making backups of databases created in MySQL 8.0 , Percona Server for MySQL 8.0 , or Percona XtraDB Cluster 8.0 . Use Percona XtraBackup 8.0 to make backups for these versions. Release Highlights \u00b6 Fixed a segmentation fault when creating a tmpdir. Bugs Fixed \u00b6 PXB-2756 : Fixed a segmentation fault when creating a tmpdir. Useful Links \u00b6 The Percona XtraBackup installation instructions The Percona XtraBackup downloads The Percona XtraBackup GitHub location To contribute to the documentation, review the Documentation Contribution Guide","title":"Percona XtraBackup 2.4.26"},{"location":"release-notes/2.4/2.4.26.html#percona-xtrabackup-2426","text":"Date May 9, 2022 Percona XtraBackup for MySQL Databases enables MySQL backups without blocking user queries. Percona XtraBackup is ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, Percona XtraBackup drives down backup costs while providing unique features for MySQL backups. Percona XtraBackup 2.4 does not support making backups of databases created in MySQL 8.0 , Percona Server for MySQL 8.0 , or Percona XtraDB Cluster 8.0 . Use Percona XtraBackup 8.0 to make backups for these versions.","title":"Percona XtraBackup 2.4.26"},{"location":"release-notes/2.4/2.4.26.html#release-highlights","text":"Fixed a segmentation fault when creating a tmpdir.","title":"Release Highlights"},{"location":"release-notes/2.4/2.4.26.html#bugs-fixed","text":"PXB-2756 : Fixed a segmentation fault when creating a tmpdir.","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.26.html#useful-links","text":"The Percona XtraBackup installation instructions The Percona XtraBackup downloads The Percona XtraBackup GitHub location To contribute to the documentation, review the Documentation Contribution Guide","title":"Useful Links"},{"location":"release-notes/2.4/2.4.3.html","text":"Percona XtraBackup 2.4.3 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.3 on May 23 rd 2016. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series. New features \u00b6 Percona XtraBackup has implemented new xtrabackup --reencrypt-for-server-id option. Using this option allows users to start the server instance with different server_id from the one the encrypted backup was taken from, like a replication slave or a galera node. When this option is used, xtrabackup will, as a prepare step, generate a new master key with ID based on the new server_id , store it into keyring file and re-encrypt the tablespace keys inside of tablespace headers. Bugs Fixed \u00b6 Running DDL statements on Percona Server for MySQL 5.7 during the backup process could in some cases lead to failure while preparing the backup. Bug fixed PXB-247 . MySQL 5.7 can sometimes skip redo logging when creating an index. If such ALTER TABLE is being issued during the backup, the backup would be inconsistent. xtrabackup will now abort with error message if such ALTER TABLE has been done during the backup. Bug fixed PXB-249 . .ibd files for remote tablespaces were not copied back to original location pointed by the .isl files. Bug fixed PXB-466 . When called with insufficient parameters, like specifying the empty xtrabackup --defaults-file option, Percona XtraBackup could crash. Bug fixed PXB-471 . Documentation states that the default value for xtrabackup --ftwrl-wait-query-type is all , however it was update . Changed the default value to reflect the documentation. Bug fixed PXB-472 . When xtrabackup --keyring-file-data option was specified, but no keyring file was found, xtrabackup would create an empty one instead of reporting an error. Bug fixed PXB-476 . If ALTER INSTANCE ROTATE INNODB MASTER KEY was run at same time when xtrabackup --backup was bootstrapping it could catch a moment when the key was not written into the keyring file yet and xtrabackup would overwrite the keyring with the old copy of a keyring, so the new key would be lost. Bug fixed PXB-478 . Output of xtrabackup --slave-info option was missing an apostrophe. Bug fixed PXB-940 .","title":"Percona XtraBackup 2.4.3"},{"location":"release-notes/2.4/2.4.3.html#percona-xtrabackup-243","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.3 on May 23 rd 2016. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series.","title":"Percona XtraBackup 2.4.3"},{"location":"release-notes/2.4/2.4.3.html#new-features","text":"Percona XtraBackup has implemented new xtrabackup --reencrypt-for-server-id option. Using this option allows users to start the server instance with different server_id from the one the encrypted backup was taken from, like a replication slave or a galera node. When this option is used, xtrabackup will, as a prepare step, generate a new master key with ID based on the new server_id , store it into keyring file and re-encrypt the tablespace keys inside of tablespace headers.","title":"New features"},{"location":"release-notes/2.4/2.4.3.html#bugs-fixed","text":"Running DDL statements on Percona Server for MySQL 5.7 during the backup process could in some cases lead to failure while preparing the backup. Bug fixed PXB-247 . MySQL 5.7 can sometimes skip redo logging when creating an index. If such ALTER TABLE is being issued during the backup, the backup would be inconsistent. xtrabackup will now abort with error message if such ALTER TABLE has been done during the backup. Bug fixed PXB-249 . .ibd files for remote tablespaces were not copied back to original location pointed by the .isl files. Bug fixed PXB-466 . When called with insufficient parameters, like specifying the empty xtrabackup --defaults-file option, Percona XtraBackup could crash. Bug fixed PXB-471 . Documentation states that the default value for xtrabackup --ftwrl-wait-query-type is all , however it was update . Changed the default value to reflect the documentation. Bug fixed PXB-472 . When xtrabackup --keyring-file-data option was specified, but no keyring file was found, xtrabackup would create an empty one instead of reporting an error. Bug fixed PXB-476 . If ALTER INSTANCE ROTATE INNODB MASTER KEY was run at same time when xtrabackup --backup was bootstrapping it could catch a moment when the key was not written into the keyring file yet and xtrabackup would overwrite the keyring with the old copy of a keyring, so the new key would be lost. Bug fixed PXB-478 . Output of xtrabackup --slave-info option was missing an apostrophe. Bug fixed PXB-940 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.4.html","text":"Percona XtraBackup 2.4.4 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.4 on July 25 th 2016. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series. New features \u00b6 Percona XtraBackup has been rebased on MySQL 5.7.13. Bugs Fixed \u00b6 Percona XtraBackup reported the difference in the actual size of the system tablespace and the size which was stored in the tablespace header. This check is now skipped for tablespaces with autoextend support. Bug fixed PXB-462 . Because Percona Server for MySQL 5.5 and MySQL 5.6 store the LSN offset for large log files at different places inside the redo log header, Percona Xtrabackup was trying to guess which offset is better to use by trying to read from each one and compare the log block numbers and assert lsn_chosen == 1 when both LSNs looked correct, but they were different. Fixed by improving the server detection. Bug fixed PXB-473 . Percona XtraBackup didn\u2019t correctly detect when tables were both compressed and encrypted. Bug fixed PXB-477 . Percona XtraBackup would crash if the keyring file was empty. Bug fixed PXB-479 . Backup couldn\u2019t be prepared when the size in cache didn\u2019t match the physical size. Bug fixed PXB-482 . Free Software Foundation address in copyright notices was outdated. Bug fixed PXB-663 . Backup process would fail if the datadir specified on the command-line was not the same as one that is reported by the server. Percona XtraBackup now allows the datadir from my.cnf override the one from SHOW VARIABLES . xtrabackup will print a warning that they don\u2019t match, but continue. Bug fixed PXB-741 . With upstream change of maximum page size from 16K to 64K, the size of incremental buffer became 1G. Which increased the requirement to 1G of RAM in order to prepare the backup. While in fact there is no need to allocate such a large buffer for smaller pages. Bug fixed PXB-753 . Backup process would fail on MariaDB Galera cluster operating in GTID mode if binary logs were in non-standard directory. Bug fixed PXB-936 . Other bugs fixed: PXB-755 , PXB-756 , and PXB-759 .","title":"Percona XtraBackup 2.4.4"},{"location":"release-notes/2.4/2.4.4.html#percona-xtrabackup-244","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.4 on July 25 th 2016. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series.","title":"Percona XtraBackup 2.4.4"},{"location":"release-notes/2.4/2.4.4.html#new-features","text":"Percona XtraBackup has been rebased on MySQL 5.7.13.","title":"New features"},{"location":"release-notes/2.4/2.4.4.html#bugs-fixed","text":"Percona XtraBackup reported the difference in the actual size of the system tablespace and the size which was stored in the tablespace header. This check is now skipped for tablespaces with autoextend support. Bug fixed PXB-462 . Because Percona Server for MySQL 5.5 and MySQL 5.6 store the LSN offset for large log files at different places inside the redo log header, Percona Xtrabackup was trying to guess which offset is better to use by trying to read from each one and compare the log block numbers and assert lsn_chosen == 1 when both LSNs looked correct, but they were different. Fixed by improving the server detection. Bug fixed PXB-473 . Percona XtraBackup didn\u2019t correctly detect when tables were both compressed and encrypted. Bug fixed PXB-477 . Percona XtraBackup would crash if the keyring file was empty. Bug fixed PXB-479 . Backup couldn\u2019t be prepared when the size in cache didn\u2019t match the physical size. Bug fixed PXB-482 . Free Software Foundation address in copyright notices was outdated. Bug fixed PXB-663 . Backup process would fail if the datadir specified on the command-line was not the same as one that is reported by the server. Percona XtraBackup now allows the datadir from my.cnf override the one from SHOW VARIABLES . xtrabackup will print a warning that they don\u2019t match, but continue. Bug fixed PXB-741 . With upstream change of maximum page size from 16K to 64K, the size of incremental buffer became 1G. Which increased the requirement to 1G of RAM in order to prepare the backup. While in fact there is no need to allocate such a large buffer for smaller pages. Bug fixed PXB-753 . Backup process would fail on MariaDB Galera cluster operating in GTID mode if binary logs were in non-standard directory. Bug fixed PXB-936 . Other bugs fixed: PXB-755 , PXB-756 , and PXB-759 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.5.html","text":"Percona XtraBackup 2.4.5 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.5 on November 29 th 2016. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series. New features \u00b6 Percona XtraBackup now supports SHA256 passwords. Using the SHA256 algorithm requires either SSL encrypted connection, or using public key encryption for password exchange which is only available when both client and server are linked with OpenSSL. Percona XtraBackup now supports Command Options for Secure Connections . NOTE: Due to xbcrypt format changes, backups encrypted with this Percona XtraBackup version will not be recoverable by older versions. Bugs Fixed \u00b6 Percona XtraBackup would crash while preparing the backup, during the shutdown, when master thread was performing checkpoint and purge thread was expecting that all other threads completed or were idle. Bug fixed PXB-483 . Safe slave backup algorithm performed too short delays between retries which could cause backups to fail on a busy servers. Bug fixed PXB-484 . Percona XtraBackup didn\u2019t check the logblock checksums. Bug fixed PXB-485 . Fixed new compilation warnings with GCC 6. Bug fixed PXB-487 . xbcrypt was not setting the Initialization Vector (IV) correctly (and thus is was not using an IV). This was causing the same ciphertext to be generated across different runs (for the same message/same key). The IV provides the extra randomness to ensure that the same ciphertext is not generated across runs. Bug fixed PXB-490 . target-dir was no longer relative to current directory but to datadir instead. Bug fixed PXB-760 . Backup would still succeed even if xtrabackup would fail to write the metadata. Bug fixed PXB-763 . xbcloud now supports EMC ECS Swift API Authorization requests. Bugs fixed PXB-769 and PXB-770 ( Txomin Barturen ). Some older versions of MySQL did not bother to initialize page type field for pages which are not index pages (see upstream #76262 for more information). Having this page type uninitialized could cause xtrabackup to crash on prepare. Bug fixed PXB-772 . Percona XtraBackup would fail to backup MariaDB 10.2 with the unsupported server version error message. Bug fixed PXB-1027 . Fixed misleading error message about missing metadata. Bug fixed PXB-752 . Backing up with an SSL user didn\u2019t work correctly. Bug fixed PXB-750 . Other bugs fixed: PXB-486 , PXB-771 , PXB-773 , and PXB-774 .","title":"Percona XtraBackup 2.4.5"},{"location":"release-notes/2.4/2.4.5.html#percona-xtrabackup-245","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.5 on November 29 th 2016. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series.","title":"Percona XtraBackup 2.4.5"},{"location":"release-notes/2.4/2.4.5.html#new-features","text":"Percona XtraBackup now supports SHA256 passwords. Using the SHA256 algorithm requires either SSL encrypted connection, or using public key encryption for password exchange which is only available when both client and server are linked with OpenSSL. Percona XtraBackup now supports Command Options for Secure Connections . NOTE: Due to xbcrypt format changes, backups encrypted with this Percona XtraBackup version will not be recoverable by older versions.","title":"New features"},{"location":"release-notes/2.4/2.4.5.html#bugs-fixed","text":"Percona XtraBackup would crash while preparing the backup, during the shutdown, when master thread was performing checkpoint and purge thread was expecting that all other threads completed or were idle. Bug fixed PXB-483 . Safe slave backup algorithm performed too short delays between retries which could cause backups to fail on a busy servers. Bug fixed PXB-484 . Percona XtraBackup didn\u2019t check the logblock checksums. Bug fixed PXB-485 . Fixed new compilation warnings with GCC 6. Bug fixed PXB-487 . xbcrypt was not setting the Initialization Vector (IV) correctly (and thus is was not using an IV). This was causing the same ciphertext to be generated across different runs (for the same message/same key). The IV provides the extra randomness to ensure that the same ciphertext is not generated across runs. Bug fixed PXB-490 . target-dir was no longer relative to current directory but to datadir instead. Bug fixed PXB-760 . Backup would still succeed even if xtrabackup would fail to write the metadata. Bug fixed PXB-763 . xbcloud now supports EMC ECS Swift API Authorization requests. Bugs fixed PXB-769 and PXB-770 ( Txomin Barturen ). Some older versions of MySQL did not bother to initialize page type field for pages which are not index pages (see upstream #76262 for more information). Having this page type uninitialized could cause xtrabackup to crash on prepare. Bug fixed PXB-772 . Percona XtraBackup would fail to backup MariaDB 10.2 with the unsupported server version error message. Bug fixed PXB-1027 . Fixed misleading error message about missing metadata. Bug fixed PXB-752 . Backing up with an SSL user didn\u2019t work correctly. Bug fixed PXB-750 . Other bugs fixed: PXB-486 , PXB-771 , PXB-773 , and PXB-774 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.6.html","text":"Percona XtraBackup 2.4.6 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.6 on February 22 nd 2017. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series. New features \u00b6 Percona XtraBackup has implemented new xtrabackup --remove-original option that can be used to remove the encrypted and compressed files once they\u2019ve been decrypted/decompressed. Bugs Fixed \u00b6 xtrabackup was using username set for server in a configuration file even if a different user was defined in the users configuration file. Bug fixed PXB-463 . Incremental backups did not include xtrabackup_binlog_info and xtrabackup_galera_info files. Bug fixed PXB-489 . In case a warning was written to stout instead of stderr during the streaming backup, it could cause assertion in the xbstream. Bug fixed PXB-491 . xtrabackup --move-back did not always restore out-of-datadir tablespaces to their original directories. Bug fixed PXB-492 . innobackupex and xtrabackup scripts were showing the password in the ps output when it was passed as a command line argument. Bug fixed PXB-585 Incremental backup would fail with path like ~/backup/inc_1 because xtrabackup didn\u2019t properly expand tilde. Bug fixed PXB-775 . Fixed missing dependency check for perl(Digest::MD5) in rpm packages. Bug fixed PXB-777 . Percona XtraBackup now supports -H , -h , -u and -p shortcuts for --hostname , --datadir , --user and --password respectively. Bugs fixed PXB-947 and PXB-1032 . [UPDATE 2016-02-28]: New packages have been pushed to repositories with incremented package version to address the bug PXB-497 . Other bugs fixed: PXB-945 .","title":"Percona XtraBackup 2.4.6"},{"location":"release-notes/2.4/2.4.6.html#percona-xtrabackup-246","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.6 on February 22 nd 2017. Downloads are available from our download site and from apt and yum repositories. This release is the GA (Generally Available) stable release in the 2.4 series.","title":"Percona XtraBackup 2.4.6"},{"location":"release-notes/2.4/2.4.6.html#new-features","text":"Percona XtraBackup has implemented new xtrabackup --remove-original option that can be used to remove the encrypted and compressed files once they\u2019ve been decrypted/decompressed.","title":"New features"},{"location":"release-notes/2.4/2.4.6.html#bugs-fixed","text":"xtrabackup was using username set for server in a configuration file even if a different user was defined in the users configuration file. Bug fixed PXB-463 . Incremental backups did not include xtrabackup_binlog_info and xtrabackup_galera_info files. Bug fixed PXB-489 . In case a warning was written to stout instead of stderr during the streaming backup, it could cause assertion in the xbstream. Bug fixed PXB-491 . xtrabackup --move-back did not always restore out-of-datadir tablespaces to their original directories. Bug fixed PXB-492 . innobackupex and xtrabackup scripts were showing the password in the ps output when it was passed as a command line argument. Bug fixed PXB-585 Incremental backup would fail with path like ~/backup/inc_1 because xtrabackup didn\u2019t properly expand tilde. Bug fixed PXB-775 . Fixed missing dependency check for perl(Digest::MD5) in rpm packages. Bug fixed PXB-777 . Percona XtraBackup now supports -H , -h , -u and -p shortcuts for --hostname , --datadir , --user and --password respectively. Bugs fixed PXB-947 and PXB-1032 . [UPDATE 2016-02-28]: New packages have been pushed to repositories with incremented package version to address the bug PXB-497 . Other bugs fixed: PXB-945 .","title":"Bugs Fixed"},{"location":"release-notes/2.4/2.4.7-2.html","text":"Percona XtraBackup 2.4.7-2 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.7-2 on May 29 th 2017. Downloads are available from our download site and from apt and yum repositories. This release is the current GA (Generally Available) stable release in the 2.4 series. Bugs fixed \u00b6 Fixed build failure on Debian 9.0 ( Stretch ). Bug fixed PXB-501 .","title":"Percona XtraBackup 2.4.7-2"},{"location":"release-notes/2.4/2.4.7-2.html#percona-xtrabackup-247-2","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.7-2 on May 29 th 2017. Downloads are available from our download site and from apt and yum repositories. This release is the current GA (Generally Available) stable release in the 2.4 series.","title":"Percona XtraBackup 2.4.7-2"},{"location":"release-notes/2.4/2.4.7-2.html#bugs-fixed","text":"Fixed build failure on Debian 9.0 ( Stretch ). Bug fixed PXB-501 .","title":"Bugs fixed"},{"location":"release-notes/2.4/2.4.7.html","text":"Percona XtraBackup 2.4.7 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.7 on April 17 th 2017. Downloads are available from our download site and from apt and yum repositories. This release is the current GA (Generally Available) stable release in the 2.4 series. New Features \u00b6 Percona XtraBackup now uses hardware accelerated implementation of crc32 where it is supported. Percona XtraBackup has implemented new options: xtrabackup --tables-exclude and xtrabackup --databases-exclude that work similar to xtrabackup --tables and xtrabackup --databases options, but exclude given names/paths from backup. The xbstream binary now supports parallel extraction with the --parallel option. The xbstream binary now supports following new options: --decrypt , --encrypt-threads , --encrypt-key , and --encrypt-key-file . When --decrypt option is specified xbstream will automatically decrypt encrypted files when extracting input stream. Either --encrypt-key or --encrypt-key-file options must be specified to provide encryption key, but not both. Option --encrypt-threads specifies the number of worker threads doing the encryption, default is 1 . Bugs fixed \u00b6 Backups were missing *.isl files for general tablespace. Bug fixed PXB-494 . In 5.7 MySQL changed default checksum algorithm to crc32 , while xtrabackup was using innodb . This caused xtrabackup to perform extra checksum calculations which were not needed. Bug fixed PXB-495 . For system tablespaces consisting of multiple files xtrabackup updated LSN only in first file. This caused MySQL versions lower than 5.7 to fail on startup. Bug fixed PXB-498 . xtrabackup --export can now export tables that have more than 31 index. Bug fixed PXB-58 . Unrecognized character \\x01; marked by <-- HERE message could be seen if backups were taken with the version check enabled. Bug fixed PXB-944 .","title":"Percona XtraBackup 2.4.7"},{"location":"release-notes/2.4/2.4.7.html#percona-xtrabackup-247","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.7 on April 17 th 2017. Downloads are available from our download site and from apt and yum repositories. This release is the current GA (Generally Available) stable release in the 2.4 series.","title":"Percona XtraBackup 2.4.7"},{"location":"release-notes/2.4/2.4.7.html#new-features","text":"Percona XtraBackup now uses hardware accelerated implementation of crc32 where it is supported. Percona XtraBackup has implemented new options: xtrabackup --tables-exclude and xtrabackup --databases-exclude that work similar to xtrabackup --tables and xtrabackup --databases options, but exclude given names/paths from backup. The xbstream binary now supports parallel extraction with the --parallel option. The xbstream binary now supports following new options: --decrypt , --encrypt-threads , --encrypt-key , and --encrypt-key-file . When --decrypt option is specified xbstream will automatically decrypt encrypted files when extracting input stream. Either --encrypt-key or --encrypt-key-file options must be specified to provide encryption key, but not both. Option --encrypt-threads specifies the number of worker threads doing the encryption, default is 1 .","title":"New Features"},{"location":"release-notes/2.4/2.4.7.html#bugs-fixed","text":"Backups were missing *.isl files for general tablespace. Bug fixed PXB-494 . In 5.7 MySQL changed default checksum algorithm to crc32 , while xtrabackup was using innodb . This caused xtrabackup to perform extra checksum calculations which were not needed. Bug fixed PXB-495 . For system tablespaces consisting of multiple files xtrabackup updated LSN only in first file. This caused MySQL versions lower than 5.7 to fail on startup. Bug fixed PXB-498 . xtrabackup --export can now export tables that have more than 31 index. Bug fixed PXB-58 . Unrecognized character \\x01; marked by <-- HERE message could be seen if backups were taken with the version check enabled. Bug fixed PXB-944 .","title":"Bugs fixed"},{"location":"release-notes/2.4/2.4.8.html","text":"Percona XtraBackup 2.4.8 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.8 on July 24 th 2017. Downloads are available from our download site and from apt and yum repositories. This release is the current GA (Generally Available) stable release in the 2.4 series. New Features \u00b6 To avoid issues with MySQL 5.7 skipping redo log for DDL Percona XtraBackup has implemented three new options ( xtrabackup --lock-ddl , xtrabackup --lock-ddl-timeout , xtrabackup --lock-ddl-per-table ) that can be used to place MDL locks on tables while they are copied. New xtrabackup --check-privileges option has been implemented that can be used to check if Percona XtraBackup has all required privileges to perform the backup. Bugs fixed \u00b6 xtrabackup would hang with Waiting for master thread to be suspended message when backup was being prepared. Bug fixed PXB-499 . xtrabackup would fail to prepare the backup with 6th page is not initialized message in case server didn\u2019t properly initialize the page. Bug fixed PXB-500 . xbstream could run out of file descriptors while extracting the backup which contains many tables. Bug fixed PXB-503 When a table was created with the DATA DIRECTORY option xtrabackup would back up the .frm and .isl files, but not the .ibd file. Due to the missing .ibd files backup then could not be restored. Bug fixed PXB-504 . Percona XtraBackup incorrectly determined use of master_auto_postion on a slave, and thus generated invalid xtrabackup_slave_info file. Bug fixed PXB-505 . Percona XtraBackup will now print a warning if it encounters unsupported storage engine. Bug fixed PXB-713 . Percona XtraBackup would crash while backing up MariaDB 10.2.x with --ftwrl-\\* options. Bug fixed PXB-790 . xtrabackup --slave-info didn\u2019t write the correct information into xtrabackup_slave_info file when multi-source replication was used. Bug fixed PXB-1022 . Along with xtrabackup_checkpints file, xtrabackup now copies xtrabackup_info file into directory specified by xtrabackup --extra-lsndir option. Bug fixed PXB-1026 . GTID position was not recorded when xtrabackup \u2013binlog-info option was set to AUTO . Bug fixed PXB-1030 .","title":"Percona XtraBackup 2.4.8"},{"location":"release-notes/2.4/2.4.8.html#percona-xtrabackup-248","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.8 on July 24 th 2017. Downloads are available from our download site and from apt and yum repositories. This release is the current GA (Generally Available) stable release in the 2.4 series.","title":"Percona XtraBackup 2.4.8"},{"location":"release-notes/2.4/2.4.8.html#new-features","text":"To avoid issues with MySQL 5.7 skipping redo log for DDL Percona XtraBackup has implemented three new options ( xtrabackup --lock-ddl , xtrabackup --lock-ddl-timeout , xtrabackup --lock-ddl-per-table ) that can be used to place MDL locks on tables while they are copied. New xtrabackup --check-privileges option has been implemented that can be used to check if Percona XtraBackup has all required privileges to perform the backup.","title":"New Features"},{"location":"release-notes/2.4/2.4.8.html#bugs-fixed","text":"xtrabackup would hang with Waiting for master thread to be suspended message when backup was being prepared. Bug fixed PXB-499 . xtrabackup would fail to prepare the backup with 6th page is not initialized message in case server didn\u2019t properly initialize the page. Bug fixed PXB-500 . xbstream could run out of file descriptors while extracting the backup which contains many tables. Bug fixed PXB-503 When a table was created with the DATA DIRECTORY option xtrabackup would back up the .frm and .isl files, but not the .ibd file. Due to the missing .ibd files backup then could not be restored. Bug fixed PXB-504 . Percona XtraBackup incorrectly determined use of master_auto_postion on a slave, and thus generated invalid xtrabackup_slave_info file. Bug fixed PXB-505 . Percona XtraBackup will now print a warning if it encounters unsupported storage engine. Bug fixed PXB-713 . Percona XtraBackup would crash while backing up MariaDB 10.2.x with --ftwrl-\\* options. Bug fixed PXB-790 . xtrabackup --slave-info didn\u2019t write the correct information into xtrabackup_slave_info file when multi-source replication was used. Bug fixed PXB-1022 . Along with xtrabackup_checkpints file, xtrabackup now copies xtrabackup_info file into directory specified by xtrabackup --extra-lsndir option. Bug fixed PXB-1026 . GTID position was not recorded when xtrabackup \u2013binlog-info option was set to AUTO . Bug fixed PXB-1030 .","title":"Bugs fixed"},{"location":"release-notes/2.4/2.4.9.html","text":"Percona XtraBackup 2.4.9 \u00b6 Percona is glad to announce the release of Percona XtraBackup 2.4.9 on November 29 th 2017. Downloads are available from our download site and from apt and yum repositories. This release is the current GA (Generally Available) stable release in the 2.4 series. New Features \u00b6 Percona XtraBackup packages are now available for Ubuntu 17.10 (Artful) . xbcrypt now has an ability to decrypt files in parallel by specifying the number of threads with the xtrabackup --encrypt-threads option. xtrabackup --copy-back option can now be used with xtrabackup --parallel option to copy the user data files in parallel (redo logs and system tablespaces are copied in the main thread). Bugs fixed \u00b6 Percona XtraBackup would fail to backup large databases on 32-bit platforms. Bug fixed PXB-481 . Percona XtraBackup failed to build with GCC 7 . Bug fixed PXB-502 . Percona XtraBackup would hang during the prepare phase if there was not enough room in log buffer to accommodate checkpoint information at the end of the crash recovery process. Bug fixed PXB-506 . When backup was streamed in tar format with with the xtrabackup --slave-info option output file xtrabackup_slave_info did not contain the slave information. Bug fixed PXB-507 . If xtrabackup \u2013slave-info was used while backing up 5.7 instances, the master binary log coordinates were not properly displayed in the logs. Bug fixed PXB-508 . innobackupex --slave-info would report a single m instead of slave info in the standard output. Bug fixed PXB-510 . Percona XtraBackup would crash while preparing the 5.5 backup with utf8_general50_ci collation. Bug fixed PXB-748 ( Fungo Wang ). Percona XtraBackup would crash if xtrabackup --throttle was used while preparing backups. Fixed by making this option available only during the backup process. Bug fixed PXB-789 . Percona XtraBackup could get stuck if backups are taken with xtrabackup --safe-slave-backup option, while there were long running queries. Bug fixed PXB-1039 . Other bugs fixed: PXB-250 , PXB-511 , and PXB-512 .","title":"Percona XtraBackup 2.4.9"},{"location":"release-notes/2.4/2.4.9.html#percona-xtrabackup-249","text":"Percona is glad to announce the release of Percona XtraBackup 2.4.9 on November 29 th 2017. Downloads are available from our download site and from apt and yum repositories. This release is the current GA (Generally Available) stable release in the 2.4 series.","title":"Percona XtraBackup 2.4.9"},{"location":"release-notes/2.4/2.4.9.html#new-features","text":"Percona XtraBackup packages are now available for Ubuntu 17.10 (Artful) . xbcrypt now has an ability to decrypt files in parallel by specifying the number of threads with the xtrabackup --encrypt-threads option. xtrabackup --copy-back option can now be used with xtrabackup --parallel option to copy the user data files in parallel (redo logs and system tablespaces are copied in the main thread).","title":"New Features"},{"location":"release-notes/2.4/2.4.9.html#bugs-fixed","text":"Percona XtraBackup would fail to backup large databases on 32-bit platforms. Bug fixed PXB-481 . Percona XtraBackup failed to build with GCC 7 . Bug fixed PXB-502 . Percona XtraBackup would hang during the prepare phase if there was not enough room in log buffer to accommodate checkpoint information at the end of the crash recovery process. Bug fixed PXB-506 . When backup was streamed in tar format with with the xtrabackup --slave-info option output file xtrabackup_slave_info did not contain the slave information. Bug fixed PXB-507 . If xtrabackup \u2013slave-info was used while backing up 5.7 instances, the master binary log coordinates were not properly displayed in the logs. Bug fixed PXB-508 . innobackupex --slave-info would report a single m instead of slave info in the standard output. Bug fixed PXB-510 . Percona XtraBackup would crash while preparing the 5.5 backup with utf8_general50_ci collation. Bug fixed PXB-748 ( Fungo Wang ). Percona XtraBackup would crash if xtrabackup --throttle was used while preparing backups. Fixed by making this option available only during the backup process. Bug fixed PXB-789 . Percona XtraBackup could get stuck if backups are taken with xtrabackup --safe-slave-backup option, while there were long running queries. Bug fixed PXB-1039 . Other bugs fixed: PXB-250 , PXB-511 , and PXB-512 .","title":"Bugs fixed"},{"location":"using_xtrabackup/configuring.html","text":"Configuring xtrabackup \u00b6 All of the xtrabackup configuration is done through options, which behave exactly like standard MySQL program options: they can be specified either at the command-line, or through a file such as /etc/my.cnf . The xtrabackup binary reads the [mysqld] and [xtrabackup] sections from any configuration files, in that order. That is so that it can read its options from your existing MySQL installation, such as the datadir or some of the InnoDB options. If you want to override these, just specify them in the [xtrabackup] section, and because it is read later, it will take precedence. You don\u2019t need to put any configuration in your my.cnf if you don\u2019t want to. You can simply specify the options on the command-line. Normally, the only thing you might find convenient to place in the [xtrabackup] section of your my.cnf file is the target_dir option to default the directory in which the backups will be placed, for example: [xtrabackup] target_dir = /data/backups/mysql/ This manual will assume that you do not have any file-based configuration for xtrabackup , so it will always show command-line options being used explicitly. Please see the option and variable reference for details on all of the configuration options. The xtrabackup binary does not accept exactly the same syntax in the my.cnf file as the mysqld server binary does. For historical reasons, the mysqld server binary accepts parameters with a --set-variable=<variable>=<value> syntax, which xtrabackup does not understand. If your my.cnf file has such configuration directives, you should rewrite them in the --variable=value syntax. System Configuration and NFS Volumes \u00b6 The xtrabackup tool requires no special configuration on most systems. However, the storage where the xtrabackup --target-dir is located must behave properly when fsync() is called. In particular, we have noticed that NFS volumes not mounted with the sync option might not really sync the data. As a result, if you back up to an NFS volume mounted with the async option, and then try to prepare the backup from a different server that also mounts that volume, the data might appear to be corrupt. You can use the sync mount option to avoid this problem.","title":"Configuring xtrabackup"},{"location":"using_xtrabackup/configuring.html#configuring-xtrabackup","text":"All of the xtrabackup configuration is done through options, which behave exactly like standard MySQL program options: they can be specified either at the command-line, or through a file such as /etc/my.cnf . The xtrabackup binary reads the [mysqld] and [xtrabackup] sections from any configuration files, in that order. That is so that it can read its options from your existing MySQL installation, such as the datadir or some of the InnoDB options. If you want to override these, just specify them in the [xtrabackup] section, and because it is read later, it will take precedence. You don\u2019t need to put any configuration in your my.cnf if you don\u2019t want to. You can simply specify the options on the command-line. Normally, the only thing you might find convenient to place in the [xtrabackup] section of your my.cnf file is the target_dir option to default the directory in which the backups will be placed, for example: [xtrabackup] target_dir = /data/backups/mysql/ This manual will assume that you do not have any file-based configuration for xtrabackup , so it will always show command-line options being used explicitly. Please see the option and variable reference for details on all of the configuration options. The xtrabackup binary does not accept exactly the same syntax in the my.cnf file as the mysqld server binary does. For historical reasons, the mysqld server binary accepts parameters with a --set-variable=<variable>=<value> syntax, which xtrabackup does not understand. If your my.cnf file has such configuration directives, you should rewrite them in the --variable=value syntax.","title":"Configuring xtrabackup"},{"location":"using_xtrabackup/configuring.html#system-configuration-and-nfs-volumes","text":"The xtrabackup tool requires no special configuration on most systems. However, the storage where the xtrabackup --target-dir is located must behave properly when fsync() is called. In particular, we have noticed that NFS volumes not mounted with the sync option might not really sync the data. As a result, if you back up to an NFS volume mounted with the async option, and then try to prepare the backup from a different server that also mounts that volume, the data might appear to be corrupt. You can use the sync mount option to avoid this problem.","title":"System Configuration and NFS Volumes"},{"location":"using_xtrabackup/privileges.html","text":"Connection and Privileges Needed \u00b6 Percona XtraBackup needs to be able to connect to the database server and perform operations on the server and the datadir when creating a backup, when preparing in some scenarios and when restoring it. In order to do so, there are privileges and permission requirements on its execution that must be fulfilled. Privileges refers to the operations that a system user is permitted to do in the database server. They are set at the database server and only apply to users in the database server . Permissions are those which permits a user to perform operations on the system, like reading, writing or executing on a certain directory or start/stop a system service. They are set at a system level and only apply to system users . Whether xtrabackup or innobackupex is used, there are two actors involved: the user invoking the program - a system user - and the user performing action in the database server - a database user . Note that these are different users in different places, even though they may have the same username. All the invocations of innobackupex and xtrabackup in this documentation assume that the system user has the appropriate permissions and you are providing the relevant options for connecting the database server - besides the options for the action to be performed - and the database user has adequate privileges. Connecting to the server \u00b6 The database user used to connect to the server and its password are specified by the xtrabackup --user and xtrabackup \u2013password option: $ xtrabackup --user = DVADER --password = 14MY0URF4TH3R --backup \\ --target-dir = /data/bkps/ $ innobackupex --user = DBUSER --password = SECRET /path/to/backup/dir/ $ innobackupex --user = LUKE --password = US3TH3F0RC3 --stream = tar ./ | bzip2 - If you don\u2019t use the xtrabackup --user option, Percona XtraBackup will assume the database user whose name is the system user executing it. Other Connection Options \u00b6 According to your system, you may need to specify one or more of the following options to connect to the server: Option Description \u2013port The port to use when connecting to the database server with TCP/IP. \u2013socket The socket to use when connecting to the local database. \u2013host The host to use when connecting to the database server with TCP/IP. These options are passed to the mysql child process without alteration, see mysql --help for details. Note In case of multiple server instances the correct connection parameters (port, socket, host) must be specified in order for xtrabackup to talk to the correct server. Permissions and Privileges Needed \u00b6 Once connected to the server, in order to perform a backup you will need READ and EXECUTE permissions at a filesystem level in the server\u2019s datadir. The database user needs the following privileges on the tables/databases to be backed up: RELOAD and LOCK TABLES (unless the \u2013no-lock option is specified) in order to FLUSH TABLES WITH READ LOCK and FLUSH ENGINE LOGS prior to start copying the files, and LOCK TABLES FOR BACKUP and LOCK BINLOG FOR BACKUP require this privilege when Backup Locks are used. REPLICATION CLIENT in order to obtain the binary log position. CREATE TABLESPACE in order to import tables (see Restoring Individual Tables ). PROCESS in order to run SHOW ENGINE INNODB STATUS (which is mandatory), and optionally to see all threads which are running on the server (see Improved FLUSH TABLES WITH READ LOCK handling ). SUPER in order to start/stop the replica threads in a replication environment, use XtraDB Changed Page Tracking for Incremental Backups and for Improved FLUSH TABLES WITH READ LOCK handling . CREATE privilege in order to create the PERCONA_SCHEMA.xtrabackup_history database and table. ALTER privilege in order to upgrade the PERCONA_SCHEMA.xtrabackup_history database and table. INSERT privilege in order to add history records to the PERCONA_SCHEMA.xtrabackup_history table. SELECT privilege in order to use innobackupex --incremental-history-name or innobackupex --incremental-history-uuid in order for the feature to look up the innodb_to_lsn values in the PERCONA_SCHEMA.xtrabackup_history table. The explanation of when these are used can be found in How Percona XtraBackup Works . An SQL example of creating a database user with the minimum privileges required to full backups would be: mysql > CREATE USER 'bkpuser' @ 'localhost' IDENTIFIED BY 's3cret' ; mysql > GRANT RELOAD , LOCK TABLES , PROCESS , REPLICATION CLIENT ON * . * TO 'bkpuser' @ 'localhost' ; mysql > FLUSH PRIVILEGES ;","title":"Connection and Privileges Needed"},{"location":"using_xtrabackup/privileges.html#connection-and-privileges-needed","text":"Percona XtraBackup needs to be able to connect to the database server and perform operations on the server and the datadir when creating a backup, when preparing in some scenarios and when restoring it. In order to do so, there are privileges and permission requirements on its execution that must be fulfilled. Privileges refers to the operations that a system user is permitted to do in the database server. They are set at the database server and only apply to users in the database server . Permissions are those which permits a user to perform operations on the system, like reading, writing or executing on a certain directory or start/stop a system service. They are set at a system level and only apply to system users . Whether xtrabackup or innobackupex is used, there are two actors involved: the user invoking the program - a system user - and the user performing action in the database server - a database user . Note that these are different users in different places, even though they may have the same username. All the invocations of innobackupex and xtrabackup in this documentation assume that the system user has the appropriate permissions and you are providing the relevant options for connecting the database server - besides the options for the action to be performed - and the database user has adequate privileges.","title":"Connection and Privileges Needed"},{"location":"using_xtrabackup/privileges.html#connecting-to-the-server","text":"The database user used to connect to the server and its password are specified by the xtrabackup --user and xtrabackup \u2013password option: $ xtrabackup --user = DVADER --password = 14MY0URF4TH3R --backup \\ --target-dir = /data/bkps/ $ innobackupex --user = DBUSER --password = SECRET /path/to/backup/dir/ $ innobackupex --user = LUKE --password = US3TH3F0RC3 --stream = tar ./ | bzip2 - If you don\u2019t use the xtrabackup --user option, Percona XtraBackup will assume the database user whose name is the system user executing it.","title":"Connecting to the server"},{"location":"using_xtrabackup/privileges.html#other-connection-options","text":"According to your system, you may need to specify one or more of the following options to connect to the server: Option Description \u2013port The port to use when connecting to the database server with TCP/IP. \u2013socket The socket to use when connecting to the local database. \u2013host The host to use when connecting to the database server with TCP/IP. These options are passed to the mysql child process without alteration, see mysql --help for details. Note In case of multiple server instances the correct connection parameters (port, socket, host) must be specified in order for xtrabackup to talk to the correct server.","title":"Other Connection Options"},{"location":"using_xtrabackup/privileges.html#permissions-and-privileges-needed","text":"Once connected to the server, in order to perform a backup you will need READ and EXECUTE permissions at a filesystem level in the server\u2019s datadir. The database user needs the following privileges on the tables/databases to be backed up: RELOAD and LOCK TABLES (unless the \u2013no-lock option is specified) in order to FLUSH TABLES WITH READ LOCK and FLUSH ENGINE LOGS prior to start copying the files, and LOCK TABLES FOR BACKUP and LOCK BINLOG FOR BACKUP require this privilege when Backup Locks are used. REPLICATION CLIENT in order to obtain the binary log position. CREATE TABLESPACE in order to import tables (see Restoring Individual Tables ). PROCESS in order to run SHOW ENGINE INNODB STATUS (which is mandatory), and optionally to see all threads which are running on the server (see Improved FLUSH TABLES WITH READ LOCK handling ). SUPER in order to start/stop the replica threads in a replication environment, use XtraDB Changed Page Tracking for Incremental Backups and for Improved FLUSH TABLES WITH READ LOCK handling . CREATE privilege in order to create the PERCONA_SCHEMA.xtrabackup_history database and table. ALTER privilege in order to upgrade the PERCONA_SCHEMA.xtrabackup_history database and table. INSERT privilege in order to add history records to the PERCONA_SCHEMA.xtrabackup_history table. SELECT privilege in order to use innobackupex --incremental-history-name or innobackupex --incremental-history-uuid in order for the feature to look up the innodb_to_lsn values in the PERCONA_SCHEMA.xtrabackup_history table. The explanation of when these are used can be found in How Percona XtraBackup Works . An SQL example of creating a database user with the minimum privileges required to full backups would be: mysql > CREATE USER 'bkpuser' @ 'localhost' IDENTIFIED BY 's3cret' ; mysql > GRANT RELOAD , LOCK TABLES , PROCESS , REPLICATION CLIENT ON * . * TO 'bkpuser' @ 'localhost' ; mysql > FLUSH PRIVILEGES ;","title":"Permissions and Privileges Needed"},{"location":"xbcloud/xbcloud.html","text":"The xbcloud Binary \u00b6 The purpose of xbcloud is to download and upload full or part of xbstream archive from/to the cloud. xbcloud will not overwrite the backup with the same name. xbcloud accepts input via a pipe from xbstream so that it can be invoked as a pipeline with xtrabackup to stream directly to the cloud without needing a local storage. Note In a Bash shell, the $? parameter returns the exit code from the last binary. If you use pipes the ${PIPESTATUS[x]} array parameter returns the exit codes for each binary in the pipe string. $ xtrabackup --backup --stream = xbstream --target-dir = /storage/backups/ | xbcloud put [ options ] full_backup > true | false > echo $? > 1 # with PIPESTATUS > true | false > echo ${ PIPESTATUS [0] } ${ PIPESTATUS [1] } > 0 1 The xbcloud binary stores each chunk as a separate object with a name backup_name/database/table.ibd.NNNNNNNNNNNNNNNNNNNN , where NNN... is a 0-padded serial number of chunk within a file. Size of chunk produced by xtrabackup and xbstream changed to 10MB. Note Use --read-buffer-size to adjust the chunk size. If you use encryption, specify both the --read-buffer-size and --encrypt-chunk-size options to adjust the chunk size. xbcloud has three essential operations: put , get , and delete . With these operations, backups are created, stored, retrieved, restored, and deleted. xbcloud operations clearly map to similar operations within the AWS S3 API. Version specific information \u00b6 2.4.25 - Added the support for Microsoft Azure Cloud Storage 2.4.21 - Added s3-storage-class and google-storage-class 2.4.14 - Added the support of Amazon S3 , MinIO and Google Cloud Storage storage types. 2.3.1-beta1 - Implemented ability to store xbcloud parameters in a .cnf file 2.3.1-beta1 - Implemented support different authentication options for Swift 2.3.1-beta1 - Implemented support for partial download of the cloud backups 2.3.1-beta1 - xbcloud --swift-url option has been renamed to xbcloud --swift-auth-url 2.3.0-alpha1 - Initial implementation Supported Cloud Storage Types \u00b6 Swift was the only option for storing backups in cloud storage until Percona XtraBackup 2.4.14 . Currently, the xbcloud binary supports Amazon S3 , Azure , MinIO and Google Cloud Storage . Amazon S3-compatible cloud storage types, such as Wasabi and Digital Ocean Spaces, are also supported. See also OpenStack Object Storage (\u201cSwift\u201d) Amazon Simple Storage Service Azure Cloud Storage MinIO Google Cloud Storage Wasabi Digital Ocean Spaces Usage \u00b6 $ xtrabackup --backup --stream = xbstream --target-dir = /tmp | xbcloud \\ put [ options ] <name> Creating a full backup with Swift \u00b6 The following example shows how to make a full backup and upload it to Swift. $ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = swift \\ --swift-container = test \\ --swift-user = test:tester \\ --swift-auth-url = http://192.168.8.80:8080/ \\ --swift-key = testing \\ --parallel = 10 \\ full_backup Creating a full backup with Amazon S3 \u00b6 $ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = s3 \\ --s3-endpoint = 's3.amazonaws.com' \\ --s3-access-key = 'YOUR-ACCESSKEYID' \\ --s3-secret-key = 'YOUR-SECRETACCESSKEY' \\ --s3-bucket = 'mysql_backups' --parallel = 10 \\ $( date -I ) -full_backup The following options are available when using Amazon S3 : Option Details \u2013s3-access-key Use to supply the AWS access key ID \u2013s3-secret-key Use to supply the AWS secret access key \u2013s3-bucket Use supply the AWS bucket name \u2013s3-region Use to specify the AWS region. The default value is us-east-1 \u2013s3-api-version = <AUTO|2|4> Select the signing algorithm. The default value is AUTO. In this case, xbcloud will probe. \u2013s3-bucket-lookup = <AUTO|PATH|DNS> Specify whether to use bucket.endpoint.com or endpoint.com/bucket* style requests. The default value is AUTO. In this case, xbcloud will probe. \u2013s3-storage-class=<name> Specify the S3 storage class . The name options are the following: STANDARD STANDARD_IA GLACIER NOTE: If you use the GLACIER storage class, the object must be restored to S3 before restoring the backup. Also, supports using custom S3 implementations such as MinIO or CephRadosGW. Creating a full backup with MinIO \u00b6 $ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = s3 \\ --s3-endpoint = 'play.minio.io:9000' \\ --s3-access-key = 'YOUR-ACCESSKEYID' \\ --s3-secret-key = 'YOUR-SECRETACCESSKEY' \\ --s3-bucket = 'mysql_backups' --parallel = 10 \\ $( date -I ) -full_backup Creating a full backup with Google Cloud Storage \u00b6 The support for Google Cloud Storage is implemented using the interoperability mode. This mode was especially designed to interact with cloud services compatible with Amazon S3 . See also Cloud Storage Interoperability https://cloud.google.com/storage/docs/interoperability $ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = google \\ --google-endpoint = ` storage.googleapis.com ` \\ --google-access-key = 'YOUR-ACCESSKEYID' \\ --google-secret-key = 'YOUR-SECRETACCESSKEY' \\ --google-bucket = 'mysql_backups' --parallel = 10 \\ $( date -I ) -full_backup The following options are available when using Google Cloud Storage: \u2013google-access-key = <ACCESS KEY ID> \u2013google-secret-key = <SECRET ACCESS KEY> \u2013google-bucket = <BUCKET NAME> \u2013google-storage-class=name Note The Google storage class name options are the following: STANDARD NEARLINE COLDLINE ARCHIVE See also : Google storage classes Supplying parameters \u00b6 Each storage type has mandatory parameters that you can supply on the command line, in a configuration file, and via environment variables. Configuration files \u00b6 The parameters the values of which do not change frequently can be stored in my.cnf or in a custom configuration file. The following example is a template of configuration options under the [xbcloud] group: [xbcloud] storage=s3 s3-endpoint=http://localhost:9000/ s3-access-key=minio s3-secret-key=minio123 s3-bucket=backupsx s3-bucket-lookup=path s3-api-version=4 Note If you explicitly use a parameter on the command line and in a configuration file, xbcloud uses the the value provided on the command line. Environment variables \u00b6 The following environment variables are recognized. xbcloud maps them automatically to corresponding parameters applicable to the selected storage. AWS_ACCESS_KEY_ID (or ACCESS_KEY_ID) AWS_SECRET_ACCESS_KEY (or SECRET_ACCESS_KEY) AWS_DEFAULT_REGION (or DEFAULT_REGION) AWS_ENDPOINT (or ENDPOINT) AWS_CA_BUNDLE Note If you explicitly use a parameter on the command line, in a configuration file, and the corresponding environment variable contains a value, xbcloud uses the the value provided on the command line or in the configuration file. OpenStack environment variables are also recognized and mapped automatically to corresponding swift parameters ( --storage=swift ). OS_AUTH_URL OS_TENANT_NAME OS_TENANT_ID OS_USERNAME OS_PASSWORD OS_USER_DOMAIN OS_USER_DOMAIN_ID OS_PROJECT_DOMAIN OS_PROJECT_DOMAIN_ID OS_REGION_NAME OS_STORAGE_URL OS_CACERT Shortcuts \u00b6 For all operations (put, get, and delete), you can use a shortcut to specify the storage type, bucket name, and backup name as one parameter instead of using three distinct parameters (\u2013storage, \u2013s3-bucket, and backup name per se). Using a shortcut syntax to provide a storage type, bucket, and backup name Use the following format: storage-type://bucket-name/backup-name $ xbcloud get s3://operator-testing/bak22 ... In this example, s3 refers to a storage type, operator-testing is a bucket name, and bak22 is the backup name. This shortcut expands as follows: $ xbcloud get --storage = s3 --s3-bucket = operator-testing bak22 ... You can supply the mandatory parameters not only on the command line. You may use configuration files and environment variables. Additional parameters \u00b6 xbcloud accepts additional parameters that you can use with any storage type. The --md5 parameter computes the MD5 hash value of the backup chunks. The result is stored in files that following the backup_name.md5 pattern. $ xtrabackup --backup --stream = xbstream \\ --parallel = 8 2 >backup.log | xbcloud put s3://operator-testing/bak22 \\ --parallel = 8 --md5 2 >upload.log You may use the --header parameter to pass an additional HTTP header with the server side encryption while specifying a customer key. Example of using \u2013header for AES256 encryption $ xtrabackup --backup --stream = xbstream --parallel = 4 | \\ xbcloud put s3://operator-testing/bak-enc/ \\ --header = \"X-Amz-Server-Side-Encryption-Customer-Algorithm: AES256\" \\ --header = \"X-Amz-Server-Side-Encryption-Customer-Key: CuStoMerKey=\" \\ --header = \"X-Amz-Server-Side-Encryption-Customer-Key-MD5: CuStoMerKeyMd5==\" \\ --parallel = 8 The --header parameter is also useful to set the access control list (ACL) permissions: --header=\"x-amz-acl: bucket-owner-full-control Restoring with Swift \u00b6 xbcloud get [options] <name> [<list-of-files>] | xbstream -x The following example shows how to fetch and restore the backup from Swift: $ xbcloud get --storage = swift \\ --swift-container = test \\ --swift-user = test:tester \\ --swift-auth-url = http://192.168.8.80:8080/ \\ --swift-key = testing \\ full_backup | xbstream -xv -C /tmp/downloaded_full $ xtrabackup --prepare --target-dir = /tmp/downloaded_full $ xtrabackup --copy-back --target-dir = /tmp/downloaded_full Restoring with Amazon S3 \u00b6 $ xbcloud get s3://operator-testing/bak22 \\ --s3-endpoint = https://storage.googleapis.com/ \\ --parallel = 10 2 >download.log | xbstream -x -C restore --parallel = 8 Incremental backups \u00b6 First, make the full backup which is the base for an incremental backup: $ xtrabackup --backup --stream = xbstream --extra-lsndir = /storage/backups/ \\ --target-dir = /storage/backups/ | xbcloud put \\ --storage = swift --swift-container = test_backup \\ --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ --parallel = 10 \\ full_backup Then make the incremental backup: $ xtrabackup --backup --incremental-basedir = /storage/backups \\ --stream = xbstream --target-dir = /storage/inc_backup | xbcloud put \\ --storage = swift --swift-container = test_backup \\ --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ --parallel = 10 \\ inc_backup Preparing an incremental backup \u00b6 To prepare a backup, download the full backup: $ xbcloud get --swift-container = test_backup \\ --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ --parallel = 10 \\ full_backup | xbstream -xv -C /storage/downloaded_full Prepare the downloaded full backup: $ xtrabackup --prepare --apply-log-only --target-dir = /storage/downloaded_full After the full backup has been prepared, download the incremental backup: $ xbcloud get --swift-container = test_backup \\ --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ --parallel = 10 \\ inc_backup | xbstream -xv -C /storage/downloaded_inc Prepare the incremental backup: $ xtrabackup --prepare --apply-log-only \\ --target-dir = /storage/downloaded_full \\ --incremental-dir = /storage/downloaded_inc $ xtrabackup --prepare --target-dir = /storage/downloaded_full Partial download of the cloud backup \u00b6 If you don\u2019t want to download the entire backup to restore a database you can restore only specific tables: $ xbcloud get --swift-container = test_backup --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ full_backup \\ ibdata1 sakila/payment.ibd \\ > /storage/partial/partial.xbs $ xbstream -xv -C /storage/partial < /storage/partial/partial.xbs This command downloads the ibdata1 table and the sakila/payment.ibd table from a full backup. Command-line options \u00b6 xbcloud has the following command line options: \u2013storage=[swift Amazon S3 google] \u00b6 Cloud storage option. xbcloud supports Swift, MinIO, and AWS S3. The default value is swift . \u2013swift-auth-url \u00b6 URL of Swift cluster. \u2013swift-url \u00b6 Renamed to xbcloud \u2013swift-auth-url \u2013swift-storage-url \u00b6 xbcloud attempts to get object-store URL for a specified region (if any specified) from the keystone response. One can override that URL by passing \u2013swift-storage-url=URL argument. \u2013swift-user \u00b6 Swift username (X-Auth-User, specific to Swift) \u2013swift-key \u00b6 Swift key/password (X-Auth-Key, specific to Swift) \u2013swift-container \u00b6 Container to backup into (specific to Swift) \u2013parallel=N \u00b6 Maximum number of concurrent upload/download requests. Default is 1 . \u2013cacert \u00b6 Path to the file with CA certificates \u2013insecure \u00b6 Do not verify servers certificate Swift authentication options \u00b6 Swift specification describe several authentication options . xbcloud can authenticate against keystone with API version 2 and 3. \u2013swift-auth-version \u00b6 Specifies the swift authentication version. Possible values are: 1.0 - TempAuth, 2.0 - Keystone v2.0, and 3 - Keystone v3. Default value is 1.0 . For v2 additional options are: \u2013swift-tenant \u00b6 Swift tenant name. \u2013swift-tenant-id \u00b6 Swift tenant ID. \u2013swift-region \u00b6 Swift endpoint region. \u2013swift-password \u00b6 Swift password for the user. For v3 additional options are: \u2013swift-user-id \u00b6 Swift user ID. \u2013swift-project \u00b6 Swift project name. \u2013swift-project-id \u00b6 Swift project ID. \u2013swift-domain \u00b6 Swift domain name. \u2013swift-domain-id \u00b6 Swift domain ID.","title":"The xbcloud Binary"},{"location":"xbcloud/xbcloud.html#the-xbcloud-binary","text":"The purpose of xbcloud is to download and upload full or part of xbstream archive from/to the cloud. xbcloud will not overwrite the backup with the same name. xbcloud accepts input via a pipe from xbstream so that it can be invoked as a pipeline with xtrabackup to stream directly to the cloud without needing a local storage. Note In a Bash shell, the $? parameter returns the exit code from the last binary. If you use pipes the ${PIPESTATUS[x]} array parameter returns the exit codes for each binary in the pipe string. $ xtrabackup --backup --stream = xbstream --target-dir = /storage/backups/ | xbcloud put [ options ] full_backup > true | false > echo $? > 1 # with PIPESTATUS > true | false > echo ${ PIPESTATUS [0] } ${ PIPESTATUS [1] } > 0 1 The xbcloud binary stores each chunk as a separate object with a name backup_name/database/table.ibd.NNNNNNNNNNNNNNNNNNNN , where NNN... is a 0-padded serial number of chunk within a file. Size of chunk produced by xtrabackup and xbstream changed to 10MB. Note Use --read-buffer-size to adjust the chunk size. If you use encryption, specify both the --read-buffer-size and --encrypt-chunk-size options to adjust the chunk size. xbcloud has three essential operations: put , get , and delete . With these operations, backups are created, stored, retrieved, restored, and deleted. xbcloud operations clearly map to similar operations within the AWS S3 API.","title":"The xbcloud Binary"},{"location":"xbcloud/xbcloud.html#version-specific-information","text":"2.4.25 - Added the support for Microsoft Azure Cloud Storage 2.4.21 - Added s3-storage-class and google-storage-class 2.4.14 - Added the support of Amazon S3 , MinIO and Google Cloud Storage storage types. 2.3.1-beta1 - Implemented ability to store xbcloud parameters in a .cnf file 2.3.1-beta1 - Implemented support different authentication options for Swift 2.3.1-beta1 - Implemented support for partial download of the cloud backups 2.3.1-beta1 - xbcloud --swift-url option has been renamed to xbcloud --swift-auth-url 2.3.0-alpha1 - Initial implementation","title":"Version specific information"},{"location":"xbcloud/xbcloud.html#supported-cloud-storage-types","text":"Swift was the only option for storing backups in cloud storage until Percona XtraBackup 2.4.14 . Currently, the xbcloud binary supports Amazon S3 , Azure , MinIO and Google Cloud Storage . Amazon S3-compatible cloud storage types, such as Wasabi and Digital Ocean Spaces, are also supported. See also OpenStack Object Storage (\u201cSwift\u201d) Amazon Simple Storage Service Azure Cloud Storage MinIO Google Cloud Storage Wasabi Digital Ocean Spaces","title":"Supported Cloud Storage Types"},{"location":"xbcloud/xbcloud.html#usage","text":"$ xtrabackup --backup --stream = xbstream --target-dir = /tmp | xbcloud \\ put [ options ] <name>","title":"Usage"},{"location":"xbcloud/xbcloud.html#creating-a-full-backup-with-swift","text":"The following example shows how to make a full backup and upload it to Swift. $ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = swift \\ --swift-container = test \\ --swift-user = test:tester \\ --swift-auth-url = http://192.168.8.80:8080/ \\ --swift-key = testing \\ --parallel = 10 \\ full_backup","title":"Creating a full backup with Swift"},{"location":"xbcloud/xbcloud.html#creating-a-full-backup-with-amazon-s3","text":"$ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = s3 \\ --s3-endpoint = 's3.amazonaws.com' \\ --s3-access-key = 'YOUR-ACCESSKEYID' \\ --s3-secret-key = 'YOUR-SECRETACCESSKEY' \\ --s3-bucket = 'mysql_backups' --parallel = 10 \\ $( date -I ) -full_backup The following options are available when using Amazon S3 : Option Details \u2013s3-access-key Use to supply the AWS access key ID \u2013s3-secret-key Use to supply the AWS secret access key \u2013s3-bucket Use supply the AWS bucket name \u2013s3-region Use to specify the AWS region. The default value is us-east-1 \u2013s3-api-version = <AUTO|2|4> Select the signing algorithm. The default value is AUTO. In this case, xbcloud will probe. \u2013s3-bucket-lookup = <AUTO|PATH|DNS> Specify whether to use bucket.endpoint.com or endpoint.com/bucket* style requests. The default value is AUTO. In this case, xbcloud will probe. \u2013s3-storage-class=<name> Specify the S3 storage class . The name options are the following: STANDARD STANDARD_IA GLACIER NOTE: If you use the GLACIER storage class, the object must be restored to S3 before restoring the backup. Also, supports using custom S3 implementations such as MinIO or CephRadosGW.","title":"Creating a full backup with Amazon S3"},{"location":"xbcloud/xbcloud.html#creating-a-full-backup-with-minio","text":"$ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = s3 \\ --s3-endpoint = 'play.minio.io:9000' \\ --s3-access-key = 'YOUR-ACCESSKEYID' \\ --s3-secret-key = 'YOUR-SECRETACCESSKEY' \\ --s3-bucket = 'mysql_backups' --parallel = 10 \\ $( date -I ) -full_backup","title":"Creating a full backup with MinIO"},{"location":"xbcloud/xbcloud.html#creating-a-full-backup-with-google-cloud-storage","text":"The support for Google Cloud Storage is implemented using the interoperability mode. This mode was especially designed to interact with cloud services compatible with Amazon S3 . See also Cloud Storage Interoperability https://cloud.google.com/storage/docs/interoperability $ xtrabackup --backup --stream = xbstream --extra-lsndir = /tmp --target-dir = /tmp | \\ xbcloud put --storage = google \\ --google-endpoint = ` storage.googleapis.com ` \\ --google-access-key = 'YOUR-ACCESSKEYID' \\ --google-secret-key = 'YOUR-SECRETACCESSKEY' \\ --google-bucket = 'mysql_backups' --parallel = 10 \\ $( date -I ) -full_backup The following options are available when using Google Cloud Storage: \u2013google-access-key = <ACCESS KEY ID> \u2013google-secret-key = <SECRET ACCESS KEY> \u2013google-bucket = <BUCKET NAME> \u2013google-storage-class=name Note The Google storage class name options are the following: STANDARD NEARLINE COLDLINE ARCHIVE See also : Google storage classes","title":"Creating a full backup with Google Cloud Storage"},{"location":"xbcloud/xbcloud.html#supplying-parameters","text":"Each storage type has mandatory parameters that you can supply on the command line, in a configuration file, and via environment variables.","title":"Supplying parameters"},{"location":"xbcloud/xbcloud.html#configuration-files","text":"The parameters the values of which do not change frequently can be stored in my.cnf or in a custom configuration file. The following example is a template of configuration options under the [xbcloud] group: [xbcloud] storage=s3 s3-endpoint=http://localhost:9000/ s3-access-key=minio s3-secret-key=minio123 s3-bucket=backupsx s3-bucket-lookup=path s3-api-version=4 Note If you explicitly use a parameter on the command line and in a configuration file, xbcloud uses the the value provided on the command line.","title":"Configuration files"},{"location":"xbcloud/xbcloud.html#environment-variables","text":"The following environment variables are recognized. xbcloud maps them automatically to corresponding parameters applicable to the selected storage. AWS_ACCESS_KEY_ID (or ACCESS_KEY_ID) AWS_SECRET_ACCESS_KEY (or SECRET_ACCESS_KEY) AWS_DEFAULT_REGION (or DEFAULT_REGION) AWS_ENDPOINT (or ENDPOINT) AWS_CA_BUNDLE Note If you explicitly use a parameter on the command line, in a configuration file, and the corresponding environment variable contains a value, xbcloud uses the the value provided on the command line or in the configuration file. OpenStack environment variables are also recognized and mapped automatically to corresponding swift parameters ( --storage=swift ). OS_AUTH_URL OS_TENANT_NAME OS_TENANT_ID OS_USERNAME OS_PASSWORD OS_USER_DOMAIN OS_USER_DOMAIN_ID OS_PROJECT_DOMAIN OS_PROJECT_DOMAIN_ID OS_REGION_NAME OS_STORAGE_URL OS_CACERT","title":"Environment variables"},{"location":"xbcloud/xbcloud.html#shortcuts","text":"For all operations (put, get, and delete), you can use a shortcut to specify the storage type, bucket name, and backup name as one parameter instead of using three distinct parameters (\u2013storage, \u2013s3-bucket, and backup name per se). Using a shortcut syntax to provide a storage type, bucket, and backup name Use the following format: storage-type://bucket-name/backup-name $ xbcloud get s3://operator-testing/bak22 ... In this example, s3 refers to a storage type, operator-testing is a bucket name, and bak22 is the backup name. This shortcut expands as follows: $ xbcloud get --storage = s3 --s3-bucket = operator-testing bak22 ... You can supply the mandatory parameters not only on the command line. You may use configuration files and environment variables.","title":"Shortcuts"},{"location":"xbcloud/xbcloud.html#additional-parameters","text":"xbcloud accepts additional parameters that you can use with any storage type. The --md5 parameter computes the MD5 hash value of the backup chunks. The result is stored in files that following the backup_name.md5 pattern. $ xtrabackup --backup --stream = xbstream \\ --parallel = 8 2 >backup.log | xbcloud put s3://operator-testing/bak22 \\ --parallel = 8 --md5 2 >upload.log You may use the --header parameter to pass an additional HTTP header with the server side encryption while specifying a customer key. Example of using \u2013header for AES256 encryption $ xtrabackup --backup --stream = xbstream --parallel = 4 | \\ xbcloud put s3://operator-testing/bak-enc/ \\ --header = \"X-Amz-Server-Side-Encryption-Customer-Algorithm: AES256\" \\ --header = \"X-Amz-Server-Side-Encryption-Customer-Key: CuStoMerKey=\" \\ --header = \"X-Amz-Server-Side-Encryption-Customer-Key-MD5: CuStoMerKeyMd5==\" \\ --parallel = 8 The --header parameter is also useful to set the access control list (ACL) permissions: --header=\"x-amz-acl: bucket-owner-full-control","title":"Additional parameters"},{"location":"xbcloud/xbcloud.html#restoring-with-swift","text":"xbcloud get [options] <name> [<list-of-files>] | xbstream -x The following example shows how to fetch and restore the backup from Swift: $ xbcloud get --storage = swift \\ --swift-container = test \\ --swift-user = test:tester \\ --swift-auth-url = http://192.168.8.80:8080/ \\ --swift-key = testing \\ full_backup | xbstream -xv -C /tmp/downloaded_full $ xtrabackup --prepare --target-dir = /tmp/downloaded_full $ xtrabackup --copy-back --target-dir = /tmp/downloaded_full","title":"Restoring with Swift"},{"location":"xbcloud/xbcloud.html#restoring-with-amazon-s3","text":"$ xbcloud get s3://operator-testing/bak22 \\ --s3-endpoint = https://storage.googleapis.com/ \\ --parallel = 10 2 >download.log | xbstream -x -C restore --parallel = 8","title":"Restoring with Amazon S3"},{"location":"xbcloud/xbcloud.html#incremental-backups","text":"First, make the full backup which is the base for an incremental backup: $ xtrabackup --backup --stream = xbstream --extra-lsndir = /storage/backups/ \\ --target-dir = /storage/backups/ | xbcloud put \\ --storage = swift --swift-container = test_backup \\ --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ --parallel = 10 \\ full_backup Then make the incremental backup: $ xtrabackup --backup --incremental-basedir = /storage/backups \\ --stream = xbstream --target-dir = /storage/inc_backup | xbcloud put \\ --storage = swift --swift-container = test_backup \\ --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ --parallel = 10 \\ inc_backup","title":"Incremental backups"},{"location":"xbcloud/xbcloud.html#preparing-an-incremental-backup","text":"To prepare a backup, download the full backup: $ xbcloud get --swift-container = test_backup \\ --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ --parallel = 10 \\ full_backup | xbstream -xv -C /storage/downloaded_full Prepare the downloaded full backup: $ xtrabackup --prepare --apply-log-only --target-dir = /storage/downloaded_full After the full backup has been prepared, download the incremental backup: $ xbcloud get --swift-container = test_backup \\ --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ --parallel = 10 \\ inc_backup | xbstream -xv -C /storage/downloaded_inc Prepare the incremental backup: $ xtrabackup --prepare --apply-log-only \\ --target-dir = /storage/downloaded_full \\ --incremental-dir = /storage/downloaded_inc $ xtrabackup --prepare --target-dir = /storage/downloaded_full","title":"Preparing an incremental backup"},{"location":"xbcloud/xbcloud.html#partial-download-of-the-cloud-backup","text":"If you don\u2019t want to download the entire backup to restore a database you can restore only specific tables: $ xbcloud get --swift-container = test_backup --swift-auth-version = 2 .0 --swift-user = admin \\ --swift-tenant = admin --swift-password = xoxoxoxo \\ --swift-auth-url = http://127.0.0.1:35357/ full_backup \\ ibdata1 sakila/payment.ibd \\ > /storage/partial/partial.xbs $ xbstream -xv -C /storage/partial < /storage/partial/partial.xbs This command downloads the ibdata1 table and the sakila/payment.ibd table from a full backup.","title":"Partial download of the cloud backup"},{"location":"xbcloud/xbcloud.html#command-line-options","text":"xbcloud has the following command line options:","title":"Command-line options"},{"location":"xbcloud/xbcloud.html#-storageswiftamazon-s3google","text":"Cloud storage option. xbcloud supports Swift, MinIO, and AWS S3. The default value is swift .","title":"--storage=[swiftAmazon S3google]"},{"location":"xbcloud/xbcloud.html#-swift-auth-url","text":"URL of Swift cluster.","title":"--swift-auth-url"},{"location":"xbcloud/xbcloud.html#-swift-url","text":"Renamed to xbcloud \u2013swift-auth-url","title":"--swift-url"},{"location":"xbcloud/xbcloud.html#-swift-storage-url","text":"xbcloud attempts to get object-store URL for a specified region (if any specified) from the keystone response. One can override that URL by passing \u2013swift-storage-url=URL argument.","title":"--swift-storage-url"},{"location":"xbcloud/xbcloud.html#-swift-user","text":"Swift username (X-Auth-User, specific to Swift)","title":"--swift-user"},{"location":"xbcloud/xbcloud.html#-swift-key","text":"Swift key/password (X-Auth-Key, specific to Swift)","title":"--swift-key"},{"location":"xbcloud/xbcloud.html#-swift-container","text":"Container to backup into (specific to Swift)","title":"--swift-container"},{"location":"xbcloud/xbcloud.html#-paralleln","text":"Maximum number of concurrent upload/download requests. Default is 1 .","title":"--parallel=N"},{"location":"xbcloud/xbcloud.html#-cacert","text":"Path to the file with CA certificates","title":"--cacert"},{"location":"xbcloud/xbcloud.html#-insecure","text":"Do not verify servers certificate","title":"--insecure"},{"location":"xbcloud/xbcloud.html#swift-authentication-options","text":"Swift specification describe several authentication options . xbcloud can authenticate against keystone with API version 2 and 3.","title":"Swift authentication options"},{"location":"xbcloud/xbcloud.html#-swift-auth-version","text":"Specifies the swift authentication version. Possible values are: 1.0 - TempAuth, 2.0 - Keystone v2.0, and 3 - Keystone v3. Default value is 1.0 . For v2 additional options are:","title":"--swift-auth-version"},{"location":"xbcloud/xbcloud.html#-swift-tenant","text":"Swift tenant name.","title":"--swift-tenant"},{"location":"xbcloud/xbcloud.html#-swift-tenant-id","text":"Swift tenant ID.","title":"--swift-tenant-id"},{"location":"xbcloud/xbcloud.html#-swift-region","text":"Swift endpoint region.","title":"--swift-region"},{"location":"xbcloud/xbcloud.html#-swift-password","text":"Swift password for the user. For v3 additional options are:","title":"--swift-password"},{"location":"xbcloud/xbcloud.html#-swift-user-id","text":"Swift user ID.","title":"--swift-user-id"},{"location":"xbcloud/xbcloud.html#-swift-project","text":"Swift project name.","title":"--swift-project"},{"location":"xbcloud/xbcloud.html#-swift-project-id","text":"Swift project ID.","title":"--swift-project-id"},{"location":"xbcloud/xbcloud.html#-swift-domain","text":"Swift domain name.","title":"--swift-domain"},{"location":"xbcloud/xbcloud.html#-swift-domain-id","text":"Swift domain ID.","title":"--swift-domain-id"},{"location":"xbcloud/xbcloud_azure.html","text":"Using the xbcloud binary with Microsoft Azure Cloud Storage \u00b6 This feature is technical preview quality. Implemented in Percona XtraBackup 2.4.25, the xbcloud binary adds support for the Microsoft Azure Cloud Storage using the REST API. Options \u00b6 The following are the options, environment variables, and descriptions for uploading a backup to Azure using the REST API. The environment variables are recognized by xbcloud , which maps them automatically to the corresponding parameters: Option name Environment variables Description --azure-storage-account=name AZURE_STORAGE_ACCOUNT An Azure storage account is a unique namespace to access and store your Azure data objects. --azure-container-name=name AZURE_CONTAINER_NAME A container name is a valid DNS name that conforms to the Azure naming rules --azure-access-key=name AZURE_ACCESS_KEY A generated key that can be used to authorize access to data in your account using the Shared Key authorization. --azure-endpoint=name AZURE_ENDPOINT The endpoint allows clients to securely access data. --azure-tier-class=name AZURE_STORAGE_CLASS Cloud tier can decrease the local storage required while maintaining the performance. When enabled, this feature has the following categories: * Hot - Frequently accessed or modified data * Cool - Infrequently accessed or modified data * Archive - Rarely accessed or modified data Test your Azure applications with the Azurite open-source emulator . For testing purposes, the xbcloud binary adds the --azure-development-storage option that uses the default access_key and storage account of azurite and testcontainer for the container. You can overwrite these options, if needed. Usage \u00b6 All of the available options for xbcloud , such as parallel, max-retries, and others, can be used. For more information, see The xbcloud Binary . Examples \u00b6 An example of a xbcloud backup. $ xtrabackup --backup --stream = xbstream --target-dir = $TARGET_DIR | xbcloud put backup_name --azure-storage-account = pxbtesting --azure-access-key = $AZURE_KEY --azure-container-name = test --storage = azure An example of restoring a backup from xbcloud . $ xbcloud get backup_name --azure-storage-account = pxbtesting --azure-access-key = $AZURE_KEY --azure-container-name = test --storage = azure --parallel = 10 2 >download.log | xbstream -x -C restore An example of deleting a backup from xbcloud . $ xbcloud delete backup_name --azure-storage-account = pxbtesting --azure-access-key = $AZURE_KEY --azure-container-name = test --storage = azure An example of using a shortcut restore. $ xbcloud get azure://operator-testing/bak22 ...","title":"Using the xbcloud binary with Microsoft Azure Cloud Storage"},{"location":"xbcloud/xbcloud_azure.html#using-the-xbcloud-binary-with-microsoft-azure-cloud-storage","text":"This feature is technical preview quality. Implemented in Percona XtraBackup 2.4.25, the xbcloud binary adds support for the Microsoft Azure Cloud Storage using the REST API.","title":"Using the xbcloud binary with Microsoft Azure Cloud Storage"},{"location":"xbcloud/xbcloud_azure.html#options","text":"The following are the options, environment variables, and descriptions for uploading a backup to Azure using the REST API. The environment variables are recognized by xbcloud , which maps them automatically to the corresponding parameters: Option name Environment variables Description --azure-storage-account=name AZURE_STORAGE_ACCOUNT An Azure storage account is a unique namespace to access and store your Azure data objects. --azure-container-name=name AZURE_CONTAINER_NAME A container name is a valid DNS name that conforms to the Azure naming rules --azure-access-key=name AZURE_ACCESS_KEY A generated key that can be used to authorize access to data in your account using the Shared Key authorization. --azure-endpoint=name AZURE_ENDPOINT The endpoint allows clients to securely access data. --azure-tier-class=name AZURE_STORAGE_CLASS Cloud tier can decrease the local storage required while maintaining the performance. When enabled, this feature has the following categories: * Hot - Frequently accessed or modified data * Cool - Infrequently accessed or modified data * Archive - Rarely accessed or modified data Test your Azure applications with the Azurite open-source emulator . For testing purposes, the xbcloud binary adds the --azure-development-storage option that uses the default access_key and storage account of azurite and testcontainer for the container. You can overwrite these options, if needed.","title":"Options"},{"location":"xbcloud/xbcloud_azure.html#usage","text":"All of the available options for xbcloud , such as parallel, max-retries, and others, can be used. For more information, see The xbcloud Binary .","title":"Usage"},{"location":"xbcloud/xbcloud_azure.html#examples","text":"An example of a xbcloud backup. $ xtrabackup --backup --stream = xbstream --target-dir = $TARGET_DIR | xbcloud put backup_name --azure-storage-account = pxbtesting --azure-access-key = $AZURE_KEY --azure-container-name = test --storage = azure An example of restoring a backup from xbcloud . $ xbcloud get backup_name --azure-storage-account = pxbtesting --azure-access-key = $AZURE_KEY --azure-container-name = test --storage = azure --parallel = 10 2 >download.log | xbstream -x -C restore An example of deleting a backup from xbcloud . $ xbcloud delete backup_name --azure-storage-account = pxbtesting --azure-access-key = $AZURE_KEY --azure-container-name = test --storage = azure An example of using a shortcut restore. $ xbcloud get azure://operator-testing/bak22 ...","title":"Examples"},{"location":"xbcloud/xbcloud_exbackoff.html","text":"Exponential Backoff \u00b6 This feature was implemented in Percona XtraBackup 2.4.24 in the xbcloud binary. Exponential backoff increases the chances for the completion of a backup or a restore operation. For example, a chunk upload or download may fail if you have an unstable network connection or other network issues. This feature adds an exponential backoff, or sleep, time and then retries the upload or download. When a chunk upload or download operation fails, xbcloud checks the reason for the failure. This failure can be a CURL error or an HTTP error, or a client-specific error. If the error is listed in the Retriable errors list, xbcloud pauses for a calculated time before retrying the operation until that time reaches the --max-backoff value. The operation is retried until the --max-retries value is reached. If the chunk operation fails on the last retry, xbcloud aborts the process. The default values are the following: \u2013max-backoff = 300000 (5 minutes) \u2013max-retries = 10 You can adjust the number of retries by adding the --max-retries parameter and adjust the maximum length of time between retries by adding the --max-backoff parameter to an xbcloud command. Since xbcloud does multiple asynchronous requests in parallel, a calculated value, measured in milliseconds, is used for max-backoff . This algorithm calculates how many milliseconds to sleep before the next retry. A number generated is based on the combining the power of two (2), the number of retries already attempted and adds a random number between 1 and 1000. This number avoids network congestion if multiple chunks have the same backoff value. If the default values are used, the final retry attempt to be approximately 17 minutes after the first try. The number is no longer calculated when the milliseconds reach the --max-backoff setting. At that point, the retries continue by using the --max-backoff setting until the max-retries parameter is reached. Retriable errors \u00b6 We retry for the following CURL operations: CURLE_GOT_NOTHING CURLE_OPERATION_TIMEOUT CURLE_RECV_ERROR CURLE_SEND_ERROR CURLE_SEND_FAIL_REWIND CURLE_PARTIAL_FILE CURLE_SSL_CONNECT_ERROR We retry for the following HTTP operation status codes: 503 500 504 408 Each cloud provider may return a different CURL error or an HTTP error, depending on the issue. Add new errors by setting the following variables --curl-retriable-errors or --http-retriable-errors on the command line or in my.cnf or in a custom configuration file under the [xbcloud] section. The error handling is enhanced when using the --verbose output. This output specifies which error caused xbcloud to fail and what parameter a user must add to retry on this error. The following is an example of a verbose output: 210701 14:34:23 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Server returned nothing (no headers, no data) 210701 14:34:23 /work/pxb/ins/2.4/bin/xbcloud: Curl error (52) Server returned nothing (no headers, no data) is not configured as retriable. You can allow it by adding --curl-retriable-errors=52 parameter Example \u00b6 The following example adjusts the maximum number of retries and the maximum time between retries. xbcloud [options] --max-retries=5 --max-backoff=10000 The following text is an example of the exponential backoff used with the command: 210702 10:07:05 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Server returned nothing (no headers, no data) 210702 10:07:05 /work/pxb/ins/2.4/bin/xbcloud: Sleeping for 2384 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [1] . . . 210702 10:07:23 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Server returned nothing (no headers, no data) 210702 10:07:23 /work/pxb/ins/2.4/bin/xbcloud: Sleeping for 4387 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [2] . . . 210702 10:07:52 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Failed sending data to the peer 210702 10:07:52 /work/pxb/ins/2.4/bin/xbcloud: Sleeping for 8691 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [3] . . . 210702 10:08:47 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Failed sending data to the peer 210702 10:08:47 /work/pxb/ins/2.4/bin/xbcloud: Sleeping for 10000 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [4] . . . 210702 10:10:12 /work/pxb/ins/2.4/bin/xbcloud: successfully uploaded chunk: backup3/xtrabackup_logfile.00000000000000000006, size: 8388660 The following list details the example output: [1.] Chunk xtrabackup_logfile.00000000000000000006 fails to upload the first time and slept for 2384 milliseconds. [2.] The same chunk fails for the second time and the time is increased to 4387 milliseconds. [3.] The same chunk fails for the third time and the time is increased to 8691 milliseconds. [4.] The same chunk fails for the fourth time. The max-backoff =10000, which defines the maximum sleep time as 10000. Any retry sleeps the same amount of time after reaching the parameter. [5.] The same chunk is successfully uploaded.","title":"Exponential Backoff"},{"location":"xbcloud/xbcloud_exbackoff.html#exponential-backoff","text":"This feature was implemented in Percona XtraBackup 2.4.24 in the xbcloud binary. Exponential backoff increases the chances for the completion of a backup or a restore operation. For example, a chunk upload or download may fail if you have an unstable network connection or other network issues. This feature adds an exponential backoff, or sleep, time and then retries the upload or download. When a chunk upload or download operation fails, xbcloud checks the reason for the failure. This failure can be a CURL error or an HTTP error, or a client-specific error. If the error is listed in the Retriable errors list, xbcloud pauses for a calculated time before retrying the operation until that time reaches the --max-backoff value. The operation is retried until the --max-retries value is reached. If the chunk operation fails on the last retry, xbcloud aborts the process. The default values are the following: \u2013max-backoff = 300000 (5 minutes) \u2013max-retries = 10 You can adjust the number of retries by adding the --max-retries parameter and adjust the maximum length of time between retries by adding the --max-backoff parameter to an xbcloud command. Since xbcloud does multiple asynchronous requests in parallel, a calculated value, measured in milliseconds, is used for max-backoff . This algorithm calculates how many milliseconds to sleep before the next retry. A number generated is based on the combining the power of two (2), the number of retries already attempted and adds a random number between 1 and 1000. This number avoids network congestion if multiple chunks have the same backoff value. If the default values are used, the final retry attempt to be approximately 17 minutes after the first try. The number is no longer calculated when the milliseconds reach the --max-backoff setting. At that point, the retries continue by using the --max-backoff setting until the max-retries parameter is reached.","title":"Exponential Backoff"},{"location":"xbcloud/xbcloud_exbackoff.html#retriable-errors","text":"We retry for the following CURL operations: CURLE_GOT_NOTHING CURLE_OPERATION_TIMEOUT CURLE_RECV_ERROR CURLE_SEND_ERROR CURLE_SEND_FAIL_REWIND CURLE_PARTIAL_FILE CURLE_SSL_CONNECT_ERROR We retry for the following HTTP operation status codes: 503 500 504 408 Each cloud provider may return a different CURL error or an HTTP error, depending on the issue. Add new errors by setting the following variables --curl-retriable-errors or --http-retriable-errors on the command line or in my.cnf or in a custom configuration file under the [xbcloud] section. The error handling is enhanced when using the --verbose output. This output specifies which error caused xbcloud to fail and what parameter a user must add to retry on this error. The following is an example of a verbose output: 210701 14:34:23 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Server returned nothing (no headers, no data) 210701 14:34:23 /work/pxb/ins/2.4/bin/xbcloud: Curl error (52) Server returned nothing (no headers, no data) is not configured as retriable. You can allow it by adding --curl-retriable-errors=52 parameter","title":"Retriable errors"},{"location":"xbcloud/xbcloud_exbackoff.html#example","text":"The following example adjusts the maximum number of retries and the maximum time between retries. xbcloud [options] --max-retries=5 --max-backoff=10000 The following text is an example of the exponential backoff used with the command: 210702 10:07:05 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Server returned nothing (no headers, no data) 210702 10:07:05 /work/pxb/ins/2.4/bin/xbcloud: Sleeping for 2384 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [1] . . . 210702 10:07:23 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Server returned nothing (no headers, no data) 210702 10:07:23 /work/pxb/ins/2.4/bin/xbcloud: Sleeping for 4387 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [2] . . . 210702 10:07:52 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Failed sending data to the peer 210702 10:07:52 /work/pxb/ins/2.4/bin/xbcloud: Sleeping for 8691 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [3] . . . 210702 10:08:47 /work/pxb/ins/2.4/bin/xbcloud: Operation failed. Error: Failed sending data to the peer 210702 10:08:47 /work/pxb/ins/2.4/bin/xbcloud: Sleeping for 10000 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [4] . . . 210702 10:10:12 /work/pxb/ins/2.4/bin/xbcloud: successfully uploaded chunk: backup3/xtrabackup_logfile.00000000000000000006, size: 8388660 The following list details the example output: [1.] Chunk xtrabackup_logfile.00000000000000000006 fails to upload the first time and slept for 2384 milliseconds. [2.] The same chunk fails for the second time and the time is increased to 4387 milliseconds. [3.] The same chunk fails for the third time and the time is increased to 8691 milliseconds. [4.] The same chunk fails for the fourth time. The max-backoff =10000, which defines the maximum sleep time as 10000. Any retry sleeps the same amount of time after reaching the parameter. [5.] The same chunk is successfully uploaded.","title":"Example"},{"location":"xbcrypt/xbcrypt.html","text":"The xbcrypt binary \u00b6 To support encryption and decryption of the backups, a new tool xbcrypt was introduced to Percona XtraBackup . Percona XtraBackup 2.4.25 implements the XBCRYPT_ENCRYPTION_KEY environment variable. The variable is only used in place of the --encrypt_key=name option. You can use the environment variable or command line option. If you use both, the command line option takes precedence over the value specified in the environment variable. This utility has been modeled after The xbstream binary to perform encryption and decryption outside of Percona XtraBackup . xbcrypt has following command line options: -d, \u2013decrypt \u00b6 Decrypt data input to output. -i, \u2013input=name \u00b6 Optional input file. If not specified, input will be read from standard input. -o, \u2013output=name \u00b6 Optional output file. If not specified, output will be written to standard output. -a, \u2013encrypt-algo=name \u00b6 Encryption algorithm. -k, \u2013encrypt-key=name \u00b6 Encryption key. -f, \u2013encrypt-key-file=name \u00b6 File which contains encryption key. -s, \u2013encrypt-chunk-size= \u00b6 Size of working buffer for encryption in bytes. The default value is 64K. \u2013encrypt-threads= \u00b6 This option specifies the number of worker threads that will be used for parallel encryption/decryption. -v, \u2013verbose \u00b6 Display verbose status output.","title":"The xbcrypt binary"},{"location":"xbcrypt/xbcrypt.html#the-xbcrypt-binary","text":"To support encryption and decryption of the backups, a new tool xbcrypt was introduced to Percona XtraBackup . Percona XtraBackup 2.4.25 implements the XBCRYPT_ENCRYPTION_KEY environment variable. The variable is only used in place of the --encrypt_key=name option. You can use the environment variable or command line option. If you use both, the command line option takes precedence over the value specified in the environment variable. This utility has been modeled after The xbstream binary to perform encryption and decryption outside of Percona XtraBackup . xbcrypt has following command line options:","title":"The xbcrypt binary"},{"location":"xbcrypt/xbcrypt.html#-d-decrypt","text":"Decrypt data input to output.","title":"-d, --decrypt"},{"location":"xbcrypt/xbcrypt.html#-i-inputname","text":"Optional input file. If not specified, input will be read from standard input.","title":"-i, --input=name"},{"location":"xbcrypt/xbcrypt.html#-o-outputname","text":"Optional output file. If not specified, output will be written to standard output.","title":"-o, --output=name"},{"location":"xbcrypt/xbcrypt.html#-a-encrypt-algoname","text":"Encryption algorithm.","title":"-a, --encrypt-algo=name"},{"location":"xbcrypt/xbcrypt.html#-k-encrypt-keyname","text":"Encryption key.","title":"-k, --encrypt-key=name"},{"location":"xbcrypt/xbcrypt.html#-f-encrypt-key-filename","text":"File which contains encryption key.","title":"-f, --encrypt-key-file=name"},{"location":"xbcrypt/xbcrypt.html#-s-encrypt-chunk-size","text":"Size of working buffer for encryption in bytes. The default value is 64K.","title":"-s, --encrypt-chunk-size="},{"location":"xbcrypt/xbcrypt.html#-encrypt-threads","text":"This option specifies the number of worker threads that will be used for parallel encryption/decryption.","title":"--encrypt-threads="},{"location":"xbcrypt/xbcrypt.html#-v-verbose","text":"Display verbose status output.","title":"-v, --verbose"},{"location":"xbstream/xbstream.html","text":"The xbstream binary \u00b6 To support simultaneous compression and streaming, a new custom streaming format called xbstream was introduced to Percona XtraBackup in addition to the TAR format. That was required to overcome some limitations of traditional archive formats such as tar, cpio and others which did not allow streaming dynamically generated files, for example dynamically compressed files. Other advantages of xbstream over traditional streaming/archive format include ability to stream multiple files concurrently (so it is possible to use streaming in the xbstream format together with the \u2013parallel option) and more compact data storage. This utility has a tar-like interface: with the -x option it extracts files from the stream read from its standard input to the current directory unless specified otherwise with the -c option. Support for parallel extraction with the --parallel option has been implemented in Percona XtraBackup 2.4.7. with the -c option it streams files specified on the command line to its standard output. with the --decrypt=ALGO option specified xbstream will automatically decrypt encrypted files when extracting input stream. Supported values for this option are: AES128 , AES192 , and AES256 . Either --encrypt-key or --encrypt-key-file options must be specified to provide encryption key, but not both. This option has been implemented in Percona XtraBackup 2.4.7. with the --encrypt-threads option you can specify the number of threads for parallel data encryption. The default value is 1 . This option has been implemented in Percona XtraBackup 2.4.7. the --encrypt-key option is used to specify the encryption key that will be used. It can\u2019t be used with --encrypt-key-file option because they are mutually exclusive. This option has been implemented in Percona XtraBackup 2.4.7. the --encrypt-key-file option is used to specify the file that contains the encryption key. It can\u2019t be used with --encrypt-key option because they are mutually exclusive. This option has been implemented in Percona XtraBackup 2.4.7. The utility also tries to minimize its impact on the OS page cache by using the appropriate posix_fadvise() calls when available. When compression is enabled with xtrabackup all data is being compressed, including the transaction log file and meta data files, using the specified compression algorithm. The only currently supported algorithm is quicklz . The resulting files have the qpress archive format, i.e., every \\*.qp file produced by xtrabackup is essentially a one-file qpress archive and can be extracted and uncompressed by the qpress file archiver . This means that there is no need to decompress entire backup to restore a single table as with tar.gz . Files can be decompressed using the qpress tool that can be downloaded from here . Qpress supports multi-threaded decompression. The default size of the chunk file is 10MB. Note Use --read-buffer-size to adjust the chunk size. If you use encryption, specify both the --read-buffer-size and --encrypt-chunk-size options to adjust the chunk size.","title":"The xbstream binary"},{"location":"xbstream/xbstream.html#the-xbstream-binary","text":"To support simultaneous compression and streaming, a new custom streaming format called xbstream was introduced to Percona XtraBackup in addition to the TAR format. That was required to overcome some limitations of traditional archive formats such as tar, cpio and others which did not allow streaming dynamically generated files, for example dynamically compressed files. Other advantages of xbstream over traditional streaming/archive format include ability to stream multiple files concurrently (so it is possible to use streaming in the xbstream format together with the \u2013parallel option) and more compact data storage. This utility has a tar-like interface: with the -x option it extracts files from the stream read from its standard input to the current directory unless specified otherwise with the -c option. Support for parallel extraction with the --parallel option has been implemented in Percona XtraBackup 2.4.7. with the -c option it streams files specified on the command line to its standard output. with the --decrypt=ALGO option specified xbstream will automatically decrypt encrypted files when extracting input stream. Supported values for this option are: AES128 , AES192 , and AES256 . Either --encrypt-key or --encrypt-key-file options must be specified to provide encryption key, but not both. This option has been implemented in Percona XtraBackup 2.4.7. with the --encrypt-threads option you can specify the number of threads for parallel data encryption. The default value is 1 . This option has been implemented in Percona XtraBackup 2.4.7. the --encrypt-key option is used to specify the encryption key that will be used. It can\u2019t be used with --encrypt-key-file option because they are mutually exclusive. This option has been implemented in Percona XtraBackup 2.4.7. the --encrypt-key-file option is used to specify the file that contains the encryption key. It can\u2019t be used with --encrypt-key option because they are mutually exclusive. This option has been implemented in Percona XtraBackup 2.4.7. The utility also tries to minimize its impact on the OS page cache by using the appropriate posix_fadvise() calls when available. When compression is enabled with xtrabackup all data is being compressed, including the transaction log file and meta data files, using the specified compression algorithm. The only currently supported algorithm is quicklz . The resulting files have the qpress archive format, i.e., every \\*.qp file produced by xtrabackup is essentially a one-file qpress archive and can be extracted and uncompressed by the qpress file archiver . This means that there is no need to decompress entire backup to restore a single table as with tar.gz . Files can be decompressed using the qpress tool that can be downloaded from here . Qpress supports multi-threaded decompression. The default size of the chunk file is 10MB. Note Use --read-buffer-size to adjust the chunk size. If you use encryption, specify both the --read-buffer-size and --encrypt-chunk-size options to adjust the chunk size.","title":"The xbstream binary"},{"location":"xtrabackup_bin/analyzing_table_statistics.html","text":"Analyzing Table Statistics \u00b6 The xtrabackup binary can analyze InnoDB data files in read-only mode to give statistics about them. To do this, you should use the xtrabackup --stats option. You can combine this with the xtrabackup --tables option to limit the files to examine. It also uses the xtrabackup --use-memory option. You can perform the analysis on a running server, with some chance of errors due to the data being changed during analysis. Or, you can analyze a backup copy of the database. Either way, to use the statistics feature, you need a clean copy of the database including correctly sized log files, so you need to execute with xtrabackup --prepare twice to use this functionality on a backup. The result of running on a backup might look like the following: <INDEX STATISTICS> table: test/table1, index: PRIMARY, space id: 12, root page 3 estimated statistics in dictionary: key vals: 25265338, leaf pages 497839, size pages 498304 real statistics: level 2 pages: pages=1, data=5395 bytes, data/pages=32% level 1 pages: pages=415, data=6471907 bytes, data/pages=95% leaf pages: recs=25958413, pages=497839, data=7492026403 bytes, data/pages=91% This can be interpreted as follows: The first line simply shows the table and index name and its internal identifiers. If you see an index named GEN_CLUST_INDEX , that is the table\u2019s clustered index, automatically created because you did not explicitly create a PRIMARY KEY . The estimated statistics in dictionary information is similar to the data that\u2019s gathered through ANALYZE TABLE inside of InnoDB to be stored as estimated cardinality statistics and passed to the query optimizer. The real statistics information is the result of scanning the data pages and computing exact information about the index. The level <X> pages : output means that the line shows information about pages at that level in the index tree. The larger <X> is, the farther it is from the leaf pages, which are level 0. The first line is the root page. The leaf pages output shows the leaf pages, of course. This is where the table\u2019s data is stored. The external pages : output (not shown) shows large external pages that hold values too long to fit in the row itself, such as long BLOB and TEXT values. The recs is the real number of records (rows) in leaf pages. The pages is the page count. The data is the total size of the data in the pages, in bytes. The data/pages is calculated as ( data / ( pages * PAGE_SIZE )) * 100%. It will never reach 100% because of space reserved for page headers and footers. Note A more detailed example as a MySQL Performance blog post http://www.mysqlperformanceblog.com/2009/09/14/statistics-of-innodb-tables-and-indexes-available-in-xtrabackup/ Script to Format Output \u00b6 The following script can be used to summarize and tabulate the output of the statistics information: tabulate-xtrabackup-stats.pl #!/usr/bin/env perl use strict; use warnings FATAL => 'all'; my $script_version = \"0.1\"; my $PG_SIZE = 16_384; # InnoDB defaults to 16k pages, change if needed. my ($cur_idx, $cur_tbl); my (%idx_stats, %tbl_stats); my ($max_tbl_len, $max_idx_len) = (0, 0); while ( my $line = <> ) { if ( my ($t, $i) = $line =~ m/table: (.*), index: (.*), space id:/ ) { $t =~ s!/!.!; $cur_tbl = $t; $cur_idx = $i; if ( length($i) > $max_idx_len ) { $max_idx_len = length($i); } if ( length($t) > $max_tbl_len ) { $max_tbl_len = length($t); } } elsif ( my ($kv, $lp, $sp) = $line =~ m/key vals: (\\d+), \\D*(\\d+), \\D*(\\d+)/ ) { @{$idx_stats{$cur_tbl}->{$cur_idx}}{qw(est_kv est_lp est_sp)} = ($kv, $lp, $sp); $tbl_stats{$cur_tbl}->{est_kv} += $kv; $tbl_stats{$cur_tbl}->{est_lp} += $lp; $tbl_stats{$cur_tbl}->{est_sp} += $sp; } elsif ( my ($l, $pages, $bytes) = $line =~ m/(?:level (\\d+)|leaf) pages:.*pages=(\\d+), data=(\\d+) bytes/ ) { $l ||= 0; $idx_stats{$cur_tbl}->{$cur_idx}->{real_pages} += $pages; $idx_stats{$cur_tbl}->{$cur_idx}->{real_bytes} += $bytes; $tbl_stats{$cur_tbl}->{real_pages} += $pages; $tbl_stats{$cur_tbl}->{real_bytes} += $bytes; } } my $hdr_fmt = \"%${max_tbl_len}s %${max_idx_len}s %9s %10s %10s\\n\"; my @headers = qw(TABLE INDEX TOT_PAGES FREE_PAGES PCT_FULL); printf $hdr_fmt, @headers; my $row_fmt = \"%${max_tbl_len}s %${max_idx_len}s %9d %10d %9.1f%%\\n\"; foreach my $t ( sort keys %tbl_stats ) { my $tbl = $tbl_stats{$t}; printf $row_fmt, $t, \"\", $tbl->{est_sp}, $tbl->{est_sp} - $tbl->{real_pages}, $tbl->{real_bytes} / ($tbl->{real_pages} * $PG_SIZE) * 100; foreach my $i ( sort keys %{$idx_stats{$t}} ) { my $idx = $idx_stats{$t}->{$i}; printf $row_fmt, $t, $i, $idx->{est_sp}, $idx->{est_sp} - $idx->{real_pages}, $idx->{real_bytes} / ($idx->{real_pages} * $PG_SIZE) * 100; } } Sample Script Output \u00b6 The output of the above Perl script, when run against the sample shown in the previously mentioned blog post, will appear as follows: TABLE INDEX TOT_PAGES FREE_PAGES PCT_FULL art.link_out104 832383 38561 86.8% art.link_out104 PRIMARY 498304 49 91.9% art.link_out104 domain_id 49600 6230 76.9% art.link_out104 domain_id_2 26495 3339 89.1% art.link_out104 from_message_id 28160 142 96.3% art.link_out104 from_site_id 38848 4874 79.4% art.link_out104 revert_domain 153984 19276 71.4% art.link_out104 site_message 36992 4651 83.4% The columns are the table and index, followed by the total number of pages in that index, the number of pages not actually occupied by data, and the number of bytes of real data as a percentage of the total size of the pages of real data. The first line in the above output, in which the INDEX column is empty, is a summary of the entire table.","title":"Analyzing Table Statistics"},{"location":"xtrabackup_bin/analyzing_table_statistics.html#analyzing-table-statistics","text":"The xtrabackup binary can analyze InnoDB data files in read-only mode to give statistics about them. To do this, you should use the xtrabackup --stats option. You can combine this with the xtrabackup --tables option to limit the files to examine. It also uses the xtrabackup --use-memory option. You can perform the analysis on a running server, with some chance of errors due to the data being changed during analysis. Or, you can analyze a backup copy of the database. Either way, to use the statistics feature, you need a clean copy of the database including correctly sized log files, so you need to execute with xtrabackup --prepare twice to use this functionality on a backup. The result of running on a backup might look like the following: <INDEX STATISTICS> table: test/table1, index: PRIMARY, space id: 12, root page 3 estimated statistics in dictionary: key vals: 25265338, leaf pages 497839, size pages 498304 real statistics: level 2 pages: pages=1, data=5395 bytes, data/pages=32% level 1 pages: pages=415, data=6471907 bytes, data/pages=95% leaf pages: recs=25958413, pages=497839, data=7492026403 bytes, data/pages=91% This can be interpreted as follows: The first line simply shows the table and index name and its internal identifiers. If you see an index named GEN_CLUST_INDEX , that is the table\u2019s clustered index, automatically created because you did not explicitly create a PRIMARY KEY . The estimated statistics in dictionary information is similar to the data that\u2019s gathered through ANALYZE TABLE inside of InnoDB to be stored as estimated cardinality statistics and passed to the query optimizer. The real statistics information is the result of scanning the data pages and computing exact information about the index. The level <X> pages : output means that the line shows information about pages at that level in the index tree. The larger <X> is, the farther it is from the leaf pages, which are level 0. The first line is the root page. The leaf pages output shows the leaf pages, of course. This is where the table\u2019s data is stored. The external pages : output (not shown) shows large external pages that hold values too long to fit in the row itself, such as long BLOB and TEXT values. The recs is the real number of records (rows) in leaf pages. The pages is the page count. The data is the total size of the data in the pages, in bytes. The data/pages is calculated as ( data / ( pages * PAGE_SIZE )) * 100%. It will never reach 100% because of space reserved for page headers and footers. Note A more detailed example as a MySQL Performance blog post http://www.mysqlperformanceblog.com/2009/09/14/statistics-of-innodb-tables-and-indexes-available-in-xtrabackup/","title":"Analyzing Table Statistics"},{"location":"xtrabackup_bin/analyzing_table_statistics.html#script-to-format-output","text":"The following script can be used to summarize and tabulate the output of the statistics information: tabulate-xtrabackup-stats.pl #!/usr/bin/env perl use strict; use warnings FATAL => 'all'; my $script_version = \"0.1\"; my $PG_SIZE = 16_384; # InnoDB defaults to 16k pages, change if needed. my ($cur_idx, $cur_tbl); my (%idx_stats, %tbl_stats); my ($max_tbl_len, $max_idx_len) = (0, 0); while ( my $line = <> ) { if ( my ($t, $i) = $line =~ m/table: (.*), index: (.*), space id:/ ) { $t =~ s!/!.!; $cur_tbl = $t; $cur_idx = $i; if ( length($i) > $max_idx_len ) { $max_idx_len = length($i); } if ( length($t) > $max_tbl_len ) { $max_tbl_len = length($t); } } elsif ( my ($kv, $lp, $sp) = $line =~ m/key vals: (\\d+), \\D*(\\d+), \\D*(\\d+)/ ) { @{$idx_stats{$cur_tbl}->{$cur_idx}}{qw(est_kv est_lp est_sp)} = ($kv, $lp, $sp); $tbl_stats{$cur_tbl}->{est_kv} += $kv; $tbl_stats{$cur_tbl}->{est_lp} += $lp; $tbl_stats{$cur_tbl}->{est_sp} += $sp; } elsif ( my ($l, $pages, $bytes) = $line =~ m/(?:level (\\d+)|leaf) pages:.*pages=(\\d+), data=(\\d+) bytes/ ) { $l ||= 0; $idx_stats{$cur_tbl}->{$cur_idx}->{real_pages} += $pages; $idx_stats{$cur_tbl}->{$cur_idx}->{real_bytes} += $bytes; $tbl_stats{$cur_tbl}->{real_pages} += $pages; $tbl_stats{$cur_tbl}->{real_bytes} += $bytes; } } my $hdr_fmt = \"%${max_tbl_len}s %${max_idx_len}s %9s %10s %10s\\n\"; my @headers = qw(TABLE INDEX TOT_PAGES FREE_PAGES PCT_FULL); printf $hdr_fmt, @headers; my $row_fmt = \"%${max_tbl_len}s %${max_idx_len}s %9d %10d %9.1f%%\\n\"; foreach my $t ( sort keys %tbl_stats ) { my $tbl = $tbl_stats{$t}; printf $row_fmt, $t, \"\", $tbl->{est_sp}, $tbl->{est_sp} - $tbl->{real_pages}, $tbl->{real_bytes} / ($tbl->{real_pages} * $PG_SIZE) * 100; foreach my $i ( sort keys %{$idx_stats{$t}} ) { my $idx = $idx_stats{$t}->{$i}; printf $row_fmt, $t, $i, $idx->{est_sp}, $idx->{est_sp} - $idx->{real_pages}, $idx->{real_bytes} / ($idx->{real_pages} * $PG_SIZE) * 100; } }","title":"Script to Format Output"},{"location":"xtrabackup_bin/analyzing_table_statistics.html#sample-script-output","text":"The output of the above Perl script, when run against the sample shown in the previously mentioned blog post, will appear as follows: TABLE INDEX TOT_PAGES FREE_PAGES PCT_FULL art.link_out104 832383 38561 86.8% art.link_out104 PRIMARY 498304 49 91.9% art.link_out104 domain_id 49600 6230 76.9% art.link_out104 domain_id_2 26495 3339 89.1% art.link_out104 from_message_id 28160 142 96.3% art.link_out104 from_site_id 38848 4874 79.4% art.link_out104 revert_domain 153984 19276 71.4% art.link_out104 site_message 36992 4651 83.4% The columns are the table and index, followed by the total number of pages in that index, the number of pages not actually occupied by data, and the number of bytes of real data as a percentage of the total size of the pages of real data. The first line in the above output, in which the INDEX column is empty, is a summary of the entire table.","title":"Sample Script Output"},{"location":"xtrabackup_bin/implementation_details.html","text":"Implementation Details \u00b6 This page contains notes on various internal aspects of the xtrabackup tool\u2019s operation. File Permissions \u00b6 xtrabackup opens the source data files in read-write mode, although it does not modify the files. This means that you must run xtrabackup as a user who has permission to write the data files. The reason for opening the files in read-write mode is that xtrabackup uses the embedded InnoDB libraries to open and read the files, and InnoDB opens them in read-write mode because it normally assumes it is going to write to them. Tuning the OS Buffers \u00b6 Because xtrabackup reads large amounts of data from the filesystem, it uses posix_fadvise() where possible, to instruct the operating system not to try to cache the blocks it reads from disk. Without this hint, the operating system would prefer to cache the blocks, assuming that xtrabackup is likely to need them again, which is not the case. Caching such large files can place pressure on the operating system\u2019s virtual memory and cause other processes, such as the database server, to be swapped out. The xtrabackup tool avoids this with the following hint on both the source and destination files: posix_fadvise(file, 0, 0, POSIX_FADV_DONTNEED) In addition, xtrabackup asks the operating system to perform more aggressive read-ahead optimizations on the source files: posix_fadvise(file, 0, 0, POSIX_FADV_SEQUENTIAL) Copying Data Files \u00b6 When copying the data files to the target directory, xtrabackup reads and writes 1MB of data at a time. This is not configurable. When copying the log file, xtrabackup reads and writes 512 bytes at a time. This is also not possible to configure, and matches InnoDB\u2019s behavior (workaround exists in Percona Server for MySQL because it has an option to tune innodb_log_block_size for XtraDB, and in that case XtraBackup will match the tuning). After reading from the files, xtrabackup iterates over the 1MB buffer a page at a time, and checks for page corruption on each page with InnoDB\u2019s buf_page_is_corrupted() function. If the page is corrupt, it re-reads and retries up to 10 times for each page. It skips this check on the doublewrite buffer.","title":"Implementation Details"},{"location":"xtrabackup_bin/implementation_details.html#implementation-details","text":"This page contains notes on various internal aspects of the xtrabackup tool\u2019s operation.","title":"Implementation Details"},{"location":"xtrabackup_bin/implementation_details.html#file-permissions","text":"xtrabackup opens the source data files in read-write mode, although it does not modify the files. This means that you must run xtrabackup as a user who has permission to write the data files. The reason for opening the files in read-write mode is that xtrabackup uses the embedded InnoDB libraries to open and read the files, and InnoDB opens them in read-write mode because it normally assumes it is going to write to them.","title":"File Permissions"},{"location":"xtrabackup_bin/implementation_details.html#tuning-the-os-buffers","text":"Because xtrabackup reads large amounts of data from the filesystem, it uses posix_fadvise() where possible, to instruct the operating system not to try to cache the blocks it reads from disk. Without this hint, the operating system would prefer to cache the blocks, assuming that xtrabackup is likely to need them again, which is not the case. Caching such large files can place pressure on the operating system\u2019s virtual memory and cause other processes, such as the database server, to be swapped out. The xtrabackup tool avoids this with the following hint on both the source and destination files: posix_fadvise(file, 0, 0, POSIX_FADV_DONTNEED) In addition, xtrabackup asks the operating system to perform more aggressive read-ahead optimizations on the source files: posix_fadvise(file, 0, 0, POSIX_FADV_SEQUENTIAL)","title":"Tuning the OS Buffers"},{"location":"xtrabackup_bin/implementation_details.html#copying-data-files","text":"When copying the data files to the target directory, xtrabackup reads and writes 1MB of data at a time. This is not configurable. When copying the log file, xtrabackup reads and writes 512 bytes at a time. This is also not possible to configure, and matches InnoDB\u2019s behavior (workaround exists in Percona Server for MySQL because it has an option to tune innodb_log_block_size for XtraDB, and in that case XtraBackup will match the tuning). After reading from the files, xtrabackup iterates over the 1MB buffer a page at a time, and checks for page corruption on each page with InnoDB\u2019s buf_page_is_corrupted() function. If the page is corrupt, it re-reads and retries up to 10 times for each page. It skips this check on the doublewrite buffer.","title":"Copying Data Files"},{"location":"xtrabackup_bin/incremental_backups.html","text":"Incremental Backups \u00b6 Both xtrabackup and innobackupex tools supports incremental backups, which means that it can copy only the data that has changed since the last full backup. You can perform many incremental backups between each full backup, so you can set up a backup process such as a full backup once a week and an incremental backup every day, or full backups every day and incremental backups every hour. Incremental backups work because each InnoDB page (usually 16kb in size) contains a log sequence number, or LSN . The LSN is the system version number for the entire database. Each page\u2019s LSN shows how recently it was changed. An incremental backup copies each page whose LSN is newer than the previous incremental or full backup\u2019s LSN . There are two algorithms in use to find the set of such pages to be copied. The first one, available with all the server types and versions, is to check the page LSN directly by reading all the data pages. The second one, available with Percona Server for MySQL , is to enable the changed page tracking feature on the server, which will note the pages as they are being changed. This information will be then written out in a compact separate so-called bitmap file. The xtrabackup binary will use that file to read only the data pages it needs for the incremental backup, potentially saving many read requests. The latter algorithm is enabled by default if the xtrabackup binary finds the bitmap file. It is possible to specify xtrabackup --incremental-force-scan to read all the pages even if the bitmap data is available. Incremental backups do not actually compare the data files to the previous backup\u2019s data files. In fact, you can use xtrabackup \u2013incremental-lsn to perform an incremental backup without even having the previous backup, if you know its LSN . Incremental backups simply read the pages and compare their LSN to the last backup\u2019s LSN . You still need a full backup to recover the incremental changes, however; without a full backup to act as a base, the incremental backups are useless. Creating an Incremental Backup \u00b6 To make an incremental backup, begin with a full backup as usual. The xtrabackup binary writes a file called xtrabackup_checkpoints into the backup\u2019s target directory. This file contains a line showing the to_lsn , which is the database\u2019s LSN at the end of the backup. Create the full backup with a command such as the following: xtrabackup --backup --target-dir = /data/backups/base --datadir = /var/lib/mysql/ If you look at the xtrabackup_checkpoints file, you should see some contents similar to the following: backup_type = full-backuped from_lsn = 0 to_lsn = 1291135 Now that you have a full backup, you can make an incremental backup based on it. Use a command such as the following: $ xtrabackup --backup --target-dir = /data/backups/inc1 \\ --incremental-basedir = /data/backups/base --datadir = /var/lib/mysql/ The /data/backups/inc1/ directory should now contain delta files, such as ibdata1.delta and test/table1.ibd.delta . These represent the changes since the LSN 1291135 . If you examine the xtrabackup_checkpoints file in this directory, you should see something similar to the following: backup_type = incremental from_lsn = 1291135 to_lsn = 1291340 The meaning should be self-evident. It\u2019s now possible to use this directory as the base for yet another incremental backup: $ xtrabackup --backup --target-dir = /data/backups/inc2 \\ --incremental-basedir = /data/backups/inc1 --datadir = /var/lib/mysql/ Preparing the Incremental Backups \u00b6 The xtrabackup --prepare step for incremental backups is not the same as for normal backups. In normal backups, two types of operations are performed to make the database consistent: committed transactions are replayed from the log file against the data files, and uncommitted transactions are rolled back. You must skip the rollback of uncommitted transactions when preparing a backup, because transactions that were uncommitted at the time of your backup may be in progress, and it\u2019s likely that they will be committed in the next incremental backup. You should use the xtrabackup --apply-log-only option to prevent the rollback phase. Warning If you do not use the xtrabackup \u2013apply-log-only option to prevent the rollback phase, then your incremental backups will be useless. After transactions have been rolled back, further incremental backups cannot be applied. Beginning with the full backup you created, you can prepare it, and then apply the incremental differences to it. Recall that you have the following backups: /data/backups/base /data/backups/inc1 /data/backups/inc2 To prepare the base backup, you need to run xtrabackup \u2013prepare as usual, but prevent the rollback phase: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base The output should end with some text such as the following: 101107 20:49:43 InnoDB: Shutdown completed; log sequence number 1291135 The log sequence number should match the to_lsn of the base backup, which you saw previously. This backup is actually safe to restore as-is now, even though the rollback phase has been skipped. If you restore it and start MySQL , InnoDB will detect that the rollback phase was not performed, and it will do that in the background, as it usually does for a crash recovery upon start. It will notify you that the database was not shut down normally. To apply the first incremental backup to the full backup, you should use the following command: $ xtrabackup --prepare --apply-log-only --target-dir=/data/backups/base \\ --incremental-dir=/data/backups/inc1 This applies the delta files to the files in /data/backups/base , which rolls them forward in time to the time of the incremental backup. It then applies the redo log as usual to the result. The final data is in /data/backups/base , not in the incremental directory. You should see some output such as the following: incremental backup from 1291135 is enabled. xtrabackup: cd to /data/backups/base/ xtrabackup: This target seems to be already prepared. xtrabackup: xtrabackup_logfile detected: size=2097152, start_lsn=(1291340) Applying /data/backups/inc1/ibdata1.delta ... Applying /data/backups/inc1/test/table1.ibd.delta ... .... snip 101107 20:56:30 InnoDB: Shutdown completed; log sequence number 1291340 Again, the LSN should match what you saw from your earlier inspection of the first incremental backup. If you restore the files from /data/backups/base , you should see the state of the database as of the first incremental backup. Preparing the second incremental backup is a similar process: apply the deltas to the (modified) base backup, and you will roll its data forward in time to the point of the second incremental backup: $ xtrabackup --prepare --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc2 Note xtrabackup --apply-log-only should be used when merging all incrementals except the last one. That\u2019s why the previous line doesn\u2019t contain the xtrabackup --apply-log-only option. Even if the xtrabackup --apply-log-only was used on the last step, backup would still be consistent but in that case server would perform the rollback phase. If you wish to avoid the notice that InnoDB was not shut down normally, when xoyou applied the desired deltas to the base backup, you can run xtrabackup --prepare again without disabling the rollback phase.","title":"Incremental Backups"},{"location":"xtrabackup_bin/incremental_backups.html#incremental-backups","text":"Both xtrabackup and innobackupex tools supports incremental backups, which means that it can copy only the data that has changed since the last full backup. You can perform many incremental backups between each full backup, so you can set up a backup process such as a full backup once a week and an incremental backup every day, or full backups every day and incremental backups every hour. Incremental backups work because each InnoDB page (usually 16kb in size) contains a log sequence number, or LSN . The LSN is the system version number for the entire database. Each page\u2019s LSN shows how recently it was changed. An incremental backup copies each page whose LSN is newer than the previous incremental or full backup\u2019s LSN . There are two algorithms in use to find the set of such pages to be copied. The first one, available with all the server types and versions, is to check the page LSN directly by reading all the data pages. The second one, available with Percona Server for MySQL , is to enable the changed page tracking feature on the server, which will note the pages as they are being changed. This information will be then written out in a compact separate so-called bitmap file. The xtrabackup binary will use that file to read only the data pages it needs for the incremental backup, potentially saving many read requests. The latter algorithm is enabled by default if the xtrabackup binary finds the bitmap file. It is possible to specify xtrabackup --incremental-force-scan to read all the pages even if the bitmap data is available. Incremental backups do not actually compare the data files to the previous backup\u2019s data files. In fact, you can use xtrabackup \u2013incremental-lsn to perform an incremental backup without even having the previous backup, if you know its LSN . Incremental backups simply read the pages and compare their LSN to the last backup\u2019s LSN . You still need a full backup to recover the incremental changes, however; without a full backup to act as a base, the incremental backups are useless.","title":"Incremental Backups"},{"location":"xtrabackup_bin/incremental_backups.html#creating-an-incremental-backup","text":"To make an incremental backup, begin with a full backup as usual. The xtrabackup binary writes a file called xtrabackup_checkpoints into the backup\u2019s target directory. This file contains a line showing the to_lsn , which is the database\u2019s LSN at the end of the backup. Create the full backup with a command such as the following: xtrabackup --backup --target-dir = /data/backups/base --datadir = /var/lib/mysql/ If you look at the xtrabackup_checkpoints file, you should see some contents similar to the following: backup_type = full-backuped from_lsn = 0 to_lsn = 1291135 Now that you have a full backup, you can make an incremental backup based on it. Use a command such as the following: $ xtrabackup --backup --target-dir = /data/backups/inc1 \\ --incremental-basedir = /data/backups/base --datadir = /var/lib/mysql/ The /data/backups/inc1/ directory should now contain delta files, such as ibdata1.delta and test/table1.ibd.delta . These represent the changes since the LSN 1291135 . If you examine the xtrabackup_checkpoints file in this directory, you should see something similar to the following: backup_type = incremental from_lsn = 1291135 to_lsn = 1291340 The meaning should be self-evident. It\u2019s now possible to use this directory as the base for yet another incremental backup: $ xtrabackup --backup --target-dir = /data/backups/inc2 \\ --incremental-basedir = /data/backups/inc1 --datadir = /var/lib/mysql/","title":"Creating an Incremental Backup"},{"location":"xtrabackup_bin/incremental_backups.html#preparing-the-incremental-backups","text":"The xtrabackup --prepare step for incremental backups is not the same as for normal backups. In normal backups, two types of operations are performed to make the database consistent: committed transactions are replayed from the log file against the data files, and uncommitted transactions are rolled back. You must skip the rollback of uncommitted transactions when preparing a backup, because transactions that were uncommitted at the time of your backup may be in progress, and it\u2019s likely that they will be committed in the next incremental backup. You should use the xtrabackup --apply-log-only option to prevent the rollback phase. Warning If you do not use the xtrabackup \u2013apply-log-only option to prevent the rollback phase, then your incremental backups will be useless. After transactions have been rolled back, further incremental backups cannot be applied. Beginning with the full backup you created, you can prepare it, and then apply the incremental differences to it. Recall that you have the following backups: /data/backups/base /data/backups/inc1 /data/backups/inc2 To prepare the base backup, you need to run xtrabackup \u2013prepare as usual, but prevent the rollback phase: $ xtrabackup --prepare --apply-log-only --target-dir = /data/backups/base The output should end with some text such as the following: 101107 20:49:43 InnoDB: Shutdown completed; log sequence number 1291135 The log sequence number should match the to_lsn of the base backup, which you saw previously. This backup is actually safe to restore as-is now, even though the rollback phase has been skipped. If you restore it and start MySQL , InnoDB will detect that the rollback phase was not performed, and it will do that in the background, as it usually does for a crash recovery upon start. It will notify you that the database was not shut down normally. To apply the first incremental backup to the full backup, you should use the following command: $ xtrabackup --prepare --apply-log-only --target-dir=/data/backups/base \\ --incremental-dir=/data/backups/inc1 This applies the delta files to the files in /data/backups/base , which rolls them forward in time to the time of the incremental backup. It then applies the redo log as usual to the result. The final data is in /data/backups/base , not in the incremental directory. You should see some output such as the following: incremental backup from 1291135 is enabled. xtrabackup: cd to /data/backups/base/ xtrabackup: This target seems to be already prepared. xtrabackup: xtrabackup_logfile detected: size=2097152, start_lsn=(1291340) Applying /data/backups/inc1/ibdata1.delta ... Applying /data/backups/inc1/test/table1.ibd.delta ... .... snip 101107 20:56:30 InnoDB: Shutdown completed; log sequence number 1291340 Again, the LSN should match what you saw from your earlier inspection of the first incremental backup. If you restore the files from /data/backups/base , you should see the state of the database as of the first incremental backup. Preparing the second incremental backup is a similar process: apply the deltas to the (modified) base backup, and you will roll its data forward in time to the point of the second incremental backup: $ xtrabackup --prepare --target-dir = /data/backups/base \\ --incremental-dir = /data/backups/inc2 Note xtrabackup --apply-log-only should be used when merging all incrementals except the last one. That\u2019s why the previous line doesn\u2019t contain the xtrabackup --apply-log-only option. Even if the xtrabackup --apply-log-only was used on the last step, backup would still be consistent but in that case server would perform the rollback phase. If you wish to avoid the notice that InnoDB was not shut down normally, when xoyou applied the desired deltas to the base backup, you can run xtrabackup --prepare again without disabling the rollback phase.","title":"Preparing the Incremental Backups"},{"location":"xtrabackup_bin/lru_dump.html","text":"LRU dump backup \u00b6 This feature reduces the warm up time by restoring buffer pool state from ib_lru_dump file after restart. Percona XtraBackup discovers ib_lru_dump and backs it up automatically. If the buffer restore option is enabled in my.cnf buffer pool will be in the warm state after backup is restored. To enable this set the variable innodb_buffer_pool_restore_at_startup =1 in Percona Server 5.5 or innodb_auto_lru_dump =1 in Percona Server 5.1.","title":"LRU dump backup"},{"location":"xtrabackup_bin/lru_dump.html#lru-dump-backup","text":"This feature reduces the warm up time by restoring buffer pool state from ib_lru_dump file after restart. Percona XtraBackup discovers ib_lru_dump and backs it up automatically. If the buffer restore option is enabled in my.cnf buffer pool will be in the warm state after backup is restored. To enable this set the variable innodb_buffer_pool_restore_at_startup =1 in Percona Server 5.5 or innodb_auto_lru_dump =1 in Percona Server 5.1.","title":"LRU dump backup"},{"location":"xtrabackup_bin/partial_backups.html","text":"Partial Backups \u00b6 xtrabackup supports taking partial backups when the innodb_file_per_table option is enabled. There are three ways to create partial backups: matching the tables names with a regular expression providing a list of table names in a file providing a list of databases Warning If any of the matched or listed tables is deleted during the backup, xtrabackup will fail. Do not copy back the prepared backup. Restoring partial backups should be done by importing the tables, not by using the --copy-back option. It is not recommended to run incremental backups after running a partial backup. Although there are some scenarios where restoring can be done by copying back the files, this may lead to database inconsistencies in many cases and it is not a recommended way to do it. For the purposes of this manual page, we will assume that there is a database named test which contains tables named t1 and t2 . Using xtrabackup \u2013tables \u00b6 The first method involves the xtrabackup --tables option. The option\u2019s value is a regular expression that is matched against the fully qualified tablename, including the database name, in the form databasename.tablename . To back up only tables in the test database, you can use the following command: $ xtrabackup --backup --datadir = /var/lib/mysql --target-dir = /data/backups/ \\ --tables = \"^test[.].*\" To back up only the table test.t1 , you can use the following command: $ xtrabackup --backup --datadir=/var/lib/mysql --target-dir=/data/backups/ \\ --tables=\"^test[.]t1\" Using xtrabackup --tables-file \u00b6 xtrabackup --tables-file specifies a file that can contain multiple table names, one table name per line in the file. Only the tables named in the file will be backed up. Names are matched exactly, case-sensitive, with no pattern or regular expression matching. The table names must be fully qualified, in databasename.tablename format. $ echo \"mydatabase.mytable\" > /tmp/tables.txt $ xtrabackup --backup --tables-file = /tmp/tables.txt Using xtrabackup --databases and xtrabackup --databases-file \u00b6 xtrabackup --databases accepts a space-separated list of the databases and tables to backup in the format databasename[.tablename] . In addition to this list make sure to specify the mysql , sys , and performance_schema databases. These databases are required when restoring the databases using xtrabackup --copy-back . Note Tables processed during the \u2013prepare step may also be added to the backup even if they are not explicitly listed by the parameter if they were created after the backup started. $ xtrabackup --databases = 'mysql sys performance_schema ...' xtrabackup --databases-file specifies a file that can contain multiple databases and tables in the databasename[.tablename] form, one element name per line in the file. Names are matched exactly, case-sensitive, with no pattern or regular expression matching. Note Tables processed during the \u2013prepare step may also be added to the backup even if they are not explicitly listed by the parameter if they were created after the backup started. Preparing the Backup \u00b6 When you use the xtrabackup --prepare option on a partial backup, you will see warnings about tables that don\u2019t exist. This is because these tables exist in the data dictionary inside InnoDB, but the corresponding .ibd files don\u2019t exist. They were not copied into the backup directory. These tables will be removed from the data dictionary, and when you restore the backup and start InnoDB, they will no longer exist and will not cause any errors or warnings to be printed to the log file. An example of the error message you will see during the prepare phase follows. InnoDB: Reading tablespace information from the .ibd files... 101107 22:31:30 InnoDB: Error: table 'test1/t' InnoDB: in InnoDB data dictionary has tablespace id 6, InnoDB: but tablespace with that id or name does not exist. It will be removed from data dictionary.","title":"Partial Backups"},{"location":"xtrabackup_bin/partial_backups.html#partial-backups","text":"xtrabackup supports taking partial backups when the innodb_file_per_table option is enabled. There are three ways to create partial backups: matching the tables names with a regular expression providing a list of table names in a file providing a list of databases Warning If any of the matched or listed tables is deleted during the backup, xtrabackup will fail. Do not copy back the prepared backup. Restoring partial backups should be done by importing the tables, not by using the --copy-back option. It is not recommended to run incremental backups after running a partial backup. Although there are some scenarios where restoring can be done by copying back the files, this may lead to database inconsistencies in many cases and it is not a recommended way to do it. For the purposes of this manual page, we will assume that there is a database named test which contains tables named t1 and t2 .","title":"Partial Backups"},{"location":"xtrabackup_bin/partial_backups.html#using-xtrabackup-tables","text":"The first method involves the xtrabackup --tables option. The option\u2019s value is a regular expression that is matched against the fully qualified tablename, including the database name, in the form databasename.tablename . To back up only tables in the test database, you can use the following command: $ xtrabackup --backup --datadir = /var/lib/mysql --target-dir = /data/backups/ \\ --tables = \"^test[.].*\" To back up only the table test.t1 , you can use the following command: $ xtrabackup --backup --datadir=/var/lib/mysql --target-dir=/data/backups/ \\ --tables=\"^test[.]t1\"","title":"Using xtrabackup \u2013tables"},{"location":"xtrabackup_bin/partial_backups.html#using-xtrabackup-tables-file","text":"xtrabackup --tables-file specifies a file that can contain multiple table names, one table name per line in the file. Only the tables named in the file will be backed up. Names are matched exactly, case-sensitive, with no pattern or regular expression matching. The table names must be fully qualified, in databasename.tablename format. $ echo \"mydatabase.mytable\" > /tmp/tables.txt $ xtrabackup --backup --tables-file = /tmp/tables.txt","title":"Using xtrabackup --tables-file"},{"location":"xtrabackup_bin/partial_backups.html#using-xtrabackup-databases-and-xtrabackup-databases-file","text":"xtrabackup --databases accepts a space-separated list of the databases and tables to backup in the format databasename[.tablename] . In addition to this list make sure to specify the mysql , sys , and performance_schema databases. These databases are required when restoring the databases using xtrabackup --copy-back . Note Tables processed during the \u2013prepare step may also be added to the backup even if they are not explicitly listed by the parameter if they were created after the backup started. $ xtrabackup --databases = 'mysql sys performance_schema ...' xtrabackup --databases-file specifies a file that can contain multiple databases and tables in the databasename[.tablename] form, one element name per line in the file. Names are matched exactly, case-sensitive, with no pattern or regular expression matching. Note Tables processed during the \u2013prepare step may also be added to the backup even if they are not explicitly listed by the parameter if they were created after the backup started.","title":"Using xtrabackup --databases and xtrabackup --databases-file"},{"location":"xtrabackup_bin/partial_backups.html#preparing-the-backup","text":"When you use the xtrabackup --prepare option on a partial backup, you will see warnings about tables that don\u2019t exist. This is because these tables exist in the data dictionary inside InnoDB, but the corresponding .ibd files don\u2019t exist. They were not copied into the backup directory. These tables will be removed from the data dictionary, and when you restore the backup and start InnoDB, they will no longer exist and will not cause any errors or warnings to be printed to the log file. An example of the error message you will see during the prepare phase follows. InnoDB: Reading tablespace information from the .ibd files... 101107 22:31:30 InnoDB: Error: table 'test1/t' InnoDB: in InnoDB data dictionary has tablespace id 6, InnoDB: but tablespace with that id or name does not exist. It will be removed from data dictionary.","title":"Preparing the Backup"},{"location":"xtrabackup_bin/restoring_individual_tables.html","text":"Restoring Individual Tables \u00b6 With Percona XtraBackup , you can export individual tables from any InnoDB database, and import them into Percona Server for MySQL with XtraDB or MySQL 5.7. The source does not need to be XtraDB or MySQL 5.7 but the destination must be. This operation only works on individual .ibd files. A table that is not contained in its own .ibd file cannot be exported. Let\u2019s see how to export and import the following table: CREATE TABLE export_test ( a int ( 11 ) DEFAULT NULL ) ENGINE = InnoDB DEFAULT CHARSET = latin1 ; Exporting the Table \u00b6 This table should have been created in innodb_file_per_table mode, so after taking a backup as usual with xtrabackup --backup , the .ibd file should exist in the target directory: $ find /data/backups/mysql/ -name export_test.* /data/backups/mysql/test/export_test.ibd when you prepare the backup, add the extra parameter xtrabackup --export to the command. Here is an example: $ xtrabackup --prepare --export --target-dir = /data/backups/mysql/ Note If you\u2019re trying to restore encrypted InnoDB tablespace table you must specify the keyring file as well: xtrabackup --prepare --export --target-dir = /tmp/table \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Now you should see a .exp file in the target directory: $ find /data/backups/mysql/ -name export_test.* The result is similar to the following: /data/backups/mysql/test/export_test.exp /data/backups/mysql/test/export_test.ibd /data/backups/mysql/test/export_test.cfg These three files are all you need to import the table into a server running Percona Server for MySQL with XtraDB or MySQL 5.7. In case server is using InnoDB Tablespace Encryption additional .cfp file be listed for encrypted tables. Note MySQL uses .cfg file which contains InnoDB dictionary dump in special format. This format is different from the .exp\\ one which is used in XtraDB for the same purpose. Strictly speaking, a .cfg\\ file is not required to import a tablespace to MySQL 5.7 or Percona Server for MySQL 5.7. A tablespace will be imported successfully even if it is from another server, but InnoDB will do schema validation if the corresponding .cfg file is present in the same directory. Importing the Table \u00b6 On the destination server, create a table with the same structure, and then perform the following steps: Execute ALTER TABLE test.export_test DISCARD TABLESPACE; If you see the ERROR 1030 (HY000): Got error -1 from storage engine message, then enable innodb_file_per_table and create the table again: Copy the exported files to the test/ subdirectory of the destination server\u2019s data directory Execute ALTER TABLE test.export_test IMPORT TABLESPACE; The table should now be imported, and you should be able to SELECT from it and see the imported data. Note Persistent statistics for imported tablespace will be empty until you run the ANALYZE TABLE on the imported table. They are empty because they are stored in the system tables mysql.innodb_table_stats and mysql.innodb_index_stats and they are not updated by server during the import. This is due to upstream bug #72368 .","title":"Restoring Individual Tables"},{"location":"xtrabackup_bin/restoring_individual_tables.html#restoring-individual-tables","text":"With Percona XtraBackup , you can export individual tables from any InnoDB database, and import them into Percona Server for MySQL with XtraDB or MySQL 5.7. The source does not need to be XtraDB or MySQL 5.7 but the destination must be. This operation only works on individual .ibd files. A table that is not contained in its own .ibd file cannot be exported. Let\u2019s see how to export and import the following table: CREATE TABLE export_test ( a int ( 11 ) DEFAULT NULL ) ENGINE = InnoDB DEFAULT CHARSET = latin1 ;","title":"Restoring Individual Tables"},{"location":"xtrabackup_bin/restoring_individual_tables.html#exporting-the-table","text":"This table should have been created in innodb_file_per_table mode, so after taking a backup as usual with xtrabackup --backup , the .ibd file should exist in the target directory: $ find /data/backups/mysql/ -name export_test.* /data/backups/mysql/test/export_test.ibd when you prepare the backup, add the extra parameter xtrabackup --export to the command. Here is an example: $ xtrabackup --prepare --export --target-dir = /data/backups/mysql/ Note If you\u2019re trying to restore encrypted InnoDB tablespace table you must specify the keyring file as well: xtrabackup --prepare --export --target-dir = /tmp/table \\ --keyring-file-data = /var/lib/mysql-keyring/keyring Now you should see a .exp file in the target directory: $ find /data/backups/mysql/ -name export_test.* The result is similar to the following: /data/backups/mysql/test/export_test.exp /data/backups/mysql/test/export_test.ibd /data/backups/mysql/test/export_test.cfg These three files are all you need to import the table into a server running Percona Server for MySQL with XtraDB or MySQL 5.7. In case server is using InnoDB Tablespace Encryption additional .cfp file be listed for encrypted tables. Note MySQL uses .cfg file which contains InnoDB dictionary dump in special format. This format is different from the .exp\\ one which is used in XtraDB for the same purpose. Strictly speaking, a .cfg\\ file is not required to import a tablespace to MySQL 5.7 or Percona Server for MySQL 5.7. A tablespace will be imported successfully even if it is from another server, but InnoDB will do schema validation if the corresponding .cfg file is present in the same directory.","title":"Exporting the Table"},{"location":"xtrabackup_bin/restoring_individual_tables.html#importing-the-table","text":"On the destination server, create a table with the same structure, and then perform the following steps: Execute ALTER TABLE test.export_test DISCARD TABLESPACE; If you see the ERROR 1030 (HY000): Got error -1 from storage engine message, then enable innodb_file_per_table and create the table again: Copy the exported files to the test/ subdirectory of the destination server\u2019s data directory Execute ALTER TABLE test.export_test IMPORT TABLESPACE; The table should now be imported, and you should be able to SELECT from it and see the imported data. Note Persistent statistics for imported tablespace will be empty until you run the ANALYZE TABLE on the imported table. They are empty because they are stored in the system tables mysql.innodb_table_stats and mysql.innodb_index_stats and they are not updated by server during the import. This is due to upstream bug #72368 .","title":"Importing the Table"},{"location":"xtrabackup_bin/scripting_backups_xbk.html","text":"Scripting Backups With xtrabackup \u00b6 The xtrabackup tool has several features to enable scripts to control it while they perform related tasks. The innobackupex script is one example, but xtrabackup is easy to control with your own command-line scripts too. Suspending After Copying \u00b6 In backup mode, xtrabackup normally copies the log files in a background thread, copies the data files in a foreground thread, and then stops the log copying thread and finishes. As long as that file exists, xtrabackup will continue to watch the log files and copy them into the xtrabackup_logfile in the target directory. When the file is removed, xtrabackup will finish copying the log file and exit. This functionality is useful for coordinating the InnoDB data backups with other actions. Perhaps the most obvious is copying the table definitions (the .frm files) so that the backup can be restored. You can start xtrabackup in the background, wait for the xtrabackup_suspended file to be created, and then copy any other files you need to complete the backup. This is exactly what the innobackupex tool does (it also copies MyISAM data and other files). Generating Meta-Data \u00b6 It is a good idea for the backup to include all the information you need to restore the backup. The xtrabackup tool can print out the contents of a my.cnf file that are needed to restore the data and log files. xtrabackup --print-param prints out something like the following: # This MySQL options file was generated by XtraBackup. [mysqld] datadir = /data/mysql/ innodb_data_home_dir = /data/innodb/ innodb_data_file_path = ibdata1:10M:autoextend innodb_log_group_home_dir = /data/innodb-logs/ You can redirect this output into a file in the target directory of the backup. Agreeing on the Source Directory \u00b6 It\u2019s possible that the presence of a defaults file or other factors could cause xtrabackup to back up data from a different location than you expected. To prevent this, you can use xtrabackup --print-param to ask it where it will be copying data from. You can use the output to ensure that xtrabackup and your script are working on the same dataset.","title":"Scripting Backups With xtrabackup"},{"location":"xtrabackup_bin/scripting_backups_xbk.html#scripting-backups-with-xtrabackup","text":"The xtrabackup tool has several features to enable scripts to control it while they perform related tasks. The innobackupex script is one example, but xtrabackup is easy to control with your own command-line scripts too.","title":"Scripting Backups With xtrabackup"},{"location":"xtrabackup_bin/scripting_backups_xbk.html#suspending-after-copying","text":"In backup mode, xtrabackup normally copies the log files in a background thread, copies the data files in a foreground thread, and then stops the log copying thread and finishes. As long as that file exists, xtrabackup will continue to watch the log files and copy them into the xtrabackup_logfile in the target directory. When the file is removed, xtrabackup will finish copying the log file and exit. This functionality is useful for coordinating the InnoDB data backups with other actions. Perhaps the most obvious is copying the table definitions (the .frm files) so that the backup can be restored. You can start xtrabackup in the background, wait for the xtrabackup_suspended file to be created, and then copy any other files you need to complete the backup. This is exactly what the innobackupex tool does (it also copies MyISAM data and other files).","title":"Suspending After Copying"},{"location":"xtrabackup_bin/scripting_backups_xbk.html#generating-meta-data","text":"It is a good idea for the backup to include all the information you need to restore the backup. The xtrabackup tool can print out the contents of a my.cnf file that are needed to restore the data and log files. xtrabackup --print-param prints out something like the following: # This MySQL options file was generated by XtraBackup. [mysqld] datadir = /data/mysql/ innodb_data_home_dir = /data/innodb/ innodb_data_file_path = ibdata1:10M:autoextend innodb_log_group_home_dir = /data/innodb-logs/ You can redirect this output into a file in the target directory of the backup.","title":"Generating Meta-Data"},{"location":"xtrabackup_bin/scripting_backups_xbk.html#agreeing-on-the-source-directory","text":"It\u2019s possible that the presence of a defaults file or other factors could cause xtrabackup to back up data from a different location than you expected. To prevent this, you can use xtrabackup --print-param to ask it where it will be copying data from. You can use the output to ensure that xtrabackup and your script are working on the same dataset.","title":"Agreeing on the Source Directory"},{"location":"xtrabackup_bin/working_with_binary_logs.html","text":"Working with Binary Logs \u00b6 The xtrabackup binary integrates with information that InnoDB stores in its transaction log about the corresponding binary log position for committed transactions. This enables it to print out the binary log position to which a backup corresponds, so you can use it to set up new replication replicas or perform point-in-time recovery. Finding the Binary Log Position \u00b6 You can find the binary log position corresponding to a backup once the backup has been prepared. This can be done by either running xtrabackup --prepare or innobackupex --apply-log . If your backup is from a server with binary logging enabled, xtrabackup will create a file named xtrabackup_binlog_info in the target directory. This file contains the binary log file name and position of the exact point in the binary log to which the prepared backup corresponds. You will also see output similar to the following during the prepare stage: InnoDB: Last MySQL binlog file position 0 3252710, file name ./mysql-bin.000001 ... snip ... [notice (again)] If you use binary log and don't use any hack of group commit, the binary log position seems to be: InnoDB: Last MySQL binlog file position 0 3252710, file name ./mysql-bin.000001 This output can also be found in the xtrabackup_binlog_pos_innodb file, but it is only correct when no other than XtraDB or InnoDB are used as storage engines. If other storage engines are used (i.e. MyISAM ), you should use the xtrabackup_binlog_info file to retrieve the position. The message about hacking group commit refers to an early implementation of emulated group commit in Percona Server for MySQL . Point-In-Time Recovery \u00b6 To perform a point-in-time recovery from an xtrabackup backup, you should prepare and restore the backup, and then replay binary logs from the point shown in the xtrabackup_binlog_info file. A more detailed procedure is found here (with innobackupex). Setting Up a New Replication Replica \u00b6 To set up a new replica, you should prepare the backup, and restore it to the data directory of your new replication replica. Then in your CHANGE MASTER TO command, use the binary log filename and position shown in the xtrabackup_binlog_info file to start replication. A more detailed procedure is found in How to setup a replica for replication in 6 simple steps with Percona XtraBackup .","title":"Working with Binary Logs"},{"location":"xtrabackup_bin/working_with_binary_logs.html#working-with-binary-logs","text":"The xtrabackup binary integrates with information that InnoDB stores in its transaction log about the corresponding binary log position for committed transactions. This enables it to print out the binary log position to which a backup corresponds, so you can use it to set up new replication replicas or perform point-in-time recovery.","title":"Working with Binary Logs"},{"location":"xtrabackup_bin/working_with_binary_logs.html#finding-the-binary-log-position","text":"You can find the binary log position corresponding to a backup once the backup has been prepared. This can be done by either running xtrabackup --prepare or innobackupex --apply-log . If your backup is from a server with binary logging enabled, xtrabackup will create a file named xtrabackup_binlog_info in the target directory. This file contains the binary log file name and position of the exact point in the binary log to which the prepared backup corresponds. You will also see output similar to the following during the prepare stage: InnoDB: Last MySQL binlog file position 0 3252710, file name ./mysql-bin.000001 ... snip ... [notice (again)] If you use binary log and don't use any hack of group commit, the binary log position seems to be: InnoDB: Last MySQL binlog file position 0 3252710, file name ./mysql-bin.000001 This output can also be found in the xtrabackup_binlog_pos_innodb file, but it is only correct when no other than XtraDB or InnoDB are used as storage engines. If other storage engines are used (i.e. MyISAM ), you should use the xtrabackup_binlog_info file to retrieve the position. The message about hacking group commit refers to an early implementation of emulated group commit in Percona Server for MySQL .","title":"Finding the Binary Log Position"},{"location":"xtrabackup_bin/working_with_binary_logs.html#point-in-time-recovery","text":"To perform a point-in-time recovery from an xtrabackup backup, you should prepare and restore the backup, and then replay binary logs from the point shown in the xtrabackup_binlog_info file. A more detailed procedure is found here (with innobackupex).","title":"Point-In-Time Recovery"},{"location":"xtrabackup_bin/working_with_binary_logs.html#setting-up-a-new-replication-replica","text":"To set up a new replica, you should prepare the backup, and restore it to the data directory of your new replication replica. Then in your CHANGE MASTER TO command, use the binary log filename and position shown in the xtrabackup_binlog_info file to start replication. A more detailed procedure is found in How to setup a replica for replication in 6 simple steps with Percona XtraBackup .","title":"Setting Up a New Replication Replica"},{"location":"xtrabackup_bin/xbk_option_reference.html","text":"The xtrabackup Option Reference \u00b6 This page documents all of the command-line options for the xtrabackup binary. Options \u00b6 \u2013apply-log-only \u00b6 This option causes only the redo stage to be performed when preparing a backup. It is very important for incremental backups. \u2013backup \u00b6 Make a backup and place it in xtrabackup --target-dir . See Creating a backup . \u2013backup-lock-retry-count= \u00b6 The number of attempts to acquire metadata locks. \u2013backup-lock-timeout= \u00b6 The timeout in seconds for attempts to acquire metadata locks. \u2013binlog-info \u00b6 This option controls how Percona XtraBackup should retrieve the server\u2019s binary log coordinates corresponding to the backup. Possible values are OFF , ON , LOCKLESS and AUTO . See the Percona XtraBackup Lockless binary log information manual page for more information. \u2013check-privileges \u00b6 This option checks if Percona XtraBackup has all the required privileges. If a missing privilege is required for the current operation, it will terminate and print out an error message. If a missing privilege is not required for the current operation but may be necessary for some other XtraBackup operation, the process is not aborted, and a warning is printed. xtrabackup: Error: missing required privilege LOCK TABLES on *.* xtrabackup: Warning: missing required privilege REPLICATION CLIENT on *.* \u2013close-files \u00b6 Do not keep files open. When xtrabackup opens a tablespace, it normally does not close its file handle to manage the DDL operations correctly. However, if the number of tablespaces is huge and can not fit into any limit, there is an option to close file handles once they are no longer accessed. Percona XtraBackup can produce inconsistent backups with this option enabled. Use the option at your own risk. \u2013compact \u00b6 Create a compact backup by skipping secondary index pages. \u2013compress \u00b6 This option tells xtrabackup to compress all output data, including the transaction log file and metadata files, using the specified compression algorithm. The only currently supported algorithm is quicklz . The resulting files have the qpress archive format. Every \\*.qp file produced by xtrabackup is essentially a one-file qpress archive and can be extracted and uncompressed by the qpress file archiver. \u2013compress-chunk-size= \u00b6 Size of working buffer(s) for compression threads in bytes. The default value is 64K. \u2013compress-threads= \u00b6 This option specifies the number of worker threads used by xtrabackup for parallel data compression. This option defaults to 1 . Parallel compression ( xtrabackup --compress-threads ) can be used together with parallel file copying ( xtrabackup --parallel ). For example, --parallel=4 --compress --compress-threads=2 will create 4 I/O threads that will read the data and pipe it to 2 compression threads. \u2013copy-back \u00b6 Copy all the files in a previously made backup from the backup directory to their original locations. This option will not copy over existing files unless xtrabackup --force-non-empty-directories option is specified. \u2013core-file \u00b6 Write core on fatal signals. \u2013databases= \u00b6 This option specifies the list of databases and tables that should be backed up. The option accepts the list of the form \"databasename1[.table_name1] databasename2[.table_name2] . . .\" . \u2013databases-exclude=name \u00b6 Excluding databases based on name. This option operates the same way as xtrabackup --databases , but matched names are excluded from the backup. Note that this option has a higher priority than xtrabackup --databases . \u2013databases-file= \u00b6 This option specifies the path to the file containing the list of databases and tables that should be backed up. The file can contain the list elements of the form databasename1[.table_name1] , one element per line. \u2013datadir=DIRECTORY \u00b6 The source directory for the backup. This directory should be the same as the datadir for your MySQL server, and it should be read from my.cnf if that exists; otherwise, you must specify it on the command line. When combined with the xtrabackup --copy-back or xtrabackup --move-back option, xtrabackup --datadir refers to the destination directory. Once connected to the server, to perform a backup, you will need READ and EXECUTE permissions at a filesystem level in the server\u2019s datadir. \u2013debug-sleep-before-unlock= \u00b6 A debug-only option that is used by the Xtrabackup test suite. \u2013decompress \u00b6 This option decompresses all files with the .qp extension in a backup previously made with the xtrabackup --compress option. The xtrabackup --parallel option will allow multiple files to be decrypted simultaneously. To decompress, the qpress utility MUST be installed and accessible within the path. Percona XtraBackup doesn\u2019t automatically remove the compressed files. To clean up the backup directory, users should use the xtrabackup --remove-original option. \u2013decrypt=ENCRYPTION-ALGORITHM \u00b6 Decrypts all files with the .xbcrypt extension in a backup previously made with xtrabackup --encrypt option. The xtrabackup --parallel option will allow multiple files to be decrypted simultaneously. Percona XtraBackup doesn\u2019t automatically remove the encrypted files. To clean up the backup directory, users should use xtrabackup --remove-original option. \u2013defaults-extra-file=[MY.CNF] \u00b6 Read this file after the global files are read. This file must be the first option on the command-line. \u2013defaults-file=[MY.CNF] \u00b6 Only read default options from the given file. This file must be the first option on the command-line and be a real file and cannot be a symbolic link. \u2013defaults-group=GROUP-NAME \u00b6 This option sets up the group which should be read from the configuration file. The option is used by the --default-group and is required for mysqld_multi deployments. \u2013defaults-group-suffix= \u00b6 Read the usual options groups and also groups with concat(group, suffix). \u2013dump-innodb-buffer-pool \u00b6 This option controls whether or not a new dump of buffer pool content should be done. With --dump-innodb-buffer-pool , xtrabackup makes a request to the server to start the buffer pool dump (it takes some time to complete and is done in background) at the beginning of a backup provided the status variable innodb_buffer_pool_dump_status reports that the dump has been completed. $ xtrabackup --backup --dump-innodb-buffer-pool --target-dir = /home/user/backup By default, this option is set to OFF. If innodb_buffer_pool_dump_status reports that there is running dump of the buffer pool, xtrabackup waits for the dump to complete using the value of \u2013dump-innodb-buffer-pool-timeout The file ib_buffer_pool stores tablespace ID and page ID data used to warm up the buffer pool sooner. \u2013dump-innodb-buffer-pool-timeout \u00b6 This option contains the number of seconds that xtrabackup should monitor the value of innodb_buffer_pool_dump_status to determine if buffer pool dump has completed. This option is used in combination with --dump-innodb-buffer-pool . By default, it is set to 10 seconds. \u2013dump-innodb-buffer-pool-pct \u00b6 This option contains the percentage of the most recently used buffer pool pages to dump. This option is effective if \u2013dump-innodb-buffer-pool option is set to ON. If this option contains a value, xtrabackup sets the MySQL system variable innodb_buffer_pool_dump_pct . As soon as the buffer pool dump completes or it is stopped (see \u2013dump-innodb-buffer-pool-timeout), the value of the MySQL system variable is restored. \u2013encrypt=ENCRYPTION_ALGORITHM \u00b6 This option instructs xtrabackup to encrypt backup copies of InnoDB data files using the algorithm specified in the ENCRYPTION_ALGORITHM. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. \u2013encrypt-key=ENCRYPTION_KEY \u00b6 This option instructs xtrabackup to use the given ENCRYPTION_KEY when using the xtrabackup --encrypt option. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. \u2013encrypt-key-file=ENCRYPTION_KEY_FILE \u00b6 This option instructs xtrabackup to use the encryption key stored in the given ENCRYPTION_KEY_FILE when using the xtrabackup --encrypt option. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. \u2013encrypt-threads= \u00b6 This option specifies the number of worker threads that will be used for parallel encryption/decryption. See the xtrabackup documentation for more details. \u2013encrypt-chunk-size= \u00b6 This option specifies the size of the internal working buffer for each encryption thread, and is measured in bytes. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. Note To adjust the xbcloud/xbstream chunk size when you use encryption, you must adjust both the \u2013encrypt-chunk-size and \u2013read-buffer-size variables. \u2013export \u00b6 Create files necessary for exporting tables. See Restoring Individual Tables . \u2013extra-lsndir=DIRECTORY \u00b6 (for \u2013backup): save an extra copy of the xtrabackup_checkpoints and xtrabackup_info files in this directory. \u2013force-non-empty-directories \u00b6 When specified, it makes :option`xtrabackup \u2013copy-back` and xtrabackup --move-back option transfer files to non-empty directories. No existing files will be overwritten. If files that need to be copied/moved from the backup directory already exist in the destination directory, it will still fail with an error. \u2013ftwrl-wait-timeout=SECONDS \u00b6 This option specifies time in seconds that xtrabackup should wait for queries that would block FLUSH TABLES WITH READ LOCK before running it. If there are still such queries when the timeout expires, xtrabackup terminates with an error. The default is 0 , in which case it does not wait for queries to complete and starts FLUSH TABLES WITH READ LOCK immediately. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. \u2013ftwrl-wait-threshold=SECONDS \u00b6 This option specifies the query run time threshold which is used by xtrabackup to detect long-running queries with a non-zero value of xtrabackup \u2013ftwrl-wait-timeout. FLUSH TABLES WITH READ LOCK is not started until such long-running queries exist. This option has no effect if xtrabackup \u2013ftwrl-wait-timeout is 0 . The default value is 60 seconds. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. \u2013ftwrl-wait-query-type=all|update \u00b6 This option specifies which types of queries are allowed to complete before xtrabackup will issue the global lock. The default is all . \u2013galera-info \u00b6 This options creates the xtrabackup_galera_info file which contains the local node state at the time of the backup. Option should be used when performing the backup of Percona XtraDB Cluster. It has no effect when backup locks are used to create the backup. \u2013generate-new-master-key \u00b6 Generates a new master key when doing a copy-back operation. \u2013history=name \u00b6 This option enables the tracking of the backup history in the PERCONA_SCHEMA.xtrabackup_history table. An optional history series name may be specified that will be placed with the history record for the backup being taken. \u2013incremental \u00b6 This option tells xtrabackup to create an incremental backup. It is passed to the xtrabackup child process. When this option is specified, either xtrabackup --incremental-lsn or xtrabackup \u2013incremental-basedir can also be given. If neither option is given, option xtrabackup --incremental-basedir is passed to xtrabackup by default, set to the first timestamped backup directory in the backup base directory. \u2013incremental-basedir=DIRECTORY \u00b6 This directory contains the full backup, which is the base dataset used for the incremental backups. \u2013incremental-dir=DIRECTORY \u00b6 When preparing an incremental backup, this is the directory where the incremental backup is combined with the full backup to make a new full backup. \u2013incremental-force-scan \u00b6 When creating an incremental backup, force a full scan of the data pages in the instance to be used in the backup even if the complete changed page bitmap data is available. \u2013incremental-history-name=name \u00b6 This option specifies the name of the backup series stored in the PERCONA_SCHEMA.xtrabackup_history history record to base an incremental backup on. xtrabackup searches the history table for the most recent (highest innodb_to_lsn), successful backup in the series and take the to_lsn value to use as the starting lsn for the incremental backup. This will be mutually exclusive with xtrabackup --incremental-history-uuid , xtrabackup --incremental-basedir and xtrabackup --incremental-lsn . If no valid LSN can be found (no series by that name, no successful backups by that name) xtrabackup returns an error. It is used with the xtrabackup --incremental option. \u2013innodb-checksum-algorithm=name \u00b6 The algorithm InnoDB uses to calculate a page checksum. The available algorithms are CRC32, INNODB, NONE, STRICT_CRC32, STRICT_INNODB, and STRICT_NONE \u2013incremental-history-uuid=UUID \u00b6 This option specifies the UUID of the specific history record stored in the PERCONA_SCHEMA.xtrabackup_history to base an incremental backup on xtrabackup --incremental-history-name , xtrabackup --incremental-basedir and xtrabackup --incremental-lsn . If no valid LSN is found (no success record with that UUID) xtrabackup returns an error. This option is used with the xtrabackup --incremental option. \u2013incremental-lsn=LSN \u00b6 When creating an incremental backup, you can specify the log sequence number ( LSN ) instead of specifying xtrabackup --incremental-basedir . For databases created in 5.1 and later, specify the LSN as a single 64-bit integer. ATTENTION : If a wrong LSN value is specified (a user error that Percona XtraBackup cannot detect), the backup will be unusable. Be careful! \u2013innodb-log-arch-dir=DIRECTORY \u00b6 This option is used to specify the directory containing the archived logs. It can only be used with the xtrabackup --prepare option. \u2013innodb-miscellaneous \u00b6 A large group of InnoDB options are normally read from the my.cnf configuration file, so that xtrabackup boots up its embedded InnoDB in the same configuration as your current server. You normally do not need to specify these explicitly. These options have the same behavior that they have in InnoDB or XtraDB. They are as follows: --innodb-adaptive-hash-index --innodb-additional-mem-pool-size --innodb-autoextend-increment --innodb-buffer-pool-size --innodb-checksums --innodb-data-file-path --innodb-data-home-dir --innodb-doublewrite-file --innodb-doublewrite --innodb-extra-undoslots --innodb-fast-checksum --innodb-file-io-threads --innodb-file-per-table --innodb-flush-log-at-trx-commit --innodb-flush-method --innodb-force-recovery --innodb-io-capacity --innodb-lock-wait-timeout --innodb-log-buffer-size --innodb-log-files-in-group --innodb-log-file-size --innodb-log-group-home-dir --innodb-max-dirty-pages-pct --innodb-open-files --innodb-page-size --innodb-read-io-threads --innodb-write-io-threads \u2013innodb-undo-directory=name \u00b6 The directory location for the undo tablespace. The path is absolute. \u2013innodb-undo-tablespace= \u00b6 The number of undo tablespaces to use. \u2013keyring-file-data=FILENAME \u00b6 The path to the keyring file. Combine this option with xtrabackup --xtrabackup-plugin-dir . \u2013kill-long-queries-timeout= \u00b6 This options specifies the number of seconds xtrabackup waits between starting FLUSH TABLES WITH READ LOCK and killing those queries that block it. The default is 0 (zero) seconds, which means the xtrabackup does not attempt to kill any queries. \u2013kill-long-query-type=select|all \u00b6 This option specifies which types of queries should be killed to unblock the global lock. The default value is select . \u2013lock-ddl \u00b6 Issue LOCK TABLES FOR BACKUP if it is supported by server at the beginning of the backup to block all DDL operations. \u2013lock-ddl-per-table \u00b6 Lock DDL for each table before xtrabackup starts to copy it and until the backup is completed. \u2013lock-ddl-timeout \u00b6 If LOCK TABLES FOR BACKUP does not return within given timeout, abort the backup. \u2013log-bin[=name] \u00b6 The base name for the log sequence. \u2013log-copy-interval= \u00b6 This option specifies time interval between log copying thread checks in milliseconds (default is 1 second). \u2013login-path= \u00b6 Read this path from the login file. \u2013move-back \u00b6 Move all the files in a previously made backup from the backup directory to their original locations. As this option removes backup files, it must be used with caution. \u2013no-backup-locks \u00b6 This options controls if backup locks are used instead of FLUSH TABLES WITH READ LOCK during the backup stage. The backup locks are must be supported on the server for the option to have an affect. This option is enabled by default. Disable the option with --no-backup-locks . \u2013no-defaults \u00b6 Do not read the default options from any option file. Must be given as the first option on the command-line. \u2013no-lock \u00b6 This option automatically uses Backup Locks, and disables table locks, as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Only use this option if all tables are InnoDB and you do not care about the binary log position of the backup. Do not use this option if any DDL statements will be executed or if any non-InnoDB tables are being updated (this includes the MyISAM tables in the mysql database). Using this option in these conditions could cause an inconsistent backup. If your backups fail to acquire a lock and you are planning to use this option, the failure may be caused by incoming replication events that prevent the lock from succeeding. Try the --safe-slave-backup to momentarily stop the replication slave thread. The xtrabackup-binlog-info is not created when the --no-lock is used because SHOW MASTER STATUS may be inconsistent. In certain conditions, xtrabackup_binlog_pos_innodb can be used instead to get consistent binlog coordinates as described in Working with Binary Logs . \u2013no-version-check \u00b6 This option disables the version check. If you do not pass this option, the automatic version check is enabled implicitly when xtrabackup runs in the --backup mode. To disable the version check, explicitly pass the --no-version-check option when invoking xtrabackup. When the automatic version check is enabled,xtrabackup performs a version check against the server on the backup stage after creating a server connection. xtrabackup sends the following information to the server: MySQL flavour and version Operating system name Percona Toolkit version Perl version Each piece of information has a unique identifier which is an MD5 hash value that Percona Toolkit uses to obtain statistics about how it is used. This value is a random UUID; no client information is either collected or stored. \u2013open-files-limit= \u00b6 The maximum number of file descriptors to reserve with setrlimit(). \u2013parallel= \u00b6 This option specifies the number of threads to use to copy multiple data files concurrently when creating a backup. The default value is 1 (i.e., no concurrent transfer). In Percona XtraBackup 2.3.10 and newer, this option can be used with xtrabackup --copy-back option to copy the user data files in parallel (redo logs and system tablespaces are copied in the main thread). \u2013password=PASSWORD \u00b6 This option specifies the password to use when connecting to the database. It accepts a string argument. See mysql --help for details. \u2013prepare \u00b6 Makes xtrabackup perform recovery on a backup created with xtrabackup --backup , so that it is ready to use. See preparing a backup . \u2013print-defaults \u00b6 Print the program argument list and exit. Must be given as the first option on the command-line. \u2013print-param \u00b6 Makes xtrabackup print out parameters that to copy the data files back to their original locations to restore them. See Scripting Backups With xtrabackup . \u2013read-buffer-size[=#] \u00b6 Set read buffer size. The given value is scaled up to page size. The default is 10MB. Use this variable to increase the xbcloud/xbstream chunk size from the default value of 10MB. NOTE : When you use encryption, to adjust the xbcloud/xbstream chunk size, adjust both the --encrypt-chunk-size and --read-buffer-size variables. $ xtrabackup ... --read-buffer-size=1G | xbcloud put ... \u2013rebuild-indexes \u00b6 Rebuild secondary indexes in InnoDB tables after applying the log. Only use with \u2013prepare. \u2013rebuild-threads= \u00b6 This option defines the number of threads to rebuild indexes in a compact backup. Only use with --prepare and --rebuild-indexes . \u2013redo-log-version= \u00b6 The redo log version of the backup. Use only with --prepare . \u2013reencrypt-for-server-id= \u00b6 Use this option to start the server instance with different server_id from the one the encrypted backup was taken from, like a replication replica or a Galera node. When this option is used, xtrabackup will, as a prepare step, generate a new master key with ID based on the new server_id, store it into keyring file, and re-encrypt the tablespace keys inside of tablespace headers. The option should be passed for --prepare (final step). \u2013remove-original \u00b6 Implemented in Percona XtraBackup 2.4.6, this option when specified will remove .qp , .xbcrypt and .qp.xbcrypt files after decryption and decompression. \u2013rsync \u00b6 Use the rsync utility to optimize local file transfers. When this option is specified, xtrabackup uses rsync to copy all non-InnoDB files instead of spawning a separate copy command for each file. This option is faster for servers with a large number of databases or tables. This option cannot be used with --stream . \u2013safe-slave-backup \u00b6 When specified, xtrabackup will stop the replica SQL thread just before running FLUSH TABLES WITH READ LOCK and wait to start backup until Slave_open_temp_tables in SHOW STATUS is zero. If there are no open temporary tables, the backup will occur; otherwise the SQL thread will be started and stopped until there are no open temporary tables. The backup will fail if Slave_open_temp_tables does not become zero after xtrabackup \u2013safe-slave-backup-timeout seconds. The replica SQL thread will be restarted when the backup finishes. This option is implemented to deal with replicating temporary tables and isn\u2019t neccessary with Row-Based-Replication. \u2013safe-slave-backup-timeout=SECONDS \u00b6 How many seconds xtrabackup --safe-slave-backup should wait for Slave_open_temp_tables to become zero. The default is 300 seconds. \u2013secure-auth \u00b6 Refuse client connecting to the server if it uses old (pre-4.1.1) protocol. (Enabled by default; use \u2013skip-secure-auth to disable.) \u2013server-id= \u00b6 The server instance being backed up. \u2013server-public-key-path=name \u00b6 File path the server\u2019s public RSA key in PEM format. \u2013skip-tables-compatibility-check \u00b6 This option disables the engine compatibility warning. See also --tables-compatibility-check \u2013slave-info \u00b6 This option is useful when backing up a replication replica server. It prints the binary log position of the source server. It also writes the binary log coordinates to the xtrabackup_slave_info file as a CHANGE MASTER command. A new replica for this source can be set up by starting a replica server on this backup and issuing a CHANGE MASTER command with the binary log position saved in the xtrabackup_slave_info file. \u2013ssl \u00b6 Enable secure connection. More information can be found in \u2013ssl MySQL server documentation. \u2013ssl-ca \u00b6 Path of the file, which contains a list of trusted SSL CAs. More information can be found in \u2013ssl-ca MySQL server documentation. \u2013ssl-capath \u00b6 The directory path that contains trusted SSL CA certificates in the PEM format. More information can be found in \u2013ssl-capath MySQL server documentation. \u2013ssl-cert \u00b6 Path of the file which contains X509 certificate in PEM format. More information can be found in \u2013ssl-cert MySQL server documentation. \u2013ssl-cipher \u00b6 List of permitted ciphers to use for connection encryption. More information can be found in \u2013ssl-cipher MySQL server documentation. \u2013ssl-crl \u00b6 Path of the file that contains certificate revocation lists. More information can be found in \u2013ssl-crl MySQL server documentation. \u2013ssl-crlpath \u00b6 Path of the directory that contains certificate revocation list files. More information can be found in \u2013ssl-crlpath MySQL server documentation. \u2013ssl-key \u00b6 Path of the file that contains X509 key in PEM format. More information can be found in \u2013ssl-key MySQL server documentation. \u2013ssl-mode \u00b6 The security state of connection to server. More information can be found in \u2013ssl-mode MySQL server documentation. \u2013ssl-verify-server-cert \u00b6 Verify server certificate Common Name value against host name used when connecting to server. More information can be found in \u2013ssl-verify-server-cert MySQL server documentation. \u2013stats \u00b6 Causes xtrabackup to scan the specified data files and print out index statistics. \u2013stream=name \u00b6 Stream all backup files to the standard output in the specified format. Currently supported formats are xbstream and tar . \u2013tables=name \u00b6 A regular expression against which the full tablename, in databasename.tablename format, is matched. If the name matches, the table is backed up. See partial backups . \u2013tables-compatibility-check \u00b6 This option enables the engine compatibility warning. The default value is ON . Use --skip-tables-compatibility-check to disable. \u2013tables-exclude=name \u00b6 Filtering by regexp for table names. Operates the same way as xtrabackup --tables , but matched names are excluded from backup. Note that this option has a higher priority than xtrabackup --tables . \u2013tables-file=name \u00b6 A file containing one table name per line, in databasename.tablename format. The backup will be limited to the specified tables. See Scripting Backups With xtrabackup . \u2013target-dir=DIRECTORY \u00b6 This option specifies the destination directory for the backup. If the directory does not exist, xtrabackup creates it. If the directory does exist and is empty, xtrabackup will succeed. xtrabackup will not overwrite existing files; however it will fail with operating system error 17, file exists . If this option is a relative path, it is interpreted as being relative to the current working directory from which xtrabackup is executed. In order to perform a backup, you need READ , WRITE , and EXECUTE permissions at a filesystem level for the directory that you supply as the value of --target-dir . \u2013throttle= \u00b6 This option limits the number of chunks copied per second. The chunk size is 10 MB . To limit the bandwidth to 10 MB/s , set the option to 1 : --throttle=1 . \u2013tls-version=name \u00b6 The TLS version to use. The allowed values are the following: TLSv1 TLSv1.1 TLSv1.2 \u2013tmpdir=name \u00b6 This option is currently not used for anything except printing out the correct tmpdir parameter when xtrabackup --print-param is used. \u2013to-archived-lsn=LSN \u00b6 This option is used to specify the LSN to which the logs should be applied when backups are being prepared. It can only be used with the xtrabackup --prepare option. \u2013transition-key \u00b6 This option is used to enable processing the backup without accessing the keyring vault server. In this case, xtrabackup derives the AES encryption key from the specified passphrase and uses it to encrypt tablespace keys of tablespaces being backed up. If --transition-key <xtrabackup \u2013transition-key> does not have any value, xtrabackup will ask for it. The same passphrase should be specified for the xtrabackup --prepare command. \u2013use-memory= \u00b6 This option affects how much memory is allocated and is similar to innodb_buffer_pool_size . This option is only relevant in the --prepare phase or when analyzing statistics with --stats . The default value is 100MB. The recommended value is between 1GB to 2GB. Multiples are supported providing the unit (for example, 1MB, 1M, 1GB, 1G). \u2013user=USERNAME \u00b6 This option specifies the MySQL username used when connecting to the server, if that\u2019s not the current user. The option accepts a string argument. See mysql \u2013help for details. \u2013version \u00b6 This option prints xtrabackup version and exits. \u2013xtrabackup-plugin-dir=DIRNAME \u00b6 The absolute path to the directory that contains the keyring plugin. See also Percona Server for MySQL Documentation: keyring_vault plugin with Data at Rest Encryption https://www.percona.com/doc/percona-server/5.7/security/data-at-rest-encryption.html MySQL Documentation: Using the keyring_file File-Based Plugin https://dev.mysql.com/doc/refman/5.7/en/keyring-file-plugin.html","title":"The xtrabackup Option Reference"},{"location":"xtrabackup_bin/xbk_option_reference.html#the-xtrabackup-option-reference","text":"This page documents all of the command-line options for the xtrabackup binary.","title":"The xtrabackup Option Reference"},{"location":"xtrabackup_bin/xbk_option_reference.html#options","text":"","title":"Options"},{"location":"xtrabackup_bin/xbk_option_reference.html#-apply-log-only","text":"This option causes only the redo stage to be performed when preparing a backup. It is very important for incremental backups.","title":"--apply-log-only"},{"location":"xtrabackup_bin/xbk_option_reference.html#-backup","text":"Make a backup and place it in xtrabackup --target-dir . See Creating a backup .","title":"--backup"},{"location":"xtrabackup_bin/xbk_option_reference.html#-backup-lock-retry-count","text":"The number of attempts to acquire metadata locks.","title":"--backup-lock-retry-count="},{"location":"xtrabackup_bin/xbk_option_reference.html#-backup-lock-timeout","text":"The timeout in seconds for attempts to acquire metadata locks.","title":"--backup-lock-timeout="},{"location":"xtrabackup_bin/xbk_option_reference.html#-binlog-info","text":"This option controls how Percona XtraBackup should retrieve the server\u2019s binary log coordinates corresponding to the backup. Possible values are OFF , ON , LOCKLESS and AUTO . See the Percona XtraBackup Lockless binary log information manual page for more information.","title":"--binlog-info"},{"location":"xtrabackup_bin/xbk_option_reference.html#-check-privileges","text":"This option checks if Percona XtraBackup has all the required privileges. If a missing privilege is required for the current operation, it will terminate and print out an error message. If a missing privilege is not required for the current operation but may be necessary for some other XtraBackup operation, the process is not aborted, and a warning is printed. xtrabackup: Error: missing required privilege LOCK TABLES on *.* xtrabackup: Warning: missing required privilege REPLICATION CLIENT on *.*","title":"--check-privileges"},{"location":"xtrabackup_bin/xbk_option_reference.html#-close-files","text":"Do not keep files open. When xtrabackup opens a tablespace, it normally does not close its file handle to manage the DDL operations correctly. However, if the number of tablespaces is huge and can not fit into any limit, there is an option to close file handles once they are no longer accessed. Percona XtraBackup can produce inconsistent backups with this option enabled. Use the option at your own risk.","title":"--close-files"},{"location":"xtrabackup_bin/xbk_option_reference.html#-compact","text":"Create a compact backup by skipping secondary index pages.","title":"--compact"},{"location":"xtrabackup_bin/xbk_option_reference.html#-compress","text":"This option tells xtrabackup to compress all output data, including the transaction log file and metadata files, using the specified compression algorithm. The only currently supported algorithm is quicklz . The resulting files have the qpress archive format. Every \\*.qp file produced by xtrabackup is essentially a one-file qpress archive and can be extracted and uncompressed by the qpress file archiver.","title":"--compress"},{"location":"xtrabackup_bin/xbk_option_reference.html#-compress-chunk-size","text":"Size of working buffer(s) for compression threads in bytes. The default value is 64K.","title":"--compress-chunk-size="},{"location":"xtrabackup_bin/xbk_option_reference.html#-compress-threads","text":"This option specifies the number of worker threads used by xtrabackup for parallel data compression. This option defaults to 1 . Parallel compression ( xtrabackup --compress-threads ) can be used together with parallel file copying ( xtrabackup --parallel ). For example, --parallel=4 --compress --compress-threads=2 will create 4 I/O threads that will read the data and pipe it to 2 compression threads.","title":"--compress-threads="},{"location":"xtrabackup_bin/xbk_option_reference.html#-copy-back","text":"Copy all the files in a previously made backup from the backup directory to their original locations. This option will not copy over existing files unless xtrabackup --force-non-empty-directories option is specified.","title":"--copy-back"},{"location":"xtrabackup_bin/xbk_option_reference.html#-core-file","text":"Write core on fatal signals.","title":"--core-file"},{"location":"xtrabackup_bin/xbk_option_reference.html#-databases","text":"This option specifies the list of databases and tables that should be backed up. The option accepts the list of the form \"databasename1[.table_name1] databasename2[.table_name2] . . .\" .","title":"--databases="},{"location":"xtrabackup_bin/xbk_option_reference.html#-databases-excludename","text":"Excluding databases based on name. This option operates the same way as xtrabackup --databases , but matched names are excluded from the backup. Note that this option has a higher priority than xtrabackup --databases .","title":"--databases-exclude=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-databases-file","text":"This option specifies the path to the file containing the list of databases and tables that should be backed up. The file can contain the list elements of the form databasename1[.table_name1] , one element per line.","title":"--databases-file="},{"location":"xtrabackup_bin/xbk_option_reference.html#-datadirdirectory","text":"The source directory for the backup. This directory should be the same as the datadir for your MySQL server, and it should be read from my.cnf if that exists; otherwise, you must specify it on the command line. When combined with the xtrabackup --copy-back or xtrabackup --move-back option, xtrabackup --datadir refers to the destination directory. Once connected to the server, to perform a backup, you will need READ and EXECUTE permissions at a filesystem level in the server\u2019s datadir.","title":"--datadir=DIRECTORY"},{"location":"xtrabackup_bin/xbk_option_reference.html#-debug-sleep-before-unlock","text":"A debug-only option that is used by the Xtrabackup test suite.","title":"--debug-sleep-before-unlock="},{"location":"xtrabackup_bin/xbk_option_reference.html#-decompress","text":"This option decompresses all files with the .qp extension in a backup previously made with the xtrabackup --compress option. The xtrabackup --parallel option will allow multiple files to be decrypted simultaneously. To decompress, the qpress utility MUST be installed and accessible within the path. Percona XtraBackup doesn\u2019t automatically remove the compressed files. To clean up the backup directory, users should use the xtrabackup --remove-original option.","title":"--decompress"},{"location":"xtrabackup_bin/xbk_option_reference.html#-decryptencryption-algorithm","text":"Decrypts all files with the .xbcrypt extension in a backup previously made with xtrabackup --encrypt option. The xtrabackup --parallel option will allow multiple files to be decrypted simultaneously. Percona XtraBackup doesn\u2019t automatically remove the encrypted files. To clean up the backup directory, users should use xtrabackup --remove-original option.","title":"--decrypt=ENCRYPTION-ALGORITHM"},{"location":"xtrabackup_bin/xbk_option_reference.html#-defaults-extra-filemycnf","text":"Read this file after the global files are read. This file must be the first option on the command-line.","title":"--defaults-extra-file=[MY.CNF]"},{"location":"xtrabackup_bin/xbk_option_reference.html#-defaults-filemycnf","text":"Only read default options from the given file. This file must be the first option on the command-line and be a real file and cannot be a symbolic link.","title":"--defaults-file=[MY.CNF]"},{"location":"xtrabackup_bin/xbk_option_reference.html#-defaults-groupgroup-name","text":"This option sets up the group which should be read from the configuration file. The option is used by the --default-group and is required for mysqld_multi deployments.","title":"--defaults-group=GROUP-NAME"},{"location":"xtrabackup_bin/xbk_option_reference.html#-defaults-group-suffix","text":"Read the usual options groups and also groups with concat(group, suffix).","title":"--defaults-group-suffix="},{"location":"xtrabackup_bin/xbk_option_reference.html#-dump-innodb-buffer-pool","text":"This option controls whether or not a new dump of buffer pool content should be done. With --dump-innodb-buffer-pool , xtrabackup makes a request to the server to start the buffer pool dump (it takes some time to complete and is done in background) at the beginning of a backup provided the status variable innodb_buffer_pool_dump_status reports that the dump has been completed. $ xtrabackup --backup --dump-innodb-buffer-pool --target-dir = /home/user/backup By default, this option is set to OFF. If innodb_buffer_pool_dump_status reports that there is running dump of the buffer pool, xtrabackup waits for the dump to complete using the value of \u2013dump-innodb-buffer-pool-timeout The file ib_buffer_pool stores tablespace ID and page ID data used to warm up the buffer pool sooner.","title":"--dump-innodb-buffer-pool"},{"location":"xtrabackup_bin/xbk_option_reference.html#-dump-innodb-buffer-pool-timeout","text":"This option contains the number of seconds that xtrabackup should monitor the value of innodb_buffer_pool_dump_status to determine if buffer pool dump has completed. This option is used in combination with --dump-innodb-buffer-pool . By default, it is set to 10 seconds.","title":"--dump-innodb-buffer-pool-timeout"},{"location":"xtrabackup_bin/xbk_option_reference.html#-dump-innodb-buffer-pool-pct","text":"This option contains the percentage of the most recently used buffer pool pages to dump. This option is effective if \u2013dump-innodb-buffer-pool option is set to ON. If this option contains a value, xtrabackup sets the MySQL system variable innodb_buffer_pool_dump_pct . As soon as the buffer pool dump completes or it is stopped (see \u2013dump-innodb-buffer-pool-timeout), the value of the MySQL system variable is restored.","title":"--dump-innodb-buffer-pool-pct"},{"location":"xtrabackup_bin/xbk_option_reference.html#-encryptencryption_algorithm","text":"This option instructs xtrabackup to encrypt backup copies of InnoDB data files using the algorithm specified in the ENCRYPTION_ALGORITHM. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details.","title":"--encrypt=ENCRYPTION_ALGORITHM"},{"location":"xtrabackup_bin/xbk_option_reference.html#-encrypt-keyencryption_key","text":"This option instructs xtrabackup to use the given ENCRYPTION_KEY when using the xtrabackup --encrypt option. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details.","title":"--encrypt-key=ENCRYPTION_KEY"},{"location":"xtrabackup_bin/xbk_option_reference.html#-encrypt-key-fileencryption_key_file","text":"This option instructs xtrabackup to use the encryption key stored in the given ENCRYPTION_KEY_FILE when using the xtrabackup --encrypt option. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details.","title":"--encrypt-key-file=ENCRYPTION_KEY_FILE"},{"location":"xtrabackup_bin/xbk_option_reference.html#-encrypt-threads","text":"This option specifies the number of worker threads that will be used for parallel encryption/decryption. See the xtrabackup documentation for more details.","title":"--encrypt-threads="},{"location":"xtrabackup_bin/xbk_option_reference.html#-encrypt-chunk-size","text":"This option specifies the size of the internal working buffer for each encryption thread, and is measured in bytes. It is passed directly to the xtrabackup child process. See the xtrabackup documentation for more details. Note To adjust the xbcloud/xbstream chunk size when you use encryption, you must adjust both the \u2013encrypt-chunk-size and \u2013read-buffer-size variables.","title":"--encrypt-chunk-size="},{"location":"xtrabackup_bin/xbk_option_reference.html#-export","text":"Create files necessary for exporting tables. See Restoring Individual Tables .","title":"--export"},{"location":"xtrabackup_bin/xbk_option_reference.html#-extra-lsndirdirectory","text":"(for \u2013backup): save an extra copy of the xtrabackup_checkpoints and xtrabackup_info files in this directory.","title":"--extra-lsndir=DIRECTORY"},{"location":"xtrabackup_bin/xbk_option_reference.html#-force-non-empty-directories","text":"When specified, it makes :option`xtrabackup \u2013copy-back` and xtrabackup --move-back option transfer files to non-empty directories. No existing files will be overwritten. If files that need to be copied/moved from the backup directory already exist in the destination directory, it will still fail with an error.","title":"--force-non-empty-directories"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ftwrl-wait-timeoutseconds","text":"This option specifies time in seconds that xtrabackup should wait for queries that would block FLUSH TABLES WITH READ LOCK before running it. If there are still such queries when the timeout expires, xtrabackup terminates with an error. The default is 0 , in which case it does not wait for queries to complete and starts FLUSH TABLES WITH READ LOCK immediately. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables.","title":"--ftwrl-wait-timeout=SECONDS"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ftwrl-wait-thresholdseconds","text":"This option specifies the query run time threshold which is used by xtrabackup to detect long-running queries with a non-zero value of xtrabackup \u2013ftwrl-wait-timeout. FLUSH TABLES WITH READ LOCK is not started until such long-running queries exist. This option has no effect if xtrabackup \u2013ftwrl-wait-timeout is 0 . The default value is 60 seconds. Where supported (Percona Server 5.6+) xtrabackup will automatically use Backup Locks as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables.","title":"--ftwrl-wait-threshold=SECONDS"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ftwrl-wait-query-typeallupdate","text":"This option specifies which types of queries are allowed to complete before xtrabackup will issue the global lock. The default is all .","title":"--ftwrl-wait-query-type=all|update"},{"location":"xtrabackup_bin/xbk_option_reference.html#-galera-info","text":"This options creates the xtrabackup_galera_info file which contains the local node state at the time of the backup. Option should be used when performing the backup of Percona XtraDB Cluster. It has no effect when backup locks are used to create the backup.","title":"--galera-info"},{"location":"xtrabackup_bin/xbk_option_reference.html#-generate-new-master-key","text":"Generates a new master key when doing a copy-back operation.","title":"--generate-new-master-key"},{"location":"xtrabackup_bin/xbk_option_reference.html#-historyname","text":"This option enables the tracking of the backup history in the PERCONA_SCHEMA.xtrabackup_history table. An optional history series name may be specified that will be placed with the history record for the backup being taken.","title":"--history=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-incremental","text":"This option tells xtrabackup to create an incremental backup. It is passed to the xtrabackup child process. When this option is specified, either xtrabackup --incremental-lsn or xtrabackup \u2013incremental-basedir can also be given. If neither option is given, option xtrabackup --incremental-basedir is passed to xtrabackup by default, set to the first timestamped backup directory in the backup base directory.","title":"--incremental"},{"location":"xtrabackup_bin/xbk_option_reference.html#-incremental-basedirdirectory","text":"This directory contains the full backup, which is the base dataset used for the incremental backups.","title":"--incremental-basedir=DIRECTORY"},{"location":"xtrabackup_bin/xbk_option_reference.html#-incremental-dirdirectory","text":"When preparing an incremental backup, this is the directory where the incremental backup is combined with the full backup to make a new full backup.","title":"--incremental-dir=DIRECTORY"},{"location":"xtrabackup_bin/xbk_option_reference.html#-incremental-force-scan","text":"When creating an incremental backup, force a full scan of the data pages in the instance to be used in the backup even if the complete changed page bitmap data is available.","title":"--incremental-force-scan"},{"location":"xtrabackup_bin/xbk_option_reference.html#-incremental-history-namename","text":"This option specifies the name of the backup series stored in the PERCONA_SCHEMA.xtrabackup_history history record to base an incremental backup on. xtrabackup searches the history table for the most recent (highest innodb_to_lsn), successful backup in the series and take the to_lsn value to use as the starting lsn for the incremental backup. This will be mutually exclusive with xtrabackup --incremental-history-uuid , xtrabackup --incremental-basedir and xtrabackup --incremental-lsn . If no valid LSN can be found (no series by that name, no successful backups by that name) xtrabackup returns an error. It is used with the xtrabackup --incremental option.","title":"--incremental-history-name=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-innodb-checksum-algorithmname","text":"The algorithm InnoDB uses to calculate a page checksum. The available algorithms are CRC32, INNODB, NONE, STRICT_CRC32, STRICT_INNODB, and STRICT_NONE","title":"--innodb-checksum-algorithm=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-incremental-history-uuiduuid","text":"This option specifies the UUID of the specific history record stored in the PERCONA_SCHEMA.xtrabackup_history to base an incremental backup on xtrabackup --incremental-history-name , xtrabackup --incremental-basedir and xtrabackup --incremental-lsn . If no valid LSN is found (no success record with that UUID) xtrabackup returns an error. This option is used with the xtrabackup --incremental option.","title":"--incremental-history-uuid=UUID"},{"location":"xtrabackup_bin/xbk_option_reference.html#-incremental-lsnlsn","text":"When creating an incremental backup, you can specify the log sequence number ( LSN ) instead of specifying xtrabackup --incremental-basedir . For databases created in 5.1 and later, specify the LSN as a single 64-bit integer. ATTENTION : If a wrong LSN value is specified (a user error that Percona XtraBackup cannot detect), the backup will be unusable. Be careful!","title":"--incremental-lsn=LSN"},{"location":"xtrabackup_bin/xbk_option_reference.html#-innodb-log-arch-dirdirectory","text":"This option is used to specify the directory containing the archived logs. It can only be used with the xtrabackup --prepare option.","title":"--innodb-log-arch-dir=DIRECTORY"},{"location":"xtrabackup_bin/xbk_option_reference.html#-innodb-miscellaneous","text":"A large group of InnoDB options are normally read from the my.cnf configuration file, so that xtrabackup boots up its embedded InnoDB in the same configuration as your current server. You normally do not need to specify these explicitly. These options have the same behavior that they have in InnoDB or XtraDB. They are as follows: --innodb-adaptive-hash-index --innodb-additional-mem-pool-size --innodb-autoextend-increment --innodb-buffer-pool-size --innodb-checksums --innodb-data-file-path --innodb-data-home-dir --innodb-doublewrite-file --innodb-doublewrite --innodb-extra-undoslots --innodb-fast-checksum --innodb-file-io-threads --innodb-file-per-table --innodb-flush-log-at-trx-commit --innodb-flush-method --innodb-force-recovery --innodb-io-capacity --innodb-lock-wait-timeout --innodb-log-buffer-size --innodb-log-files-in-group --innodb-log-file-size --innodb-log-group-home-dir --innodb-max-dirty-pages-pct --innodb-open-files --innodb-page-size --innodb-read-io-threads --innodb-write-io-threads","title":"--innodb-miscellaneous"},{"location":"xtrabackup_bin/xbk_option_reference.html#-innodb-undo-directoryname","text":"The directory location for the undo tablespace. The path is absolute.","title":"--innodb-undo-directory=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-innodb-undo-tablespace","text":"The number of undo tablespaces to use.","title":"--innodb-undo-tablespace="},{"location":"xtrabackup_bin/xbk_option_reference.html#-keyring-file-datafilename","text":"The path to the keyring file. Combine this option with xtrabackup --xtrabackup-plugin-dir .","title":"--keyring-file-data=FILENAME"},{"location":"xtrabackup_bin/xbk_option_reference.html#-kill-long-queries-timeout","text":"This options specifies the number of seconds xtrabackup waits between starting FLUSH TABLES WITH READ LOCK and killing those queries that block it. The default is 0 (zero) seconds, which means the xtrabackup does not attempt to kill any queries.","title":"--kill-long-queries-timeout="},{"location":"xtrabackup_bin/xbk_option_reference.html#-kill-long-query-typeselectall","text":"This option specifies which types of queries should be killed to unblock the global lock. The default value is select .","title":"--kill-long-query-type=select|all"},{"location":"xtrabackup_bin/xbk_option_reference.html#-lock-ddl","text":"Issue LOCK TABLES FOR BACKUP if it is supported by server at the beginning of the backup to block all DDL operations.","title":"--lock-ddl"},{"location":"xtrabackup_bin/xbk_option_reference.html#-lock-ddl-per-table","text":"Lock DDL for each table before xtrabackup starts to copy it and until the backup is completed.","title":"--lock-ddl-per-table"},{"location":"xtrabackup_bin/xbk_option_reference.html#-lock-ddl-timeout","text":"If LOCK TABLES FOR BACKUP does not return within given timeout, abort the backup.","title":"--lock-ddl-timeout"},{"location":"xtrabackup_bin/xbk_option_reference.html#-log-binname","text":"The base name for the log sequence.","title":"--log-bin[=name]"},{"location":"xtrabackup_bin/xbk_option_reference.html#-log-copy-interval","text":"This option specifies time interval between log copying thread checks in milliseconds (default is 1 second).","title":"--log-copy-interval="},{"location":"xtrabackup_bin/xbk_option_reference.html#-login-path","text":"Read this path from the login file.","title":"--login-path="},{"location":"xtrabackup_bin/xbk_option_reference.html#-move-back","text":"Move all the files in a previously made backup from the backup directory to their original locations. As this option removes backup files, it must be used with caution.","title":"--move-back"},{"location":"xtrabackup_bin/xbk_option_reference.html#-no-backup-locks","text":"This options controls if backup locks are used instead of FLUSH TABLES WITH READ LOCK during the backup stage. The backup locks are must be supported on the server for the option to have an affect. This option is enabled by default. Disable the option with --no-backup-locks .","title":"--no-backup-locks"},{"location":"xtrabackup_bin/xbk_option_reference.html#-no-defaults","text":"Do not read the default options from any option file. Must be given as the first option on the command-line.","title":"--no-defaults"},{"location":"xtrabackup_bin/xbk_option_reference.html#-no-lock","text":"This option automatically uses Backup Locks, and disables table locks, as a lightweight alternative to FLUSH TABLES WITH READ LOCK to copy non-InnoDB data to avoid blocking DML queries that modify InnoDB tables. Only use this option if all tables are InnoDB and you do not care about the binary log position of the backup. Do not use this option if any DDL statements will be executed or if any non-InnoDB tables are being updated (this includes the MyISAM tables in the mysql database). Using this option in these conditions could cause an inconsistent backup. If your backups fail to acquire a lock and you are planning to use this option, the failure may be caused by incoming replication events that prevent the lock from succeeding. Try the --safe-slave-backup to momentarily stop the replication slave thread. The xtrabackup-binlog-info is not created when the --no-lock is used because SHOW MASTER STATUS may be inconsistent. In certain conditions, xtrabackup_binlog_pos_innodb can be used instead to get consistent binlog coordinates as described in Working with Binary Logs .","title":"--no-lock"},{"location":"xtrabackup_bin/xbk_option_reference.html#-no-version-check","text":"This option disables the version check. If you do not pass this option, the automatic version check is enabled implicitly when xtrabackup runs in the --backup mode. To disable the version check, explicitly pass the --no-version-check option when invoking xtrabackup. When the automatic version check is enabled,xtrabackup performs a version check against the server on the backup stage after creating a server connection. xtrabackup sends the following information to the server: MySQL flavour and version Operating system name Percona Toolkit version Perl version Each piece of information has a unique identifier which is an MD5 hash value that Percona Toolkit uses to obtain statistics about how it is used. This value is a random UUID; no client information is either collected or stored.","title":"--no-version-check"},{"location":"xtrabackup_bin/xbk_option_reference.html#-open-files-limit","text":"The maximum number of file descriptors to reserve with setrlimit().","title":"--open-files-limit="},{"location":"xtrabackup_bin/xbk_option_reference.html#-parallel","text":"This option specifies the number of threads to use to copy multiple data files concurrently when creating a backup. The default value is 1 (i.e., no concurrent transfer). In Percona XtraBackup 2.3.10 and newer, this option can be used with xtrabackup --copy-back option to copy the user data files in parallel (redo logs and system tablespaces are copied in the main thread).","title":"--parallel="},{"location":"xtrabackup_bin/xbk_option_reference.html#-passwordpassword","text":"This option specifies the password to use when connecting to the database. It accepts a string argument. See mysql --help for details.","title":"--password=PASSWORD"},{"location":"xtrabackup_bin/xbk_option_reference.html#-prepare","text":"Makes xtrabackup perform recovery on a backup created with xtrabackup --backup , so that it is ready to use. See preparing a backup .","title":"--prepare"},{"location":"xtrabackup_bin/xbk_option_reference.html#-print-defaults","text":"Print the program argument list and exit. Must be given as the first option on the command-line.","title":"--print-defaults"},{"location":"xtrabackup_bin/xbk_option_reference.html#-print-param","text":"Makes xtrabackup print out parameters that to copy the data files back to their original locations to restore them. See Scripting Backups With xtrabackup .","title":"--print-param"},{"location":"xtrabackup_bin/xbk_option_reference.html#-read-buffer-size","text":"Set read buffer size. The given value is scaled up to page size. The default is 10MB. Use this variable to increase the xbcloud/xbstream chunk size from the default value of 10MB. NOTE : When you use encryption, to adjust the xbcloud/xbstream chunk size, adjust both the --encrypt-chunk-size and --read-buffer-size variables. $ xtrabackup ... --read-buffer-size=1G | xbcloud put ...","title":"--read-buffer-size[=#]"},{"location":"xtrabackup_bin/xbk_option_reference.html#-rebuild-indexes","text":"Rebuild secondary indexes in InnoDB tables after applying the log. Only use with \u2013prepare.","title":"--rebuild-indexes"},{"location":"xtrabackup_bin/xbk_option_reference.html#-rebuild-threads","text":"This option defines the number of threads to rebuild indexes in a compact backup. Only use with --prepare and --rebuild-indexes .","title":"--rebuild-threads="},{"location":"xtrabackup_bin/xbk_option_reference.html#-redo-log-version","text":"The redo log version of the backup. Use only with --prepare .","title":"--redo-log-version="},{"location":"xtrabackup_bin/xbk_option_reference.html#-reencrypt-for-server-id","text":"Use this option to start the server instance with different server_id from the one the encrypted backup was taken from, like a replication replica or a Galera node. When this option is used, xtrabackup will, as a prepare step, generate a new master key with ID based on the new server_id, store it into keyring file, and re-encrypt the tablespace keys inside of tablespace headers. The option should be passed for --prepare (final step).","title":"--reencrypt-for-server-id="},{"location":"xtrabackup_bin/xbk_option_reference.html#-remove-original","text":"Implemented in Percona XtraBackup 2.4.6, this option when specified will remove .qp , .xbcrypt and .qp.xbcrypt files after decryption and decompression.","title":"--remove-original"},{"location":"xtrabackup_bin/xbk_option_reference.html#-rsync","text":"Use the rsync utility to optimize local file transfers. When this option is specified, xtrabackup uses rsync to copy all non-InnoDB files instead of spawning a separate copy command for each file. This option is faster for servers with a large number of databases or tables. This option cannot be used with --stream .","title":"--rsync"},{"location":"xtrabackup_bin/xbk_option_reference.html#-safe-slave-backup","text":"When specified, xtrabackup will stop the replica SQL thread just before running FLUSH TABLES WITH READ LOCK and wait to start backup until Slave_open_temp_tables in SHOW STATUS is zero. If there are no open temporary tables, the backup will occur; otherwise the SQL thread will be started and stopped until there are no open temporary tables. The backup will fail if Slave_open_temp_tables does not become zero after xtrabackup \u2013safe-slave-backup-timeout seconds. The replica SQL thread will be restarted when the backup finishes. This option is implemented to deal with replicating temporary tables and isn\u2019t neccessary with Row-Based-Replication.","title":"--safe-slave-backup"},{"location":"xtrabackup_bin/xbk_option_reference.html#-safe-slave-backup-timeoutseconds","text":"How many seconds xtrabackup --safe-slave-backup should wait for Slave_open_temp_tables to become zero. The default is 300 seconds.","title":"--safe-slave-backup-timeout=SECONDS"},{"location":"xtrabackup_bin/xbk_option_reference.html#-secure-auth","text":"Refuse client connecting to the server if it uses old (pre-4.1.1) protocol. (Enabled by default; use \u2013skip-secure-auth to disable.)","title":"--secure-auth"},{"location":"xtrabackup_bin/xbk_option_reference.html#-server-id","text":"The server instance being backed up.","title":"--server-id="},{"location":"xtrabackup_bin/xbk_option_reference.html#-server-public-key-pathname","text":"File path the server\u2019s public RSA key in PEM format.","title":"--server-public-key-path=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-skip-tables-compatibility-check","text":"This option disables the engine compatibility warning. See also --tables-compatibility-check","title":"--skip-tables-compatibility-check"},{"location":"xtrabackup_bin/xbk_option_reference.html#-slave-info","text":"This option is useful when backing up a replication replica server. It prints the binary log position of the source server. It also writes the binary log coordinates to the xtrabackup_slave_info file as a CHANGE MASTER command. A new replica for this source can be set up by starting a replica server on this backup and issuing a CHANGE MASTER command with the binary log position saved in the xtrabackup_slave_info file.","title":"--slave-info"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl","text":"Enable secure connection. More information can be found in \u2013ssl MySQL server documentation.","title":"--ssl"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl-ca","text":"Path of the file, which contains a list of trusted SSL CAs. More information can be found in \u2013ssl-ca MySQL server documentation.","title":"--ssl-ca"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl-capath","text":"The directory path that contains trusted SSL CA certificates in the PEM format. More information can be found in \u2013ssl-capath MySQL server documentation.","title":"--ssl-capath"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl-cert","text":"Path of the file which contains X509 certificate in PEM format. More information can be found in \u2013ssl-cert MySQL server documentation.","title":"--ssl-cert"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl-cipher","text":"List of permitted ciphers to use for connection encryption. More information can be found in \u2013ssl-cipher MySQL server documentation.","title":"--ssl-cipher"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl-crl","text":"Path of the file that contains certificate revocation lists. More information can be found in \u2013ssl-crl MySQL server documentation.","title":"--ssl-crl"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl-crlpath","text":"Path of the directory that contains certificate revocation list files. More information can be found in \u2013ssl-crlpath MySQL server documentation.","title":"--ssl-crlpath"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl-key","text":"Path of the file that contains X509 key in PEM format. More information can be found in \u2013ssl-key MySQL server documentation.","title":"--ssl-key"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl-mode","text":"The security state of connection to server. More information can be found in \u2013ssl-mode MySQL server documentation.","title":"--ssl-mode"},{"location":"xtrabackup_bin/xbk_option_reference.html#-ssl-verify-server-cert","text":"Verify server certificate Common Name value against host name used when connecting to server. More information can be found in \u2013ssl-verify-server-cert MySQL server documentation.","title":"--ssl-verify-server-cert"},{"location":"xtrabackup_bin/xbk_option_reference.html#-stats","text":"Causes xtrabackup to scan the specified data files and print out index statistics.","title":"--stats"},{"location":"xtrabackup_bin/xbk_option_reference.html#-streamname","text":"Stream all backup files to the standard output in the specified format. Currently supported formats are xbstream and tar .","title":"--stream=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-tablesname","text":"A regular expression against which the full tablename, in databasename.tablename format, is matched. If the name matches, the table is backed up. See partial backups .","title":"--tables=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-tables-compatibility-check","text":"This option enables the engine compatibility warning. The default value is ON . Use --skip-tables-compatibility-check to disable.","title":"--tables-compatibility-check"},{"location":"xtrabackup_bin/xbk_option_reference.html#-tables-excludename","text":"Filtering by regexp for table names. Operates the same way as xtrabackup --tables , but matched names are excluded from backup. Note that this option has a higher priority than xtrabackup --tables .","title":"--tables-exclude=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-tables-filename","text":"A file containing one table name per line, in databasename.tablename format. The backup will be limited to the specified tables. See Scripting Backups With xtrabackup .","title":"--tables-file=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-target-dirdirectory","text":"This option specifies the destination directory for the backup. If the directory does not exist, xtrabackup creates it. If the directory does exist and is empty, xtrabackup will succeed. xtrabackup will not overwrite existing files; however it will fail with operating system error 17, file exists . If this option is a relative path, it is interpreted as being relative to the current working directory from which xtrabackup is executed. In order to perform a backup, you need READ , WRITE , and EXECUTE permissions at a filesystem level for the directory that you supply as the value of --target-dir .","title":"--target-dir=DIRECTORY"},{"location":"xtrabackup_bin/xbk_option_reference.html#-throttle","text":"This option limits the number of chunks copied per second. The chunk size is 10 MB . To limit the bandwidth to 10 MB/s , set the option to 1 : --throttle=1 .","title":"--throttle="},{"location":"xtrabackup_bin/xbk_option_reference.html#-tls-versionname","text":"The TLS version to use. The allowed values are the following: TLSv1 TLSv1.1 TLSv1.2","title":"--tls-version=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-tmpdirname","text":"This option is currently not used for anything except printing out the correct tmpdir parameter when xtrabackup --print-param is used.","title":"--tmpdir=name"},{"location":"xtrabackup_bin/xbk_option_reference.html#-to-archived-lsnlsn","text":"This option is used to specify the LSN to which the logs should be applied when backups are being prepared. It can only be used with the xtrabackup --prepare option.","title":"--to-archived-lsn=LSN"},{"location":"xtrabackup_bin/xbk_option_reference.html#-transition-key","text":"This option is used to enable processing the backup without accessing the keyring vault server. In this case, xtrabackup derives the AES encryption key from the specified passphrase and uses it to encrypt tablespace keys of tablespaces being backed up. If --transition-key <xtrabackup \u2013transition-key> does not have any value, xtrabackup will ask for it. The same passphrase should be specified for the xtrabackup --prepare command.","title":"--transition-key"},{"location":"xtrabackup_bin/xbk_option_reference.html#-use-memory","text":"This option affects how much memory is allocated and is similar to innodb_buffer_pool_size . This option is only relevant in the --prepare phase or when analyzing statistics with --stats . The default value is 100MB. The recommended value is between 1GB to 2GB. Multiples are supported providing the unit (for example, 1MB, 1M, 1GB, 1G).","title":"--use-memory="},{"location":"xtrabackup_bin/xbk_option_reference.html#-userusername","text":"This option specifies the MySQL username used when connecting to the server, if that\u2019s not the current user. The option accepts a string argument. See mysql \u2013help for details.","title":"--user=USERNAME"},{"location":"xtrabackup_bin/xbk_option_reference.html#-version","text":"This option prints xtrabackup version and exits.","title":"--version"},{"location":"xtrabackup_bin/xbk_option_reference.html#-xtrabackup-plugin-dirdirname","text":"The absolute path to the directory that contains the keyring plugin. See also Percona Server for MySQL Documentation: keyring_vault plugin with Data at Rest Encryption https://www.percona.com/doc/percona-server/5.7/security/data-at-rest-encryption.html MySQL Documentation: Using the keyring_file File-Based Plugin https://dev.mysql.com/doc/refman/5.7/en/keyring-file-plugin.html","title":"--xtrabackup-plugin-dir=DIRNAME"},{"location":"xtrabackup_bin/xtrabackup_binary.html","text":"The xtrabackup Binary \u00b6 The xtrabackup binary is a compiled C program that is linked with the InnoDB libraries and the standard MySQL client libraries. The InnoDB libraries provide functionality necessary to apply a log to data files, and the MySQL client libraries provide command-line option parsing, configuration file parsing, and so on to give the binary a familiar look and feel. The tool runs in either xtrabackup --backup or xtrabackup --prepare mode, corresponding to the two main functions it performs. There are several variations on these functions to accomplish different tasks, and there are two less commonly used modes, xtrabackup --stats and xtrabackup --print-param . Other Types of Backups \u00b6 Incremental Backups Partial Backups Advanced Features \u00b6 Throttling Backups Scripting Backups With xtrabackup Analyzing Table Statistics Working with Binary Logs Restoring Individual Tables LRU dump backup Implementation \u00b6 Implementation Details xtrabackup Exit Codes References \u00b6 The xtrabackup Option Reference","title":"The xtrabackup Binary"},{"location":"xtrabackup_bin/xtrabackup_binary.html#the-xtrabackup-binary","text":"The xtrabackup binary is a compiled C program that is linked with the InnoDB libraries and the standard MySQL client libraries. The InnoDB libraries provide functionality necessary to apply a log to data files, and the MySQL client libraries provide command-line option parsing, configuration file parsing, and so on to give the binary a familiar look and feel. The tool runs in either xtrabackup --backup or xtrabackup --prepare mode, corresponding to the two main functions it performs. There are several variations on these functions to accomplish different tasks, and there are two less commonly used modes, xtrabackup --stats and xtrabackup --print-param .","title":"The xtrabackup Binary"},{"location":"xtrabackup_bin/xtrabackup_binary.html#other-types-of-backups","text":"Incremental Backups Partial Backups","title":"Other Types of Backups"},{"location":"xtrabackup_bin/xtrabackup_binary.html#advanced-features","text":"Throttling Backups Scripting Backups With xtrabackup Analyzing Table Statistics Working with Binary Logs Restoring Individual Tables LRU dump backup","title":"Advanced Features"},{"location":"xtrabackup_bin/xtrabackup_binary.html#implementation","text":"Implementation Details xtrabackup Exit Codes","title":"Implementation"},{"location":"xtrabackup_bin/xtrabackup_binary.html#references","text":"The xtrabackup Option Reference","title":"References"},{"location":"xtrabackup_bin/xtrabackup_exit_codes.html","text":"xtrabackup Exit Codes \u00b6 The xtrabackup binary exits with the traditional success value of 0 after a backup when no error occurs. If an error occurs during the backup, the exit value is 1. In certain cases, the exit value can be something other than 0 or 1, due to the command-line option code included from the MySQL libraries. An unknown command-line option, for example, will cause an exit code of 255.","title":"*xtrabackup* Exit Codes"},{"location":"xtrabackup_bin/xtrabackup_exit_codes.html#xtrabackup-exit-codes","text":"The xtrabackup binary exits with the traditional success value of 0 after a backup when no error occurs. If an error occurs during the backup, the exit value is 1. In certain cases, the exit value can be something other than 0 or 1, due to the command-line option code included from the MySQL libraries. An unknown command-line option, for example, will cause an exit code of 255.","title":"xtrabackup Exit Codes"}]}